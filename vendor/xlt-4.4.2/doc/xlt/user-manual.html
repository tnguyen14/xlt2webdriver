<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<link type="text/css" rel="stylesheet" href="css/shCore.css" />
<link type="text/css" rel="stylesheet" href="css/shThemeXceptance.css" />

<link type="text/css" rel="stylesheet" href="css/lightbox.css" />

<link type="text/css" rel="stylesheet" href="css/default.css" media="all"/>
<link type="text/css" rel="stylesheet" href="css/print.css" media="print"/>

<script type="text/javascript" src="js/jquery-1.7.2.min.js"><!-- empty --></script>
<script type="text/javascript" src="js/jquery.scrollTo-1.4.3.1-min.js"><!-- empty --></script>
<script type="text/javascript" src="js/jquery.toc-1.1.4.min.js"><!-- empty --></script>
<script type="text/javascript" src="js/jquery.lightbox.js"><!-- empty --></script>

<script type="text/javascript" src="js/shCore.js"><!-- empty --></script>
<script type="text/javascript" src="js/shBrushJScript.js"><!-- empty --></script>
<script type="text/javascript" src="js/shBrushBash.js"><!-- empty --></script>
<script type="text/javascript" src="js/shBrushJava.js"><!-- empty --></script>
<script type="text/javascript" src="js/shBrushCss.js"><!-- empty --></script>
<script type="text/javascript" src="js/shBrushPlain.js"><!-- empty --></script>
<script type="text/javascript" src="js/shBrushXml.js"><!-- empty --></script>
<script type="text/javascript" src="js/shBrushRuby.js"><!-- empty --></script>

<script type="text/javascript" src="js/xltdoc.js"><!-- empty --></script>
<title>Xceptance LoadTest - XLT 4.4.2 - User Manual</title></head><body><div id="globalNavigation">
    <ul>
        <li><a href="http://www.xceptance-loadtest.com/" title="XLT Online">XLT Online</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="index.html" title="The documentation index page.">Index</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="https://lab.xceptance.de/releases/xlt/4.4.2/xlt-4.4.2.zip" title="Download this release">Download</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="releasenotes.html" title="Release Notes">Release Notes</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="quick-start-guide.html" title="Quick Start Guide">Quick Start Guide</a></li>
        <li><span class="separator">|</span></li>
        <li class="current"><a href="user-manual.html" title="User Manual">User Manual</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="https://lab.xceptance.de/releases/xlt/latest/apidoc/" title="Online API Documentation">API</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="faq.html" title="Frequently Asked Questions">FAQ</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="license.html" title="License">License</a></li>
        <li><span class="separator">|</span></li>
        <li><a href="glossary.html" title="Glossary">Glossary</a></li>
    </ul>
 </div><div id="container" class="simple"><noscript><div style="text-align:center;">Please activate JavaScript in your browser to use a table of content and source code syntax highlighting in this document.</div></noscript> <h1 id="imagesXLTlogo.pngXLTLogoXLTUserManual"><img alt="XLT Logo" title="XLT Logo" border="0" src="images/XLT_logo.png"/> XLT &#8211; User Manual</h1><div id="toc" class="numbered"></div>
 <div id="content"><h2 id="Introduction">Introduction</h2><h3 id="AboutthisDocument">About this Document</h3><p>This user manual comprehensively illustrates the use and features of the regression and load testing tool <em>Xceptance LoadTest</em> (XLT). To get the most out of the following explanations and to work effectively with XLT, a basic understanding of web technologies, Java, and the JUnit concept may be helpful. </p><h3 id="WhatisXLT">What is XLT?</h3><p>XLT is a tool that lets you easily develop and run both regression and load tests for web applications. Nearly every software providing access via HTTP/HTML can be tested; for the testing of applications using Web 2.0 technologies, XLT features extensive JavaScript support. Besides pure web testing, XLT additionally offers SQL tests, RCP-based application tests, or any other test meant to run on platforms supporting Java. </p><h3 id="XLTScriptDeveloper">XLT Script Developer</h3><p>XLT Script Developer, a Firefox Add-on, is a convenient user interface for the scripting and running of test cases and test suites. While you navigate through a website, it records the page flow and features a wide range of validations. For the programming of more complex test scenarios or validations serving your individual purposes, test cases can be easily exported to Java and edited in an IDE.  </p><h3 id="XLTFramework">XLT Framework</h3><p>The XLT framework offers various programming interfaces for writing individualized test cases in Java. Starting with Java code automatically generated from the test cases recorded with XLT Script Developer, you can extend your test suite using the XLT API. It features the <em>XLT Scripting API</em>, that is a high-level command scripting API with a very intuitive syntax, and the <a href="http://seleniumhq.org/docs/03_webdriver.html#selenium-webdriver-api-commands-and-operations">WebDriver API</a>. The XLT framework furthermore includes the lower-leveled <em>XLT Action API</em> based on <a href="http://htmlunit.sourceforge.net/">HtmlUnit</a>.</p><p>The XLT API provides a programming paradigm to translate all test scenarios into <a href="http://www.junit.org/">JUnit4</a> tests. The principles of JUnit4 and its annotations are used to implement and tag test cases. Thus, each XLT test is also a JUnit test, which allows you to execute XLT tests just like any other unit test within a build process.</p><h2 id="InstallationInstructions">Installation Instructions</h2><h3 id="SystemRequirements">System Requirements</h3><h4 id="Hardware">Hardware</h4><ul><li>CPU at 1.5GHz or higher </li><li>1.0 GB RAM </li><li>1.0 GB available in the hard disk (default installation requires 85 MB but load test results might need additional capacity) </li></ul><h4 id="Software">Software</h4><ul><li>Operating System: Microsoft Windows, Linux, Oracle Solaris, HP-UX, or Mac OS X, that is any operating system for which a JVM 7 (or higher) is available. </li><li>JVM: It&#8217;s recommended to use Oracle&#8217;s JVM. XLT also runs in JVMs provided by vendors like BEA, HP, or IBM, but this has not been tested extensively.</li><li>Browser: Firefox, Chrome, Internet Explorer 8, or Safari 5 for the HTML load reports. Note that JavaScript has to be enabled to utilize all functionality. If you want to use XLT Script Developer, you need to have Firefox 17 (or higher). </li></ul><h3 id="InstallingXLT">Installing XLT</h3><p>Unzip the XLT archive<sup class="footnote"><a href="#___fn1">1</a></sup> to a file system location of your choice. The root directory is part of the archive, so you don&#8217;t need to create it separately. XLT supports spaces in the path; however, it&#8217;s easier to code tests when the path is free of them.</p><p>Copy the license file to the directory <code>&lt;XLT&gt;/config</code>. Please note that the <em>Basic License</em> does not require the installation of a license file. It restricts the number of virtual users to five; yet there are neither temporal nor functional limitations. Also note that the <em>Basic License Terms</em> apply in that case. See <XLT>/doc/license.html for more information.</p><p>Make sure the executable directory of your Java installation is listed in your <code>PATH</code> environment variable so that the XLT start scripts can find the JVM runtime.</p><p>To install the XLT Script Developer extension for Firefox, you need to:</p><ul><li>Start Firefox.</li><li>Click <em>File</em> &gt; <em>Open File...</em>.</li><li>Navigate to the <code>&lt;XLT&gt;/tools</code> directory and select the <code>.xpi</code> file. The Add-on installation dialog appears.</li><li>Click <em>Install</em> to finish. </li></ul><p>Alternatively, you can drag the <code>.xpi</code> file onto the Firefox window. </p><p id="___fn1" class="footnote"><sup>1</sup> The XLT archive can either be obtained from the <em>Xceptance</em> website or from our Maven-compatible repository which allows users of Maven and Ivy to conveniently integrate XLT and all of its dependencies. See <a href="user-manual.html#MavenIvy">Writing Web Tests</a> for additional information.</p><h3 id="UpdatingXLT">Updating XLT</h3><p>Before you update XLT, it&#8217;s highly recommended to back up all modified files and project-specific or customized settings. In particular, this includes:</p><ul><li>All your <strong>test suites</strong> (especially, when stored in a sub-folder of the XLT installation directory)</li><li><strong>Result</strong> files (stored in <code>&lt;XLT&gt;/results</code> by default)</li><li>Generated load test <strong>reports</strong> (stored in <code>&lt;XLT&gt;/reports</code> by default)</li><li>Modified XLT <strong>properties</strong> files (<code>&lt;XLT&gt;/config</code>)</li><li>The <strong>license</strong> file <em>license.xml</em> (<code>&lt;XLT&gt;/config</code>)</li></ul><p>Download and install the latest XLT version from the <em>Xceptance</em> website as described above. You can have multiple XLT versions simultaneously since the name of the unpacked installation folder includes the version number by default. </p><p>Copy your backed-up files and directories to the corresponding place in the new XLT installation directory.</p><p>New test suite settings are provided in the <strong>default.properties</strong> file of the test suite <em>testsuite-template</em>. Copy it from <code>&lt;LatestXLTversion&gt;/samples/testsuite-template/config</code> to the config directory of your test suites <code>&lt;YourTestSuite&gt;/config</code>.</p><p>To update XLT Script Developer, go to <em>File &gt; Open File</em> in the Firefox menu, and then open the latest <code>.xpi</code> file located at <code>&lt;LatestXLTversion&gt;/tools</code>. Alternatively, drag the <code>.xpi</code> file and drop it onto Firefox. If you work with Java-based test cases, add the updated XLT libraries to the Java build path of the Eclipse project. See <a href="user-manual.html#ImportPosters">Importing the Posters Test Suite into Eclipse</a> for more information.</p><blockquote class="note"><p>Note that when you configure your test project to use a newer version of XLT, do not forget to update XLT on your load machines as well. The version you have used to develop your test scripts must match the executing version of your load test environment.</p></blockquote><h3 id="UninstallingXLT">Uninstalling XLT</h3><p>Before uninstalling XLT, make sure to back up all test results and test reports you want to keep. To uninstall XLT, simply delete its installation directory. Use the Firefox Add-on dialog to remove the Script Developer Firefox extension.</p><h2 id="ScriptDeveloper">Script Developer</h2><h3 id="Introduction2">Introduction</h3><p>XLT Script Developer is a Firefox extension used to create test case scripts. Script test cases are based on a simple syntax and a reduced set of operations, which makes them a perfect fit for non-programmers. Besides Script Developer, no other tool is necessary to create, edit, and manage basic script test cases. </p><p>Script Developer <em>records</em> test cases, that is you simply use an application you want to test while your actions are being recorded in the background and stored to an XML script file. During recording, you can add commands to perform validations on the page. Any recorded value can later be extracted from the script into a test data file to separate test data from script code. Scripts may also be exported as ordinary Java code.</p><p>With Script Developer, script files can be replayed in Firefox at any time to quickly check whether the test case is running successfully. </p><p>Before you set up and use Script Developer, it is recommended to learn about its basic concepts.</p><h3 id="ConceptsScriptDeveloper">Basic Concepts</h3><p><a href="http://seleniumhq.org/">Selenium</a> is widely used for test automation of web applications. XLT and Script Developer rely on similar concepts, which shall be defined in the following and referred to throughout this manual:</p><p>An <em>Action</em> is an atomic collection of <em>Commands</em> in a test. It may actually be split into sub-parts, but it can generally be seen as mathematically closed part of a test. Also see <a href="user-manual.html#action">here</a> because the concept of an <em>Action</em> is the same for the XLT framework and Script Developer. However, be aware that our definition of the term <em>Action</em> completely differs from the one used by Selenium.</p><p>An <em>Assertion</em> verifies that a condition still holds true. Thus, it always consists of the formulation of an expected value that is compared to the actual value. If both values are different, the condition is violated and the test will abort with an error.</p><p>A <em>Command</em> in Script Developer is the same as in Selenium. It&#8217;s a single statement in the test, for example, causing the test to sleep for some time, simulating a click, or waiting for an element to become visible.  </p><p>An <em>Element Locator</em> or <em>Element Identification Strategy</em> is the approach used to identify an element on a page. While in simple examples different strategies lead to the same result causing the same effort, their benefits may differ in more complex scenarios. </p><p>These concepts will be repeatedly touched upon in the next chapters, especially those on test editing. See <a href="user-manual.html#Commands">Appendix</a> for a full list of available commands, including examples. </p><h3 id="Settings">Settings</h3><p>Before you start, make sure the Script Developer Firefox extension is installed correctly. Open the Script Developer window via the Firefox <em>Tools</em> menu or, alternatively, by clicking the XLT icon in the status bar. If you start XLT Script Developer for the very first time, you will be asked to configure a test suite. Click OK, then select a directory that contains a XLT test suite or, if no test suite exists yet, select a target directory where a new test suite should be created.</p><p>Next, you need to change some basic settings. In the upper left of the Script Developer window, go to <em>Script Developer</em> &gt; <em>Settings</em> to open the configuration dialog.</p><p class="illustration"><a href="images/user-manual/ScriptDev_Settings.png"><img alt="Script Developer Settings" title="Script Developer Settings" border="0" src="images/user-manual/ScriptDev_Settings-small.png"/></a> <span class="caption">Script Developer Settings</span></p><h4 id="JavaCodeSettings">Java Code Settings</h4><p><strong>Generate JUnit wrapper class for test cases</strong>: Activates the generation of JUnit wrapper classes needed to run recorded scripts in your Java IDE. This box has to be checked if you want to run load tests using recorded test cases or if you want to automate these tests, e.g. in a build process.</p><p><strong>Source Directory Name</strong>: The name of the directory where the generated Java source code is saved (<em>src</em> in most cases). The path is relative to the location of the current test suite.</p><p><strong>File Encoding</strong>: The character encoding scheme for the JUnit wrapper classes. It should match the settings in your IDE when running test cases as JUnit tests.</p><h4 id="RecordingSettings">Recording Settings</h4><p><strong>Element Identification Strategies</strong>: Check the boxes to select element identification strategies available during test case recording. Each web element type has a default identification strategy. If you haven&#8217;t checked it, one of the other identification strategies will be used to identify the element. For example, when you deselect the <em>Name</em> checkbox, the recorder will avoid command targets like <em>name=xyz</em> and the <em>name</em> attribute in XPath expressions. Note that XPath is used as a fall-back strategy and can&#8217;t be disabled.</p><p><strong>Attribute Filtering</strong>: <em>Include Patterns</em> and <em>Exclude Patterns</em> let you filter attributes used in element locators. If an <em>Include Pattern</em> is defined, then only attributes matching this filter pattern are used in element locators, whereas non-matching attributes will be avoided. On the other hand, an attribute will be avoided in element locators if it matches one of the <em>Exclude Patterns</em>. <em>Exclude Patterns</em> always take precedence over <em>Include Patterns</em> , that is an attribute will be avoided if it matches the <em>Exclude Pattern</em>, even if it also matches any <em>Include Pattern</em>. Both pattern types are available for <em>ID</em>, <em>Name</em>, and <em>Class</em> attributes and must be given as regular expressions. More than one <em>Include and Exclude Patterns</em> can be defined, separated by whitespace. </p><h4 id="ReplaySettings">Replay Settings</h4><p><strong>Command Timeout (in sec.)</strong>: Defines the default timeout for replaying <code>WaitForXyz</code> commands. When this time has elapsed and the condition is still false, the command will fail with an error message. This value can be changed for particular scripts by using the <code>setTimeout</code> command.</p><p><strong>Implicit Wait Timeout (in msec)</strong>: This value defines the maximum time to wait for the target element to (dis)appear before the command is seen as failed. It is used by all commands that require a target element <strong>but not</strong> by <code>assert</code> and <code>waitFor</code> commands.</p><p>The implicit wait timeout is extremely useful if your web application is very dynamic and uses asynchronous JavaScript to build and modify the page. Normally, you would use a <code>waitFor</code> command to wait for the element to appear before you can interact with it. When an implicit wait timeout is defined, most of the <code>waitFor</code> commands can be omitted, making your test cases shorter and easier to maintain.</p><h4 id="EditorSettings">Editor Settings</h4><p><strong>Display line numbers</strong>: If checked, line numbers are shown in front of a command when a test case or module is open for editing.</p><h3 id="UserInterfaceElements">User Interface Elements</h3><p>The main window of Script Developer features the following sections and elements:</p><p class="illustration"><img alt="Script Developer" title="Script Developer" border="0" src="images/user-manual/ScriptDev_MainWindow.png"/> <span class="caption">Script Developer</span></p><h4 id="ToolbarandRecordReplaySection">Toolbar and Record/Replay Section</h4><table><tr><th>Control</th><th>Description</th></tr><tr><td><span style="white-space:nowrap">Script Developer</span> </td><td>Provides access to <em>Settings</em>, <em>About</em>, <em>Manage Global Test Data</em>, and the XLT website.</td></tr><tr><td>Replay Speed</td><td>Controls the replay speed for test cases and modules. The selected replay speed determines how long elements addressed by commands are being highlighted during replay. Please note that this does NOT affect how fast those commands are being executed.</td></tr><tr><td>Replay</td><td>Replays the currently loaded test case or module.</td></tr><tr><td>Single Step Forward</td><td>Executes the currently marked command and moves to the next command.</td></tr><tr><td>Pause</td><td>Pauses the replay of a test case, and activates the <em>Continue Replay</em> and <em>Single Step Forward</em> buttons.</td></tr><tr><td>Continue Replay</td><td>Continues replay of a paused test case or module.</td></tr><tr><td>Stop</td><td>Stops recording or replay of a test case or module.</td></tr><tr><td>Record</td><td>Starts recording your actions on the web page in your active browser tab window.</td></tr><tr><td>Save</td><td>Saves the currently open test case or module.</td></tr><tr><td>Reload</td><td>Reloads all test cases and modules in the test project.</td></tr><tr><td>Base URL</td><td>Displays the base URL that is saved for the currently open test case (in <em>italics</em>). You can temporarily change it by entering another one into the edit field or by selecting from the drop-down menu. If you enter a valid URL, the URL saved in the test case will be overridden by the new value.</td></tr></table><h4 id="ProjectView">Project View</h4><p>The <em>project view</em> contains a tree view, the <em>script explorer</em>, that lists all available <strong>test cases</strong> and <strong>modules</strong> structured in <strong>packages</strong>. Script Developer loads all test cases and modules from <code>&lt;test-suite&gt;/scripts</code> and its sub-folders. The package structure of the script explorer is reflected by the structure of the sub-folders. </p><blockquote class="note"><p>The package structure in the script explorer also reflects the package structure of the generated JUnit wrapper classes.</p></blockquote><p>The drop-down box includes all test suites known to Script Developer, which lets you easily switch between projects. Use the <em>Create/Import</em> button to import existing test projects still unknown to Script Developer. </p><blockquote class="note"><p>When importing a project, make sure to choose the parent directory of the <em>scripts</em> directory, and not the <em>script</em> directory itself. To create a new project,  select an existing directory from the file dialog popping up after you&#8217;ve clicked the <em>Create/Import</em> button.</p></blockquote><p>You can filter the displayed test cases or modules using the input field below the script explorer. Click the small arrow icon on the left to filter the list either by name, description, tag, or any combination of these. Typed characters are case-insensitive. To reset the search result, click the <em>x</em> icon on the right side of the input field or manually delete all characters.</p><p>You may change the alphabetic sorting (A-Z or Z-A) of the packages by clicking on the header of the <em>Name</em> column. The second column shows a description for each test case or module. You can add or edit the description in the <em>Edit Details</em> dialog.</p><h4 id="EditorTabs">Editor Tabs</h4><p>To open a test case or module for editing, double-click on a script or select the context menu option <strong>Edit</strong>. The script&#8217;s commands are listed inside the work area on the right-hand side of the developer window. The <strong>Edit Details</strong> option allows you to edit particular details and displays the test case or module in the editor window to help you recognize that something has been changed. You can open more than one script at a time and select a test case or module by clicking on the respective tab above the command list. The script editor tab displays the <em>Name</em>, <em>Target</em>, and <em>Value</em> attributes of each command in three adjustable columns.</p><h3 id="RecordingandReplayingTestCases">Recording and Replaying Test Cases</h3><h4 id="Recording">Recording</h4><p>Recording your interaction with the application you want to test is the easiest way to create test scripts. To do this, you need to: </p><ol><li>Open the web page you want to start with in a Firefox tab. Make sure this tab remains active, that is the foreground tab.</li><li>Switch to Script Developer and create a new test case via the context menu in the script explorer. Provide a meaningful name. An empty script editor tab opens.</li><li>Click the <em>Start Recording</em> icon in the tool bar to start recording. </li><li>Switch back to your web page and start using it. All your interactions with the page are being recorded.</li><li>When you&#8217;re done with your test scenario, switch back to Script Developer and click the <em>Stop</em> icon to stop recording. </li><li>Don&#8217;t forget to save the new script by clicking the <em>Save</em> icon or by using the <em>Ctrl+S</em> shortcut. See <a href="user-manual.html#DeveloperShortcuts">Appendix</a> for a full list of available shortcuts. </li></ol><blockquote class="note"><p>If you have the Firefox Add-on <em>Firebug</em>, you need to make sure it isn&#8217;t active during recording because its highlight mechanism may cause unnecessary interactions to be recorded.</p></blockquote><p>Note that besides recording the normal web page flow, you can also validate the correctness of pages by using assert commands. Generally, new or changed pages emerge throughout an interaction with a web page and should thus be checked. To record assertions, do the following:   </p><ol><li>Open the Firefox context menu and select <em>Script Developer</em> (available only during recording). A sub-menu opens.</li><li>Choose an appropriate assertion from the sub-menu. See <a href="user-manual.html#Commands">Appendix</a> for a full list of available assertions.</li><li>Continue interacting with the web page as defined by your test scenario.</li></ol><p class="illustration"><a href="images/user-manual/ScriptDev_CmdContextMenu.png"><img alt="Record commands using the context menu" title="Record commands using the context menu" border="0" src="images/user-manual/ScriptDev_CmdContextMenu-small.png"/></a> <span class="caption">Record Commands Using the Context Menu</span></p><p>For each assertion, there is also a variant checking whether the respective condition is <em>not</em> true. You can record as many assertions as you like.</p><h4 id="ReplayingaTestCase">Replaying a Test Case</h4><p>Once you&#8217;ve recorded a test case, it can be executed inside your browser. In the script explorer, right-click the test case to open its context menu, and then click <em>Run</em>. Alternatively, do the following:</p><ol><li>Open the test case script in a test case editor tab or activate the tab if it&#8217;s already open, and</li><li>click the <em>Play</em> icon in the tool bar.</li></ol><p>The script is replayed in the active Firefox browser tab. </p><p>While the script is running, you can watch the results of the script execution. You can see how links are being clicked, input fields are being filled, buttons and checkboxes are being clicked, etc. Inside the browser tab, actions and validations the script is currently dealing with are highlighted yellow and orange, respectively. Inside the script tab, the commands are marked with a special status icon. It&#8217;s yellow as long as the command or module is being executed. When the execution is finished, the icon will either turn green in case of success or red in case of errors. Note that the script execution will immediately stop if a command fails.</p><p>If you have difficulties following the script execution, use the speed slider in the tool bar to reduce the replay speed. Increase it if you need the results as quick as possible. Be aware though that the commands are not being executed any slower or faster this way. The slider only affects the amount of time elements are being highlighted.</p><p>You may stop or pause the script execution at any time. To stop a script, click the <em>Stop</em> icon in the tool bar. As a result, the script is terminated. The <em>Pause</em> icon suspends the execution of a script (after finishing the currently running command). To continue the script execution, click the <em>Continue Replay</em> icon. Alternatively, you can click the <em>Single Step Forward</em> icon, which only causes the next command to be executed.</p><p>You can set a start point and one or more breakpoints for a test script. See <a href="user-manual.html#Start_point">Start Point for Replaying Test Cases</a> and <a href="user-manual.html#Stop_point">Breakpoints</a> for more information.</p><blockquote class="note"><p>Note that clicks on alert, confirmation, or prompt boxes are not recorded and thus don&#8217;t pop up during replay either. Instead, Script Developer simulates a return value of the alert() or confirm() function, which is equivalent to clicking &#8220;OK&#8221; or &#8220;Yes&#8221;. For prompt(), an empty string is returned as your input value.</p></blockquote><h4 id="BatchTest">Running Batch Tests</h4><p>A sequence of test cases or an entire test suite can be run as batch test. Select the desired test cases from the script explorer by clicking them while holding the <em>CTRL</em> key, or, alternatively, choose one or more packages to run. Click <em>Run as Batch Test</em> from the script explorer context menu to start the batch test. </p><p>The selected test cases are executed consecutively. The result is displayed in a new tab in the work area with a colored bullet for each test case indicating the current state of the test execution. Grey stands for not tested, yellow for currently running test cases, green for passed and red for failed test cases. The total number of selected test cases, the number of executed test cases, an error count, and the elapsed time is shown in the header of the batch test tab. </p><p class="illustration"><a href="images/user-manual/ScriptDev_BatchTest.png"><img alt="Batch Test" title="Batch Test" border="0" src="images/user-manual/ScriptDev_BatchTest-small.png"/></a> <span class="caption">Batch Test</span></p><p>After the batch test is completed, the <em>Rerun failed tests</em> button will become active if at least one test failed. Clicking it starts a new batch test in a new tab with all test cases that failed the previous batch test. To export the batch test result to HTML, click the <em>Export as HTML</em> button and provide a meaningful name and a target location. You can then view the batch execution report in any web browser.</p><p class="illustration"><a href="images/user-manual/ScriptDev_BatchReport.png"><img alt="Batch Test HTML Report" title="Batch Test HTML Report" border="0" src="images/user-manual/ScriptDev_BatchReport-small.jpg"/></a> <span class="caption">Batch Test HTML Report</span></p><h3 id="EditingTestSuiteTestCasesandModules">Editing Test Suite, Test Cases and Modules</h3><p>You can modify script test cases and modules at any time. Often enough, it&#8217;s necessary to fix mistakes made during the recording of the test scenario. For example, you may have clicked a wrong link or forgotten to add an assertion. In the first case, just go back and continue with your test scenario; the unwanted or incorrect actions can be deleted later on. In the second case, manually add the appropriate assertion commands after you&#8217;ve finished recording. To modify a test case during recording, switch to the Script Developer window, make the necessary changes, and then return to the web page to continue recording.</p><h4 id="TestCaseProp">Editing Test Suite</h4><p class="illustration"><a href="images/user-manual/ScriptDev_TestCaseMenu.png"><img alt="Test suite context menu" title="Test suite context menu" border="0" src="images/user-manual/ScriptDev_TestCaseMenu-small.png"/></a> <span class="caption">Test Suite Context Menu</span></p><p>The right-click context menu in the script explorer offers the following options for editing the test suite and scripts (test cases or modules):</p><table><tr><th colspan="2">Menu Item </th><th>Description </th></tr><tr><td rowspan="4">New </td><td>Test Case </td><td>Creates a new test case. You can define name, script package, tags, description, base URL, and enable test case-specific settings in the dialog that pops up. </td></tr><tr><td>Script Module </td><td>Creates a new script module. See <a href="user-manual.html#Modules">Script Modules</a> for details. </td></tr><tr><td>Java Module </td><td>Creates a new custom Java module. See <a href="user-manual.html#JavaModules">Creating Java Modules</a> for details. </td></tr><tr><td>Script Package </td><td>Creates a new script package. The complete package name must always be specified, e.g. <em>testcases.cart.order</em>. </td></tr><tr><td colspan="2">Run </td><td>Directly runs the selected test case without opening it in the editor. </td></tr><tr><td colspan="2">Run as Batch Test </td><td>A sequence of test cases, packages, or an entire test suite can be run as a batch test. See <a href="user-manual.html#BatchTest">Running Batch Tests</a> for details. </td></tr><tr><td colspan="2">Edit </td><td>Opens the script as a tab for editing in the work area and lists its commands. </td></tr><tr><td colspan="2">Edit Details </td><td>Opens a dialog to edit script details like name, package, tags, description, base URL, test case-specific settings, or module parameters. </td></tr><tr><td rowspan="3">Refactor </td><td>Rename Script </td><td>Opens a dialog to rename the selected script. </td></tr><tr><td>Move Script(s) </td><td>Opens a dialog to move the selected script to another package. </td></tr><tr><td style="white-space:nowrap">Rename Package </td><td>Opens a dialog to rename the selected script package. </td></tr><tr><td colspan="2">Enable/Disable </td><td>Enables/disables the selected tests. Disabled tests will be skipped by batch tests and annotated with @Ignore when exported to Java. </td></tr><tr><td colspan="2">Export </td><td>Opens a dialog to define settings for exporting the script to Java (<em>XLT Scripting API</em> or <em>XLT Action API</em>). See <a href="user-manual.html#Exporting">Export Test Case to Java</a> for details. </td></tr><tr><td colspan="2">Manage Test Data </td><td>Opens a dialog to define test data you want to store in a separate file. See <a href="user-manual.html#Manage_data">Manage Test Data</a> for details. </td></tr><tr><td colspan="2">Delete </td><td>Deletes the selected script or package. </td></tr></table><h4 id="TestCaseDetails">Editing Details</h4><p>When choosing <em>Edit Details</em> from the context menu, a dialog opens that lets you edit details of the selected test case or module.</p><p class="illustration"><img alt="Edit Test Case Details" title="Edit Test Case Details" border="0" src="images/user-manual/ScriptDev_EditTestCaseDetails.png"/> <span class="caption">Edit Test Case Details</span></p><p class="illustration"><img alt="Edit Module Details" title="Edit Module Details" border="0" src="images/user-manual/ScriptDev_ExtractModule.png"/> <span class="caption">Edit Module Details</span></p><table><tr><th>Control </th><th>Description </th></tr><tr><td>Name </td><td>The name of the test case. However changing it requires to use the <em>Rename Script</em> refactoring operation from the context menu. </td></tr><tr><td>Script Package </td><td>The script package of the test case. However changing it requires to use the <em>Move Script(s)</em> refactoring operation from the context menu. </td></tr><tr><td>Tags </td><td>Tags to group several test cases and make them easier to find. Tags have to be separated by comma. The list of available test cases can be filtered by tags. </td></tr><tr><td>Description </td><td> A description of the test case.</td></tr><tr><td>Base URL </td><td>The base URL for the test case. This is only used if the base URL input field of the Script Developer toolbar is empty or contains an invalid URL. The base URL defined here is displayed in the toolbar in <em>italics</em> and can be overridden by entering a new value. </td></tr><tr><td>Enable test case specific settings </td><td>Enables the second checkbox <em>Generate JUnit wrapper class for test case</em> to define a test case-specific setting that overrides the global value. </td></tr><tr><td>Generate JUnit wrapper class for test case </td><td>If active, the global setting for generating JUnit wrapper classes will be overridden for the current test case by the value of this checkbox. See <a href="user-manual.html#Settings">Settings</a> for more information about the global setting. </td></tr><tr><td>Parameters </td><td>The parameters of the module. </td></tr></table><p>Depending on whether the selected element for editing its details is a test case or a module, the dialog elements <em>Enable test case specific settings</em>, <em>Generate JUnit wrapper class for test case</em>, and <em>Parameters</em> are shown or not shown.</p><h4 id="EditScript">Editing Scripts</h4><p>After opening a script (test case or module) as a tab in the work area, you can edit it by selecting one of the options from the context menu. A script may consist of actions, commands, and module calls.</p><p>The following options for editing are available (item = action, command or module call):</p><table><tr><td rowspan="3">Insert </td><td>Action </td><td>Inserts an action before the selected item. See <a href="user-manual.html#Actions">Actions</a> for details. </td></tr><tr><td>Command </td><td>Inserts a command before the selected item. Inserting a command is an alternative to recording them on the page. </td></tr><tr><td>Module </td><td>Inserts a module call before the selected item. Modules can also be inserted by dragging them from the script explorer to the script. </td></tr><tr><td colspan="2">Edit </td><td>Opens a dialog to edit the attributes of the selected item. Editing a module call lets you both edit its parameter values and replace the current module with another one. </td></tr><tr><td colspan="2">Extract Module </td><td>Saves the selected items as a module and replaces these items with a call to the new module. You can select several items by holding the <em>CTRL</em> key while clicking. Alternatively, hold the <em>SHIFT</em> key and select the commands with the arrow keys. </td></tr><tr><td colspan="2">Open Module </td><td>Opens the called script module in a separate tab of the work area for editing. </td></tr><tr><td colspan="2">Enable/Disable </td><td>Enables or disables the selected item. When a module call is disabled, all commands of that module will be disabled as well. </td></tr><tr><td colspan="2">Toggle Start Point </td><td>Sets/deletes a start point for replaying the script. If set, the replay starts at the marked command. </td></tr><tr><td colspan="2">Toggle Breakpoint </td><td>Sets/deletes a break point. If set, script execution will pause upon reaching the marked command. You can set multiple break points. </td></tr><tr><td colspan="2">Execute Command </td><td>Immediately executes the selected command in the currently active browser tab. </td></tr><tr><td colspan="2">Cut </td><td>Cuts the selected items from the script to insert them at another position in the same script or in another one. </td></tr><tr><td colspan="2">Copy </td><td>Copies the selected items to insert them at another position in the same script or in another one. </td></tr><tr><td colspan="2">Paste </td><td>Pastes the previously cut or copied items. </td></tr><tr><td colspan="2">Delete </td><td>Deletes the selected items. </td></tr></table><h4 id="EditTestCaseCommands">Editing Commands</h4><p>To edit a command, select <em>Edit</em> from the context menu. The <em>XLT &#8211; Edit Command</em> dialog opens:</p><p class="illustration"><a href="images/user-manual/ScriptDev_EditCommand.png"><img alt="Edit command" title="Edit command" border="0" src="images/user-manual/ScriptDev_EditCommand-small.png"/></a> <span class="caption">Edit Command</span></p><table><tr><th colspan="2">Control </th><th>Description </th></tr><tr><td colspan="2">Command </td><td>The name of the command to be executed (interacts with the page or executes an assertion). A drop-down box next to the command input lists all available commands with a small description. See <a href="user-manual.html#Commands">Appendix</a> for a full list of commands. </td></tr><tr><td colspan="2">Target </td><td>Defines the target element(s) to be addressed by the command. Not all commands require a target (e.g. <em>open</em>), so this may be empty. For each generated locator, Script Developer keeps a list of alternative locators in memory. If desired, you can later switch to another locator type (or xpath variant) by using the drop-down box next to the target input field. These alternatives are not part of the script file and cleared after reloading the test case. </td></tr><tr><td colspan="2">Resolved Target </td><td>Targets may contain placeholders, e.g. module parameters (@{...}), test data (${...}), and <a href="user-manual.html#Macros">macros</a>. <em>Resolved Target</em> is a read-only field that displays the target expression where all placeholders are resolved to actual characters. Variables are resolved recursively, so you can use variables within resolved content. </td></tr><tr><td colspan="2">Value </td><td>Sets a value for a command, e.g. the page title to be asserted or the text to be typed into an input element. Not all commands require a value (e.g. <em>check</em> or <em>click</em>). For commands like <em>assertText</em> or similar, you can define an explicit text-matching strategy. </td></tr><tr><td colspan="2">Resolved Value </td><td>Values may contain placeholders, e.g. module parameters (@{...}), test data (${...}), and <a href="user-manual.html#Macros">macros</a>. The <em>Resolved Value</em> field shows the value of the command with all placeholders resolved. Variables are resolved recursively, so you can use variables within resolved content. </td></tr><tr><td colspan="2">Comment </td><td>An optional comment for the command, used to document the purpose of this test step. </td></tr><tr><td colspan="2">Context Window </td><td>The window page to look upon. </td></tr><tr><td rowspan="2">Element Lookup </td><td>Elements found </td><td>When you enter a target expression in the <em>Target</em> field, Script Developer will analyze the window page and display the number of found elements matching this target. It will also display how many of them are visible at the moment, e.g. <em>Elements found: 3 (visible: 1)</em>. If more than one element can be found on the window page, the command will be executed for the first match. </td></tr><tr><td>Highlight Element(s) </td><td>Highlights all target elements found on the set window page. </td></tr><tr><td colspan="2">Evaluate </td><td>Clicking the <em>Evaluate</em> button executes the command and shows the result (Passed or Failed) considering the resolved target and resolved value. During script debugging and script execution, you can instantly evaluate assertions without executing the whole test case to see whether or not your verification expression matches. </td></tr></table><h4 id="Start_point">Start Point for Replaying Test Cases</h4><p>By default, the execution of a script starts at its beginning. Sometimes, however, it might come in handy to start it from a specific command. Select the respective command and press <em>S</em> to set a start point or, alternatively, choose <em>Set Start Point</em> from the context menu. A start point marker looking like the replay icon appears next to the command. Now click the <em>Play</em> icon in the toolbar to start the execution from the marked command.  </p><blockquote class="note"><p>Make sure the right page is displayed in the active Firefox tab so that the script can be started at the selected command. Otherwise, it is likely to fail.</p></blockquote><h4 id="Stop_point">Breakpoints</h4><p>To automatically pause the script execution at a certain command, you can set a breakpoint by selecting the respective command and pressing <em>B</em>. A breakpoint marker appears next to the command. When the script is being replayed, execution will automatically pause upon reaching the command. You can go on replaying the script by clicking <em>Continue Replay</em> or <em>Single Step Forward</em> in the toolbar. To clear the breakpoint, mark the command again and press <em>B</em>. A breakpoint may also be set or deleted using the context menu or by double-clicking the breakpoint column (the grey leftmost column). When you set a breakpoint for a command inside a module that is used multiple times, the respective command in each module call will be marked with a breakpoint and the replay will pause each time one of these module calls is reached.</p><blockquote class="note"><p>Breakpoints exist in memory only. Closing the appropriate script editor tab or reloading the script clears all of its breakpoints.</p></blockquote><h4 id="Actions">Actions</h4><p>As you may have noticed, Script Developer automatically inserts <em>Actions</em> while recording. An action is a sequence of steps that belong together. For example, filling in the inputs of a form, submitting the form by clicking the submit button, and checking the resulting page with assertions is typically one action. Actions are primarily used to break the page flow down into atomic steps and to give those steps a name. Action execution times are measured and reported while running load tests.</p><p>Script Developer gives actions generic names, but you can rename them to facilitate script maintenance. You can also manually insert a new action at any position of your script (<em>Insert &gt; Action</em>). The start of a new action automatically ends the previous action, even if the following action is part of a module call.</p><blockquote class="note"><p>Note that actions are not meant to structure your scripts visually since they are not comments. In other words, you should <em>not</em> use actions to structure a long list of validations without loading a new page. Using actions in this context conflicts with their <a href="user-manual.html#BasicConcepts">basic concept</a>, namely that they should always load one new page. The misuse of actions in script test cases makes it difficult to analyze the test results because actions then fail to represent the page flow. Also remember to give your actions meaningful names (e.g. <em>Search</em>,  <em>Browse</em>, or <em>AddToCart</em>).</p></blockquote><h3 id="Modules">Script Modules</h3><h4 id="ExtractingaModule">Extracting a Module</h4><p>Like test cases, script modules are sequences of commands, actions, and optional calls to other modules. They can be written from scratch, but it&#8217;s much easier to extract them from an existing test case. That means you record your test script first to identify reusable parts of that script afterwards. These parts are then factored out into separate modules.</p><p>To extract a module, do the following:   </p><ol><li>Open the original test case.</li><li>Select the groups of commands you want to extract by clicking them while holding the <em>CTRL</em> key.</li><li>Choose <em>Extract Module</em> from the context menu.</li><li>Provide a meaningful module name and the script package.</li><li>Optionally provide tags, a description, and module parameters.</li></ol><p>After confirming with the OK button, you can find the new module in the script explorer. Additionally, the original test case is modified now so that the selected items are replaced by a call to the new module.</p><p>Most editing options for test cases are available for modules as well, such as the editing of module properties and commands of a module. Therefore, <a href="user-manual.html#TestCaseProp">Edit Test Suite</a> and <a href="user-manual.html#EditTestCaseCommands">Editing Commands</a> also apply to modules. You can open a module in a separate tab and edit its commands there. It&#8217;s also possible to edit a module&#8217;s command from within a test case using this module. When you do so, keep in mind that editing a module&#8217;s command may also affect other test cases using this module. </p><p>The script explorer context menu offers two options for creating new modules. Select <em>New Script Module</em> to create a new script module from scratch as an alternative to extracting it from a test case. <em>New Java Module</em> creates a script interface to integrate Java code into a script test case for the purpose of running the test case outside Script Developer later, but will be skipped when creating the test case using Script Developer. See <a href="user-manual.html#JavaModules">Creating Java modules</a> for more information.</p><h4 id="DefiningModuleParameters">Defining Module Parameters</h4><p>You can parameterize modules to increase their reusability in other contexts and scenarios. For example, a module that logs in a user should be parameterized with the user name and password. Script module parameters are defined as part of the module&#8217;s meta-data. To use these parameters later in the module&#8217;s script code as target or value, you refer to them by using the special @{} placeholder notation (for example: @{userName} and @{password}) in the target and/or value of a command).</p><p>To parameterize a module, you need to: </p><ol><li>Select the module from the library and open the <em>Edit Details</em> dialog.</li><li>Declare the module parameters. You can add parameters with the <strong>+</strong> button and delete parameters with <strong>-</strong> (see figure &#8220;Edit Module Details&#8221; below).</li><li>After confirming the changes, open the module in an editor tab in the work area.</li><li>Edit the commands that should use any of the defined parameters. Where needed, replace literal values in the target or value of the command with @{name} placeholders (see figure &#8220;Refer to module parameters in a command&#8221; below).</li><li>Save the module.</li></ol><p class="illustration"><img alt="Edit Module Details" title="Edit Module Details" border="0" src="images/user-manual/ScriptDev_ExtractModule.png"/> <span class="caption">Edit Module Details</span></p><p id="Module_Parameters" class="illustration"><img alt="Refer to module parameters in a command" title="Refer to module parameters in a command" border="0" src="images/user-manual/ScriptDev_EditModuleCommand.png"/> <span class="caption">Refer to Module Parameters in a Command</span></p><p>For all test cases using this module, you have to provide a value for each placeholder:</p><ol><li>Open the test case in an editor tab.</li><li>Select <em>Edit</em> from the context menu of the module call inside the test case, then open the <em>Edit module</em> dialog.</li><li>Provide a valid value for each placeholder.</li><li>Close the <em>Edit module</em> dialog and save the test case.</li></ol><p class="illustration"><img alt="Provide Values for Module Parameters" title="Provide Values for Module Parameters" border="0" src="images/user-manual/ScriptDev_InsertModule.png"/> <span class="caption">Provide Values for Module Parameters</span></p><h4 id="ModulesAndActions">Modules and Actions</h4><p>You can extract a module from any part of the script, but there are some basic rules you should follow when extracting modules &#8211; especially when it comes to actions. Actions are important when you run load tests because load test reports are based on actions. Always keep in mind the <a href="user-manual.html#BasicConcepts">basic concepts</a> of XLT test cases. </p><p>In Script Developer, an action starts with an action line in the script and ends with the beginning of the next action, regardless of whether the next action is part of a module or not. </p><p>In most cases, modules are used more than once in a test suite. Be careful of actions contained in modules. If a module starts with a sequence of commands followed by an action, the commands of this module might be part of an action of a completely different module. This won&#8217;t cause your test scripts to break, but it may lead to confusion when analyzing load test reports.</p><p class="illustration"><a href="images/user-manual/hidden_actions_1.png"><img alt="Bad style: Module call in script test case with hidden action" title="Bad style: Module call in script test case with hidden action" border="0" src="images/user-manual/hidden_actions_1-small.jpg"/></a> <span class="caption">Bad style: Module call in a script test case with hidden action</span></p><p class="illustration"><a href="images/user-manual/hidden_actions_2.png"><img alt="Bad style: Hidden action in a module call" title="Bad style: Hidden action in a module call" border="0" src="images/user-manual/hidden_actions_2-small.jpg"/></a> <span class="caption">Bad style: Hidden Action in a Module Call</span></p><p>When extracting modules, it is recommended to follow these rules to facilitate test report analyzability:</p><ul><li>Modules should either contain no actions at all <strong>or</strong> start with an action if they do contain one or more actions.</li><li>If a module contains one or more actions, then the first line after the module call should also be an action.</li></ul><p>In other words, for each module you should decide if you</p><ul><li>use a module as a reusable sequence of commands that will always be part of one enclosing action <strong>or</strong></li><li>encapsulate one or more complete actions as a module.</li></ul><p class="illustration"><a href="images/user-manual/module_style_1.png"><img alt="Good style : Module calls in a script test case" title="Good style : Module calls in a script test case" border="0" src="images/user-manual/module_style_1-small.jpg"/></a> <span class="caption">Good style : Module Calls in Script Test Case</span></p><p class="illustration"><a href="images/user-manual/module_style_2.png"><img alt="Good style: Modules with no action at all or completely encapsulated actions" title="Good style: Modules with no action at all or completely encapsulated actions" border="0" src="images/user-manual/module_style_2-small.jpg"/></a> <span class="caption">Good style: Modules with no Action at all or Completely Encapsulated Actions</span></p><h3 id="Manage_data">Managing Test Data</h3><p>By default, test data is hard-coded into the test script. To simplify the changing of test data, the data values can be bound to variables and all occurrences can be replaced by references to the appropriate variables. These bounded test data values can be referred to as <em>test data mappings</em> since a test data variable is mapped to a certain value. All test data mappings of a script are managed using the separate data file <code>&lt;name&gt;_data.xml</code>, where <code>&lt;name&gt;</code> is the name of the corresponding script. For example, the test data file of the script <code>TSearch</code> is named <code>TSearch_data.xml</code> and stored next to the script file <code>TSearch.xml</code>.</p><p>To manage test data for a script test case or module, do the following: </p><ol><li>Select the script in the explorer.</li><li>Choose <em>Manage Test Data</em> from the context menu of the script explorer&#8217;s tree view, or alternatively press <em>ALT+D</em>. A dialog comes up where you can add new variable mappings, edit existing ones, or remove those you don&#8217;t want to use anymore. Note that you may also use this dialog to simply check what test data mappings exist and whether they override any default value.</li><li>Close the dialog. The data file is saved automatically.</li></ol><p class="illustration"><img alt="Manage Test Data" title="Manage Test Data" border="0" src="images/user-manual/ScriptDev_ManageTestData.png"/> <span class="caption">Manage Test Data</span></p><p>When you open the dialog the first time, it will show an empty map where you can add the mappings you want to use. Each mapping is represented as a 3-column row with the first column specifying the name of the test data variable and the third column specifying the value of the variable. The second column is a read-only field displaying the default value of the test data variable (global test data mapping) if one is set.</p><p>Each of the defined test data variables can be referenced several times in the scripts by using the <code>${...}</code> syntax. For example, <code>${foobar}</code> references the variable <code>foobar</code>.</p><p>As soon as you have a reference to a test data variable and you rename the variable in the <em>Manage Test Data</em> dialog, the reference is automatically adjusted to reflect the change(s) you&#8217;ve made.</p><p>Test data mappings may not only be specified for test case scripts but also for modules. If a module has a test data file and this module is called by a test case that also has a test data file, both test data files will be used to access the required values. If both test data files contain a test data mapping with the same name but different values, the test case data file will have priority. </p><blockquote class="note"><p>Test data variables are scoped, that is a script cannot access test variables defined by a called script. Furthermore, the calling script may overwrite test data variables defined by the called script. This forms a scope chain that is defined by the call hierarchy of your scripts. The scope of global test data variables is always the end of this scope chain.</p></blockquote><h4 id="GlobalTestData">Global Test Data</h4><p>Certain test data (such as user credentials or product data) are often the same for many or even for all test cases in a test suite. However, if the data needs to be changed, the test data mappings of all affected test cases would equally need to be adapted one by one. To minimize the effort of maintaining such data, you can define it as <em>global</em> test data.</p><p>Each of the global test data mappings is available to all of your scripts in the current test suite and their values serve as default values that can be overridden by a script if necessary. This way, you may also specify a test data fallback since a global test data value is used only when no script-specific test data value could be found.</p><p>To edit the global test data, go to <em>Script Developer</em> &gt; <em>Manage Global Test Data</em>. A dialog opens that is almost identical to the <em>Manage Test Data</em> dialog (see above), except that each test data mapping is represented as a 2-column row instead of a 3-column one.</p><p>All global test data is stored in the file <code>global_testdata.properties</code> located in the root directory of the respective test suite.</p><h3 id="Exporting">Exporting Test Cases to Java</h3><p>Scripting test cases using Script Developer is easy but limited. The scripts are strictly linear and the set of available commands can&#8217;t always replace a programming language. In certain cases, it is therefore useful to take advantage of powerful programming languages such as Java. Java provides both the structure to reuse code and a wide range of available libraries. Typically, you will have to switch to Java if your tests need to act randomly, for example, if you want the TBrowseCatalog test case to not always open the same catalog but a randomly chosen one instead. This behaviour is especially convenient for load tests meant to simulate realistic traffic.</p><p>The setting <em>Generate JUnit wrapper class for test cases</em> only lets you generate Java wrapper classes to execute the XML scripts from outside Script Developer, which is not a real export. </p><p>In contrast, <strong><em>Export</em></strong> translates the test-script file into Java syntax and generates one or more classes representing the test case and modules. When exporting to Java, you can choose the API used for the resulting code. You may either generate code based on the <a href="user-manual.html#ScriptingAPI">XLT Scripting API</a> or code based on the <a href="user-manual.html#XLTActionAPI">XLT Action API</a>. </p><p>To export a script test case or module to Java, follow these steps: </p><ol><li>Select the script(s) you want to export from the script explorer.</li><li>Choose <em>Export</em> from the context menu. The <em>XLT &#8211; Script Export</em> dialog opens.</li><li>Define the source directory, package prefix, and API (or accept the suggested values).</li><li>Click <em>OK</em>.</li></ol><p>As a result, you find the generated Java code in the specified packages. </p><p class="illustration"><img alt="Export Script to Java" title="Export Script to Java" border="0" src="images/user-manual/ScriptDev_Export.png"/> <span class="caption">Export Script to Java</span></p><ul><li><strong>Source Directory</strong>: The name of the directory where the generated Java source code is saved (<code>src</code> in most cases). The path is relative to the location of the current test suite.</li><li><strong>Package prefix</strong>: The package prefix for the generated Java classes. The resulting package is composed of this prefix and the script package of the exported test case.</li><li><strong>API</strong>: The API for the resulting Java code, <a href="user-manual.html#ScriptingAPI">XLT Scripting API</a> or <a href="user-manual.html#XLTActionAPI">XLT Action API</a>. </li></ul><blockquote class="note"><p>The <em>XLT Action API</em> does not support <a href="user-manual.html#JavaModules">Java modules</a>. When you export to <em>XLT Action API</em>, possible calls to Java modules are omitted in the resulting code and a comment is inserted instead.</p></blockquote><h4 id="ExporttoXLTScriptingAPI">Export to XLT Scripting API</h4><p>The following example shows the <em>TSearch</em> test case of the demo test suite exported to XLT Scripting API code:</p><pre class="java"><code>/**
 * Simulates storefront search including search result browsing.
 */
public class TSearch extends AbstractWebDriverScriptTestCase
{

    /**
     * Constructor.
     */
    public TSearch()
    {
        super(new XltDriver(true), "http://localhost:8080");
    }


    /**
     * Executes the test.
     *
     * @throws Throwable if anything went wrong
     */
    @Test
    public void test() throws Throwable
    {
        final OpenHomepage _openHomepage = new OpenHomepage();
        _openHomepage.execute();


        //
        // ~~~ Search-NoHits ~~~
        //
        startAction("Search_NoHits");
        // Store a search phrase that does not return any search results
        store(resolve("${searchTerm_noHits}"), "searchTerm");
        // Execute the search (module call)
        final Search _search = new Search();
        _search.execute(resolve("${searchTerm}"));

        // Assert presence of info maessage element
        assertElementPresent("id=infoMessage");
        // Validate the 'no results' message
        assertText("xpath=id('infoMessage')/div/strong", "*Sorry! No results found matching your search. Please try again.*");

        //
        // ~~~ Search ~~~
        //
        startAction("Search");
        // Store a search phrase that gives results
        store(resolve("${searchTerm_1}"), "searchTerm");
        // Execute the search (module call)
        _search.execute(resolve("${searchTerm}"));

        // Validate the entered search phrase is still visible in the input
        assertText("id=searchText", resolve("${searchTerm}"));
        // Validate presence of the search results page headline
        assertElementPresent("id=titleSearchText");
        // Validate the headline contains the search phrase
        assertText("id=titleSearchText", resolve("glob:*Your results for your search: '${searchTerm}'*"));

        //
        // ~~~ ViewProduct ~~~
        //
        startAction("ViewProduct");
        // Assert presence of one of the product thumbnails
        assertElementPresent("id=product0");
        // Store the name of the first product
        storeText("id=product0Name", "productName");
        // Click the product ilnk to open the product detail page
        clickAndWait("//*[@id='product0']//img");
        // Validate it's the correct product detail page 
        assertText("id=titleProductName", resolve("${productName}"));

        //
        // ~~~ Search ~~~
        //
        startAction("Search");
        // Store search phrase to repeat search
        store(resolve("${searchTerm_2}"), "searchTerm");
        // Execute the search (module call)
        _search.execute(resolve("${searchTerm}"));

        // Validate the entered search phrase is still visible in the input
        assertText("id=searchText", resolve("${searchTerm}"));
        // Validate presence of the search results page headline
        assertElementPresent("id=titleSearchText");
        // Validate the headline contains the search phrase
        assertText("id=titleSearchText", resolve("glob:*Your results for your search: '${searchTerm}'*"));

    }


</code></pre><p>This is the code for the module <em>Search</em>, which is called by <em>TSearch</em>: </p><pre class="java"><code>/**
 * Searches the specified term.
 */
public class Search extends AbstractWebDriverModule
{

    /**
     * {@inheritDoc}
     */
    @Override
    protected void doCommands(final String...parameters) throws Exception
    {
        final String searchTerm = parameters[0];
        // Enter the search phrse into the input
        type("id=searchText", searchTerm);
        // Cick the the search button to submit
        clickAndWait("id=btnSearch");

    }
}

</code></pre><p>Note that the XLT Scripting API Java code is very similar to the script code. You can refactor the code as you like and directly use lower-level APIs, such as the WebDriver API, to implement advanced functionality. You may also add code to randomise your tests or to retrieve test data from other sources, like the <code>GeneralDataProvider</code>. See <a href="user-manual.html#ScriptingAPI">XLT Scripting API</a> for details.</p><h4 id="ExporttoXLTActionAPI">Export to XLT Action API</h4><p>When you export to the XLT Action API, the resulting code is based on separate classes for each action of a test case. The script modules are also translated into separate classes. If a script module contains more than one action, it will be represented by a <a href="user-manual.html#Flow">flow</a>.    </p><p>To make this approach work smoothly, it&#8217;s very important that you follow the <a href="user-manual.html#ModulesAndActions">rules for defining actions in the context of modules</a>. If there is an action in a module and it&#8217;s not the first item of this module, then an automatically generated action will be inserted at the beginning of the exported module code.</p><h3 id="ExecutingTestsoutsideScriptDeveloper">Executing Tests outside Script Developer </h3><p>As soon as your test suite is complete, you can run the test cases outside Script Developer in headless mode. This especially comes in handy for functional tests and load tests as the XLT framework then is responsible for interpreting the script test cases and sending the respective requests to the system under test. </p><p>It is always recommended to have a JUnit wrapper class for each script test case. This facilitates working with test scripts in your preferred IDE since these tools generally know how to work with JUnit classes.</p><h4 id="InsideanIDE">Inside an IDE</h4><p>To check whether your scripts are running successfully in headless mode, open the test project in your favorite IDE, navigate to the JUnit test classes, choose one of them, and then run it as JUnit test.</p><h4 id="WithinBuildScripts">Within Build Scripts </h4><p>To run all or selected script test cases as part of your functional tests, you need to make the corresponding JUnit classes available to your test framework, such as a <code>junit</code> Ant task.</p><p>Typically, it&#8217;s sufficient to add the script test case wrapper classes to the list of classes you want to be run by JUnit. If you prefer not to have a wrapper class for each test script, add the class <code>com.xceptance.xlt.api.engine.scripting.ScriptTestCaseSuite</code> to the list of JUnit classes. It is a generic representative for a set of test scripts. To tell the suite class which scripts are to be executed, use the property <code>com.xceptance.xlt.api.engine.scripting.ScriptTestCaseSuite.testCases</code> and list the script names as below:</p><pre class="plain"><code>com.xceptance.xlt.api.engine.scripting.ScriptTestCaseSuite.testCases = TSearch TAddToCart

</code></pre><p>In any case, each test script is executed once and the results are part of the JUnit test report.  </p><blockquote class="note"><p>See the demo test suite in directory <code>&lt;xlt&gt;/samples/testsuite-posters</code> for an example of how to configure Ant&#8217;s <code>junit</code> task to run script test cases. </p></blockquote><h4 id="AsaLoadTest">As a Load Test</h4><p>Performing load tests with your script test cases is easy. Register the JUnit classes with the XLT load test framework and use them in your load tests like regular Java-based tests. Note that if you do so, you need a JUnit wrapper around your test script. See <a href="user-manual.html#LoadTest">Load Testing</a> for details. </p><h3 id="AdvancedTopics">Advanced Topics</h3><h4 id="FileGeneration">File Generation</h4><p>The figure below shows the different types of files generated by Script Developer when wrapper class generation is enabled. The files generated during the export to Java are not displayed.</p><p class="illustration"><a href="images/user-manual/big-picture.png"><img alt="Script Developer File Generation" title="Script Developer File Generation" border="0" src="images/user-manual/big-picture-small.jpg"/></a> <span class="caption">Script Developer File Generation</span></p><p>In the following, the sample test case <em>TSearch</em> (available in the demo application <em>testsuite-posters</em>) serves as a reference to describe all of these file types in detail. </p><h5 id="TestScriptTSearch.xml">Test Script (TSearch.xml)</h5><p>All the script files recorded by Script Developer are saved in XML format following the naming convention <em>Testcasename.xml</em>. With reference to our sample test case, <em>TSearch.xml</em> is the test case script file that consists of all the commands and modules calls being used. The syntax is very simple and easy to understand. See below for an example:</p><pre class="xml"><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;testcase xmlns="http://xlt.xceptance.com/xlt-script/2" version="4" baseURL="http://localhost:8080"&gt;
  &lt;description&gt;Simulates storefront search including search result browsing.&lt;/description&gt;
  &lt;module name="posters.functional.modules.OpenHomepage"/&gt;
  &lt;action name="Search-NoHits"/&gt;
  &lt;command name="store" target="${searchTerm_noHits}" value="searchTerm"&gt;
    &lt;comment&gt;Store a search phrase that does not return any search results&lt;/comment&gt;
  &lt;/command&gt;
  &lt;module name="posters.functional.modules.Search"&gt;
    &lt;comment&gt;Execute the search (module call)&lt;/comment&gt;
    &lt;parameter name="searchTerm" value="${searchTerm}"/&gt;
  &lt;/module&gt;
  &lt;command name="assertElementPresent" target="id=infoMessage"&gt;
    &lt;comment&gt;Assert presence of info maessage element&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="assertText" target="xpath=id('infoMessage')/div/strong" value="*Sorry! No results found matching your search. Please try again.*"&gt;
    &lt;comment&gt;Validate the 'no results' message&lt;/comment&gt;
  &lt;/command&gt;
  &lt;action name="Search"/&gt;
  &lt;command name="store" target="${searchTerm_1}" value="searchTerm"&gt;
    &lt;comment&gt;Store a search phrase that gives results&lt;/comment&gt;
  &lt;/command&gt;
  &lt;module name="posters.functional.modules.Search"&gt;
    &lt;comment&gt;Execute the search (module call)&lt;/comment&gt;
    &lt;parameter name="searchTerm" value="${searchTerm}"/&gt;
  &lt;/module&gt;
  &lt;command name="assertText" target="id=searchText" value="${searchTerm}"&gt;
    &lt;comment&gt;Validate the entered search phrase is still visible in the input&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="assertElementPresent" target="id=titleSearchText"&gt;
    &lt;comment&gt;Validate presence of the search results page headline&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="assertText" target="id=titleSearchText" value="glob:*Your results for your search: '${searchTerm}'*"&gt;
    &lt;comment&gt;Validate the headline contains the search phrase&lt;/comment&gt;
  &lt;/command&gt;
  &lt;action name="ViewProduct"/&gt;
  &lt;command name="assertElementPresent" target="id=product0"&gt;
    &lt;comment&gt;Assert presence of one of the product thumbnails&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="storeText" target="id=product0Name" value="productName"&gt;
    &lt;comment&gt;Store the name of the first product&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="clickAndWait" target="//*[@id='product0']//img"&gt;
    &lt;comment&gt;Click the product ilnk to open the product detail page&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="assertText" target="id=titleProductName" value="${productName}"&gt;
    &lt;comment&gt;Validate it's the correct product detail page &lt;/comment&gt;
  &lt;/command&gt;
  &lt;action name="Search"/&gt;
  &lt;command name="store" target="${searchTerm_2}" value="searchTerm"&gt;
    &lt;comment&gt;Store search phrase to repeat search&lt;/comment&gt;
  &lt;/command&gt;
  &lt;module name="posters.functional.modules.Search"&gt;
    &lt;comment&gt;Execute the search (module call)&lt;/comment&gt;
    &lt;parameter name="searchTerm" value="${searchTerm}"/&gt;
  &lt;/module&gt;
  &lt;command name="assertText" target="id=searchText" value="${searchTerm}"&gt;
    &lt;comment&gt;Validate the entered search phrase is still visible in the input&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="assertElementPresent" target="id=titleSearchText"&gt;
    &lt;comment&gt;Validate presence of the search results page headline&lt;/comment&gt;
  &lt;/command&gt;
  &lt;command name="assertText" target="id=titleSearchText" value="glob:*Your results for your search: '${searchTerm}'*"&gt;
    &lt;comment&gt;Validate the headline contains the search phrase&lt;/comment&gt;
  &lt;/command&gt;
&lt;/testcase&gt;
</code></pre><h5 id="TestDataTSearchdata.xml">Test Data (TSearch_data.xml)</h5><p>Separating test data from script code by extracting the recorded values into data files is a very useful feature. The naming convention being used is <em>Testcasename_data.xml</em>, so <em>TSearch_data.xml</em> represents the data file of the <em>TSearch</em> test case. A data file looks like this:</p><pre class="xml"><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;data xmlns="http://xlt.xceptance.com/xlt-script-data"&gt;
  &lt;searchTerm_1&gt;bear&lt;/searchTerm_1&gt;
  &lt;searchTerm_2&gt;red bear&lt;/searchTerm_2&gt;
  &lt;searchTerm_noHits&gt;gjP#IQ!&lt;/searchTerm_noHits&gt;
&lt;/data&gt;
 
</code></pre><h5 id="JUnitTestCaseWrapperTSearch.java">JUnit Test Case Wrapper (TSearch.java)</h5><p>This file is generated when you enable wrapper class generation for your test cases. The generation of wrapper classes lets you run your script files outside the browser via the XLT framework that simulates a headless browser. This mode suits unattended test case execution, such as functional or load testing.</p><h5 id="CustomModuleComplexUserLoggedInCheck.java">Custom Module (ComplexUserLoggedInCheck.java)</h5><p>If some special constructs can&#8217;t be expressed due to the basic script syntax, you may create your own custom modules and use them inside your scripts. Custom modules are implemented in Java. See <a href="user-manual.html#JavaModules">Using Java Modules</a> for an example.</p><h4 id="Macros">Macros</h4><p>When you create and run regression tests with Script Developer, you sometimes need to have unique data available to run a test over and over again. For example, user account creation always requires a new email address since the target system accepts each address only once.</p><p>For the generation of timestamps and random strings or numbers, the following macros are available:</p><ul><li><strong>NOW</strong>: returns the current time as time stamp (number of milliseconds elapsed since 1970-01-01)</li><li><strong>RANDOM.String(length)</strong>: returns a random string of the given length</li><li><strong>RANDOM.Number(max)</strong>: returns a randomly chosen integer between <em>0</em> (inclusive) and <em>max</em> (exclusive)</li><li><strong>RANDOM.Number(min,max)</strong>: returns a randomly chosen integer between <em>min</em> (inclusive) and <em>max</em> (inclusive)</li></ul><p>With the <strong>${variable}</strong> notation, you can use macros in any command. If you want to input a random or unique email address, you could use one of the code lines below as a value in the <em>type</em> command: </p><pre class="plain"><code>${RANDOM.String(5)}@anyserver.com
</code></pre><p>or</p><pre class="plain"><code>${NOW}@anyserver.com
</code></pre><p>The resulting email addresses might look like <em>zghfu@anyserver.com</em> or <em>1295519733483@anyserver.com</em>.</p><p>If you use one of these macros as a value for a module parameter, the random string or time stamp is created once for a test run and can then be used in several commands of the module with an identical value. For example, you can fill in a form with a random name, submit the form, and then validate the name on the confirmation page. This will only work if filling in the form, submitting it, and the validation are all part of the same module. The name has to be defined as a module parameter and referenced in the relevant commands with <em>@{name}</em>, where <em>${RANDOM.String(8)}</em> is the provided value for the module parameter <em>name</em>.</p><h4 id="JavaModules">Creating Java Modules</h4><p>The commands supported by Script Developer are sufficient to create working test scripts. However, there are some special constructs that can&#8217;t be expressed due to its restricted script syntax, for example:  </p><ul><li>Combining assertions by a logical OR.</li><li>Performing advanced assertions which are not available as commands yet.</li><li>Accessing and validating downloaded files.</li></ul><p>To overcome such limitations, you can create custom modules for test cases meant to run outside Script Developer (as a load test, as JUnit test in Eclipse, or integrated in a build process). The Java modules are skipped when you run the test case in Script Developer.</p><p>Custom modules are written in Java by implementing the <code>WebDriverCustomModule</code> interface. This interface forces you to apply the required <code>execute</code> method. See below for an example:</p><pre class="java"><code>public class ComplexUserLoggedInCheck implements WebDriverCustomModule
{
    public void execute(final WebDriver webDriver, final String... parameters)
    {
        final WebElement webElement = webDriver.findElement(By.xpath("id('sidebar')/div[1]/div[1]/span"));
        final String userName = parameters[0];

        Assert.assertTrue("Expected user name not found: " + userName, webElement.getText().contains(userName));
    }
}

</code></pre><p>Make sure the custom module class is compiled and made available on your test suite&#8217;s class path.</p><p>To integrate Java custom modules into test scripts, you first need to register them with Script Developer. You do so by creating a new Java module script:</p><ol><li>Choose <em>New | Java Module</em> from the script explorer context menu. The <em>Edit Module Details</em> dialog comes up (see the figure below). </li><li>Fill out the necessary fields as described below.</li><li>Close the dialog. </li></ol><p>The new Java module is now available in the script explorer and can be used in other scripts (test cases or modules) just like any other script module. Note that Java modules can&#8217;t be edited and will be skipped when you run the test case inside Script Developer.</p><p id="NewJavaModule" class="illustration"><img alt="New Java Module" title="New Java Module" border="0" src="images/user-manual/ScriptDev_NewJavaModule.png"/> <span class="caption">New Java Module</span> </p><ul><li><strong>Name</strong>: The name of the new Java module.</li><li><strong>Script package</strong>: The script package for the new Java module. </li><li><strong>Full Class Name</strong>: The full class name (including package) of the Java class implementing the <code>WebDriverCustomModule</code> interface.</li><li><strong>Tags</strong>: Tags to group modules and make them easier to find. Tags have to be separated by a comma. The list of available modules can be filtered by tags.</li><li><strong>Description</strong>: A description of the module.</li><li><strong>Parameters</strong>: The name of the parameters the module expects when being executed.</li></ul><p>If you&#8217;ve defined module parameters, you need to provide a value for each parameter when using the Java module in a test case. You access the first of these parameters in your Java code by reading the value from <code>parameters[0]</code>, the second one by reading the value from <code>parameters[1]</code>, and so on. The name of the parameters defined in the Java module script of Script Developer is not displayed in Java code; only the order of the parameters is relevant. </p><p>Note that you should use Java modules only if absolutely necessary because their execution will be skipped when you run the test case in Script Developer. If Script Developer comes across a Java module while replaying a script, it simply ignores it (the status icon turns grey) and continues with the next command. The next command might or might not succeed, depending on what the module does with the page. If the module leaves the page unchanged (that is it performs advanced validations only), the rest of the script will run successfully. However, if the module does change the page or loads a new page, the commands following the module are likely to fail. That&#8217;s why Java modules are mainly used for complex validations. </p><blockquote class="note"><p>Feel free to let us know about extensions you create because we might include them in a future release.</p></blockquote><h4 id="AssertionsonFormControls">Assertions on Form Controls</h4><p>When making assertions on form controls, it&#8217;s very helpful to know when to choose which kind of assertion. To be more specific, for a given form control you have to decide whether to use one of the <code>*Value</code> commands or one of the <code>*Text</code> commands.</p><p>For each affected form control, the table below lists the input that will be used when checking the element either by a <code>*Value</code> or a <code>*Text</code> assertion. The input can be the element&#8217;s text content <em>T</em> or the element&#8217;s <code>value</code> attribute <em>V</em>.</p><table><tr><th></th><th style="text-align: left;"><code>*Value</code> </th><th style="text-align: left;"><code>*Text</code> </th></tr><tr><td>textarea </td><td><em>T</em> </td><td><em>T</em> </td></tr><tr><td>text-input </td><td><em>V</em> </td><td><em>T</em> </td></tr><tr><td>submit-input </td><td><em>V</em> </td><td>- </td></tr><tr><td>button </td><td><em>V</em> </td><td><em>T</em> </td></tr><tr><td>option </td><td><em>V</em> (<em>T</em> if <code>value</code> attribute not set) </td><td><em>T</em> </td></tr><tr><td>radio-input </td><td><em>V</em> </td><td>- </td></tr><tr><td>checkbox-input </td><td><em>V</em> </td><td>- </td></tr></table><h2 id="XLTFramework2">XLT Framework </h2><h3 id="BasicConcepts">Basic Concepts</h3><p>When you perform web-based application tests, the possible paths from page to page define the web page flow. The web page flow can be depicted as a directed graph with the vertices representing the web pages and the transitions representing the actions to get from one page to another. Typically, a test scenario covers a certain part of the page flow only, such as a specific path through the application.</p><p>XLT provides a programming paradigm that makes use of a three-level architecture (transactions, actions, and requests). These levels are illustrated in the following sections.</p><h4 id="Transaction">Transaction</h4><p>A transaction is the execution of exactly one test case or test scenario. To perform the scenario, the page flow is modeled in code. The test scenario is implemented as a test case which itself executes a sequence of one or more actions.</p><h4 id="action">Action</h4><p>An action can be defined as one irreducible step within a test case. Thus, an action interacts with the <strong>current</strong> page and &#8211; as a result &#8211; loads the <strong>next</strong> page. The resulting page is associated with this action and becomes the <strong>current</strong> page for the next action in the test scenario. Generally, an action triggers one or more requests.</p><h4 id="Request">Request</h4><p>This level is equivalent to the HTTP request level used in web browsers or in any other application that relies on HTTP communication. You don&#8217;t have to deal with requests directly because they are automatically generated by the underlying HtmlUnit framework when you perform actions on HTML elements.</p><h4 id="Validation">Validation</h4><p>As you&#8217;re testing the functionality of applications or pieces of software, you have to check the correctness of all responses. It is strongly recommended that you handle all potential situations and use validations as often as possible. It&#8217;s better to have too many checks rather than too few! They can&#8217;t do any harm and will increase your confidence that your application works correctly. Thus, make sure you insert as much validation as necessary to detect any abnormal application behavior of the software being tested.</p><h5 id="Prevalidation">Pre-validation</h5><p>Each action should have a pre-validation section that checks whether or not all of the required data is available to interact with that page and allow for the advance to the next one. From the end user&#8217;s point of view, you simply look for the information on the page that you need to continue your web experience, such as a form to fill in or a link to click. In case the required information can&#8217;t be found, an exception is thrown. If you run a load test, XLT will catch this exception and log all relevant information. This lets you evaluate the results after running the test and narrow down error conditions.</p><h5 id="Postvalidation">Post-validation</h5><p>Post-validations work similarly to pre-validations. They are used to validate the result of the interaction and ensure that the data matches the expectations.</p><h4 id="Example">Example</h4><p>The following example illustrates a very simple scenario to help you understand the terminology. It is based on the <a href="user-manual.html#PosterDemo">demo application</a> shipped with XLT. Imagine a typical user searching the shop for products. The user will possibly: </p><ul><li>Open the shop&#8217;s home page,</li><li>Search for a keyword but without results,</li><li>Search for another keyword, this time with some results</li><li>Select one of the shown products and open the product detail page</li></ul><p class="illustration"><a href="images/user-manual/TSearchPageFlow.png"><img alt="Test Case And Actions" title="Test Case And Actions" border="0" src="images/user-manual/TSearchPageFlow-small.png"/></a> <span class="caption">Test Case and Actions</span></p><p>In this example, the <strong>test scenario</strong> is modeled as <strong>test case</strong> <code>TSearch</code>. A single execution of this test case is a single <strong>transaction</strong>. <code>OpenHomepage</code>, <code>Search</code> and <code>ViewProductDetails</code> are the <strong>actions</strong> of this page flow to go from one page to the next. <strong>Validations</strong> after each page transition ensure you arrived on the right page with the right content.</p><h3 id="AvailableAPI">Available Programming Interfaces</h3><h4 id="Overview">Overview</h4><p>XLT offers different approaches for writing test cases in Java. Several programming interfaces are available when using the XLT framework. Built on each other, they represent different abstraction levels.</p><p>You can extend your test suite using the <a href="user-manual.html#ScriptingAPI">XLT Scripting API</a>. It lets you start with Java code that has automatically been generated from the recorded Script Developer test cases, and features a high-level command scripting API with a very intuitive syntax. You may also directly access the <a href="user-manual.html#WebDriverAPI">WebDriver API</a> underlying the XLT Scripting API to write more advanced tests.  </p><p>XltDriver, which is also part of the XLT framework, is a WebDriver implementation extending the HtmlUnitDriver. Both in turn are built on the <a href="http://htmlunit.sourceforge.net/">HtmlUnit API</a>. HtmlUnit is a headless browser offering a low-level API that lets you have full control when creating web tests.</p><p>To serve as the main framework when creating HtmlUnit-based tests, XLT provides the <a href="user-manual.html#XLTActionAPI">XLT Action API</a> that structures the code in action classes and test case classes.</p><p class="illustration"><a href="images/user-manual/XLT_Framework_API.PNG"><img alt="XLT Framework API" title="XLT Framework API" border="0" src="images/user-manual/XLT_Framework_API-small.jpg"/></a> <span class="caption">XLT Framework API</span></p><h4 id="XLTTestCasesareJUnit4Tests">XLT Test Cases are JUnit4 Tests</h4><p>Except for script test cases exclusively springing from Script Developer, XLT test cases use a Java test case class with one <code>test()</code> method, regardless of the chosen approach for test writing.</p><p>The <code>test()</code> method of each test case class has a <em>@Test</em> annotation (see <a href="#ScriptingAPIExample">TSearch code example</a>  line 12). XLT builds upon JUnit4 principles and its annotations to implement and tag test cases. This way, each XLT test is in fact a JUnit test enabling XLT tests to be executed just like any other unit test in the IDE or within an existing build process. The sole difference between XLT and standard JUnit4 tests is that XLT tests can only take one active test method per test class. That means that, although there can be an arbitrary number of methods within a class, only one method is permitted to be annotated with <em>@Test</em>. However, this limitation rather serves the purpose of simplification than leading to actual restrictions. </p><p>Implementing the test case as a JUnit4 test also lets you use standard JUnit assertion to validate the page, mainly when you create test cases using the HtmlUnit API. </p><h3 id="ScriptingAPI">XLT Scripting API</h3><p>When exporting a Script Developer test case to Java (see <a href="#Exporting">Export test cases to Java</a>), you may choose the resulting code to be based on the XLT Scripting API. This API is an easy-to-use programming interface with a simple syntax deriving from the <a href="user-manual.html#Commands">commands</a> available in Script Developer.</p><p>You can write test cases from scratch using the XLT Scripting API. However, the most common way is to record a test case with Script Developer and, after exporting it to Java, extend it with the XLT Scripting API.</p><p>The following screenshot shows how the test case <code>TSearch</code> introduced in the section above may look like if it would have been recorded using Script Developer. Note that this test case uses validation rather poorly to keep the example short and simple; a real test should have more validations to ensure correct page display.</p><p class="illustration"><a href="images/user-manual/TSearchScriptTestCase.png"><img alt="TSearch Demo Script Test Case" title="TSearch Demo Script Test Case" border="0" src="images/user-manual/TSearchScriptTestCase-small.png"/></a> <span class="caption">TSearch Demo Script Test Case</span> </p><p id="ScriptingAPIExample">The automatically generated Java code of the <code>TSearch</code> test case after the export is shown below:</p><pre class="java"><code>/**
 * Simulates storefront search including search result browsing.
 */
public class TSearch extends AbstractWebDriverScriptTestCase
{

    /**
     * Constructor.
     */
    public TSearch()
    {
        super(new XltDriver(true), "http://localhost:8080");
    }


    /**
     * Executes the test.
     *
     * @throws Throwable if anything went wrong
     */
    @Test
    public void test() throws Throwable
    {
        // Open the homepage and delete cookies (module call)
        final OpenHomepage _openHomepage = new OpenHomepage();
        _openHomepage.execute();


        //
        // ~~~ Search-NoHits ~~~
        //
        startAction("Search_NoHits");
        // Store a search phrase that does not return any search results
        store(resolve("${searchTerm_noHits}"), "searchTerm");
        // Execute the search (module call)
        final Search _search = new Search();
        _search.execute(resolve("${searchTerm_hits}"));

        // Assert presence of info maessage element
        assertElementPresent("id=infoMessage");
        // Validate the 'no results' message
        assertText("xpath=id('infoMessage')/div/strong", "*Sorry! No results found matching your search. Please try again.*");

        //
        // ~~~ Search ~~~
        //
        startAction("Search");
        // Store a search phrase that gives results
        store(resolve("${searchTerm_hits}"), "searchTerm");
        // Execute the search (module call)
        _search.execute(resolve("${searchTerm_hits}"));

        // Validate the entered search phrase is still visible in the input
        assertText("id=searchText", resolve("${searchTerm_hits}"));
        // Validate presence of the search results page headline
        assertElementPresent("id=titleSearchText");
        // Validate the headline contains the search phrase
        assertText("id=titleSearchText", resolve("glob:*Your results for your search: '${searchTerm_hits}'*"));

        //
        // ~~~ ViewProduct ~~~
        //
        startAction("ViewProduct");
        // Assert presence of one of the product thumbnails
        assertElementPresent("id=product0");
        // Store the name of the first product
        storeText("id=product0Name", "productName");
        // Click the product ilnk to open the product detail page
        clickAndWait("//*[@id='product0']//img");
        // Validate it's the correct product detail page
        assertText("id=titleProductName", resolve("${productName}"));

    }

</code></pre><p>Note that the code is structured in blocks embodying the actions. Each action starts with a <code>startAction()</code> command. The other commands are very similar to the commands available in Script Developer. Each Script Developer command has an appropriate counterpart in the XLT Scripting API. The generated test case class extends the abstract class <code>AbstractWebDriverScriptTestCase</code>; it inherits the methods that represent the scripting commands to interact with the page and perform validations. </p><p>See package <code>com.xceptance.xlt.api.engine.scripting</code> for more information about the available scripting commands of the XLT Scripting API.</p><h4 id="ScriptModules">Script Modules</h4><p>You may have noticed that a <em>Search</em> action in <code>TSearch</code> appears twice, first time with no hits and then again with hits. To prevent implementing the search<br/>related steps twice Script Developer allows to extract parts of the script as a module &#8211; a convenient feature that, in the present example, can be easily applied to the <em>Search</em> action. </p><p>When exporting the script to Java, XLT generates a class for each of the modules that extends <code>AbstractWebDriverScriptModule</code> and extracts the code to reuse it in the test case. Using the <code>execute()</code> method, the module can be called in the test case after creating an instance of the module class. It also possible to pass parameter to a module.</p><p>Modules can embed submodules, that is they can be called by test cases or by other modules. That&#8217;s why the module class needs two different constructors, one that takes a test case and another one that takes a module as parameter.</p><h3 id="WebDriverAPI">WebDriver API</h3><p><a href="http://seleniumhq.org/docs/03_webdriver.html#selenium-webdriver-api-commands-and-operations">WebDriver</a> is a tool for automated tests of web applications. It was originally introduced by Google and features a simple and efficient API permitting control of real web browsers, such as Firefox, Internet Explorer, Safari, and the HtmlUnit headless browser. </p><p>The XLT framework incorporates the WebDriver API so that external test cases using the WebDriver API can generally run in the XLT framework as well. XLT-based WebDriver tests can&#8217;t be executed with a stand-alone WebDriver because the XLT-WebDriver API integrates the concept of action names.</p><p>When being exported from Script Developer, the generated Java code originates from the XLT Scripting API, which in turn is built upon WebDriver. When you look at the <a href="user-manual.html#ScriptingAPI">TSearch example</a>, you may notice that its strictly linear approach is limited. Using the lower-level WebDriver API in combination with the XLT scripting API is a good way to overcome possible limitations.</p><p>When you run the <code>TSearch</code> example as functional or load test, your test scenario actually turns out to be somewhat unrealistic since the we open the product details page always for the same product. Randomising the <em>ViewProductDetails</em> action would allow you to browse to different products and select the posters randomly.</p><p>The exported code uses an <code>XltDriver</code> to simulate users. Additionally, the WebDriver API lets you easily switch to other WebDrivers to imitate real-world browsers, like Chrome, FirefoxDriver, or Internet Explorer. The XltDriver extends the HtmlUnitDriver implementation of the WebDriver API.</p><p>The following code is an example of how to use the WebDriver API to introduce random factors. The WebDriver functionality is refactored into a method allowing it to be reused for random product selection. The example also contains a JUnit4 assertion and shows how you can use standard Java functionality.</p><p id="TBlogVisitorWebDriver">The refactored code described above could look like this:</p><pre class="java"><code>/**
 * Simulates storefront search including search result browsing.
 */
public class TSearch extends AbstractWebDriverScriptTestCase
{

    /**
     * Constructor.
     */
    public TSearch()
    {
        super(new XltDriver(true), "http://localhost:8080");
    }

   /**
    * This method randomly picks one of the web elements that match the given xpath
    * It uses the WebDriver API and a JUnit assertion
    */ 
    private WebElement findWebElementsAndPickOne(String xpath)
    {
        // get all elements that match the given expression
        final List&lt;WebElement&gt; articleLinks = getWebDriver().findElements(By.xpath(xpath));

        //Make sure there is at least one element; This is a pure JUnit assertion
        Assert.assertFalse("No elements found", articleLinks.isEmpty());

        // grab one of the elements randomly
        WebElement element = articleLinks.get(XltRandom.nextInt(articleLinks.size()));

        //return the element
        return element;	
    }

    /**
     * Executes the test.
     *
     * @throws Throwable if anything went wrong
     */
    @Test
    public void test() throws Throwable
    {
        // Open the homepage and delete cookies (module call)
        final OpenHomepage _openHomepage = new OpenHomepage();
        _openHomepage.execute();


        //
        // ~~~ Search-NoHits ~~~
        //
        startAction("Search_NoHits");
        // Store a search phrase that does not return any search results
        store(resolve("${searchTerm_noHits}"), "searchTerm");
        // Execute the search (module call)
        final Search _search = new Search();
        _search.execute(resolve("${searchTerm_hits}"));

        // Assert presence of info message element
        assertElementPresent("id=infoMessage");
        // Validate the 'no results' message
        assertText("xpath=id('infoMessage')/div/strong", "*Sorry! No results found matching your search. Please try again.*");

        //
        // ~~~ Search ~~~
        //
        startAction("Search");
        // Store a search phrase that gives results
        store(resolve("${searchTerm_hits}"), "searchTerm");
        // Execute the search (module call)
        _search.execute(resolve("${searchTerm_hits}"));

        // Validate the entered search phrase is still visible in the input
        assertText("id=searchText", resolve("${searchTerm_hits}"));
        // Validate presence of the search results page headline
        assertElementPresent("id=titleSearchText");
        // Validate the headline contains the search phrase
        assertText("id=titleSearchText", resolve("glob:*Your results for your search: '${searchTerm_hits}'*"));

        //
        // ~~~ ViewProduct ~~~
        //
        startAction("ViewProduct");
        //Find all product links by xpath, pick one randomly and click the link
        findWebElementsAndPickOne("id('productOverview')/div/ul[@class='thumbnails']/li/div/a").click();

    }
 

 
 
 

</code></pre><p>See file <code>&lt;XLT&gt;/doc/apidoc.zip</code> for a full documentation of the XLT framework, including information on the WebDriver support.</p><h3 id="XLTActionAPI">XLT Action API</h3><p>Besides the components described above, the XLT framework supports one more approach for modeling test scenarios in Java code. The test cases and action classes can be implemented in Java from scratch or they may be generated automatically while being exported from Script Developer. This approach is based on special action classes and makes use of the <em>XLT Action API</em>.</p><p>To test web-based applications, the XLT Action API relies on the HtmlUnit framework, which also includes the Mozilla Rhino engine for JavaScript support. Web tests are executed by a low-level web browser simulation. Basically, this means that &#8211; as in real browsers &#8211; a  web page is translated into a DOM (Document Object Model) tree. Analysis, validation, and any other access is performed afterwards on this DOM tree construction.</p><p>The XLT Action API provides a programming paradigm to translate a test scenario into a unit test. The test scenario is implemented as a test case class which itself executes a sequence of one or more actions.</p><h4 id="TestCaseandActionClasses">Test Case and Action Classes</h4><p>All test case classes should inherit the abstract class <code>AbstractTestCase</code> which supplies some basic features, like logging the test results to disk and easy access to properties.</p><p>As a test case models a transaction and as transactions rely on actions, defining the appropriate actions is the first step.</p><p>All actions must inherit the abstract class <code>AbstractAction</code> which forces you to implement the three methods <code>execute()</code>, <code>preValidate()</code>, and <code>postValidate()</code>. As mentioned earlier, the <code>preValidate()</code> and <code>postValidate()</code> methods perform validations before and after the execution of that action itself. Therefore, the call sequence of an action generated by the XLT framework is always:</p><ol><li><code>preValidate()</code></li><li><code>execute()</code></li><li><code>postValidate()</code></li></ol><p>This call sequence will be executed exactly once when the instance method <code>run()</code> of an action is called.</p><p>Note that the XLT Action API forces you to implement the validation methods and that is the whole purpose of testing: validating data. Therefore, implementing the abstract validation methods in a non-trivial way (that is not leaving them empty) is strongly recommended. Otherwise, you will sacrifice test quality.</p><p>Each of the three methods may throw an exception which always indicates a problem. To check if an action can be executed safely, the abstract class <code>AbstractAction</code> provides a method called <code>preValidateSafe()</code>. This method internally calls <code>preValidate()</code> and catches any thrown exception. If no exception is thrown, <code>preValidateSafe()</code> returns true; otherwise it returns false. This helps you determine if the prerequisites are fulfilled to continue the page flow in a certain direction. A simple example is the flow through a catalog with nested categories. As you don&#8217;t know the nesting level up-front when you create a dynamic and random test, it might be necessary to call <code>preValidateSafe()</code> before trying to go to the next level of categories. </p><p>Note that <code>AbstractAction</code> doesn&#8217;t offer any web support. Therefore, any web-based test should inherit the abstract class <code>AbstractHtmlPageAction</code>, which is a specialization of <code>AbstractAction</code> and which does offer support for web testing.</p><h4 id="Validation2">Validation</h4><h5 id="Assertion">Assertion</h5><p>JUnit provides the concept of assertions and XLT uses this concept for all validations. Since XLT doesn&#8217;t change JUnit in any way, you can use assertions just as you&#8217;re used from JUnit. </p><h5 id="Prevalidation2">Pre-validation</h5><p>XLT offers two ways of using the <code>preValidate()</code> method. Any exception on the direct path stops the test with an error message. In case you just want to check whether or not a requirement is fulfilled, you can call the <code>preValidate</code> in a safe way (by using <code>preValidateSafe()</code>) so that any exception is caught and no error is reported. Should you accidentally cause a Java exception different to <code>AssertionException</code>, such as <code>NullPointerException</code> or <code>IndexOutOfBoundException</code>, XLT issues a warning because the code might contain a problem from a programming point of view. Errors from the application under test should always come up as assertion failures.</p><h5 id="Postvalidation2">Post-validation</h5><p>The <code>postValidate()</code> method works similarly to <code>preValidate()</code>. It is used to validate the page just loaded in <code>execute()</code> and ensures that the data matches the expectations. The full set of JUnit assertions is available.</p><p>You can&#8217;t explicitly call the <code>postValidate()</code> method; the framework does so instead. Additionally, error messages can&#8217;t be suppressed. If a page has different outcomes based on random data or states, you have to explicitly handle that in your validation code.</p><h5 id="Validators">Validators</h5><p>We strongly encourage you to write individual validation classes for easy reuse. As soon as a certain check has to be done more than once, it is suited for a validator implementation. This simplifies the maintenance of tests and makes them less error-prone because copy-paste causes typical programming errors.</p><p>Some common validation routines are already covered by default validators, such as a HTTP response code, HTML end tag, and HTTP content length validation. See package <code>com.xceptance.xlt.api.validators</code> in the API documentation for more information on this topic.</p><h4 id="Example2">Example</h4><p>Let&#8217;s imagine a poster search test case again to illustrate the XLT Action API. The most important action would be to &#8220;search&#8221;, that is to fill in the search phrase and then click &#8220;Go&#8221;, &#8220;Search&#8221;, or something similar that loads a list of results. The preconditions are the existence of a search input field and of an appropriate button labeled <strong>Search</strong> or <strong>Go</strong>. The <code>execute()</code> method should fill in the search phrase and click the button.</p><p>After the new page has been loaded, the result should be validated. This validation consists of general validation, performed by validators, and action-specific validation.</p><p>The resulting implementation of the search action would then look like this:</p><pre class="java"><code>/**
 * Enter the given search phrase in the site's search bar and submit the form.
 */
public class Search extends AbstractHtmlPageAction
{
    /**
     * Search phrase.
     */
    private final String phrase;

    /**
     * Search form.
     */
    private HtmlForm searchForm;

    /**
     * Search option ({@link SearchOption#HITS} or {@link SearchOption#NO_HITS} ).
     */
    private final SearchOption searchOption;

    /**
     * Constructor
     * 
     * @param previousAction
     *            The previously performed action
     * @param phrase
     *            The search phrase
     * @param option
     *            The search option that defines if we expect a hit or a no-hit
     */
    public Search(final AbstractHtmlPageAction previousAction, final String phrase, final SearchOption option)
    {
        super(previousAction, null);
        this.phrase = phrase;
        searchOption = option;
    }

    /**
     * Validation prior to execution.
     * @throws Exception
     *             if some of the required input elements couldn't be found.
     */
    @Override
    public void preValidate() throws Exception
    {
        // Get the current page.
        final HtmlPage page = getPreviousAction().getHtmlPage();
        Assert.assertNotNull("Failed to get page from previous action.", page);

        // Check that the search form is available
        Assert.assertTrue("Search form not found.", HtmlPageUtils.isElementPresent(page, "id('search')"));

        // Remember the search form
        searchForm = HtmlPageUtils.findSingleHtmlElementByID(page, "search");
    }

    /**
     * Executes the search. Primarily this includes the input of the search
     * phrase and a click on the proper search button.
     * @throws Exception
     *             if some of the inputs have become invalid or setting the
     *             value attribute of the search input field has failed.
     */
    @Override
    protected void execute() throws Exception
    {
        // Fill the search form with the given phrase
        HtmlPageUtils.setInputValue(searchForm, "searchText", phrase);

        // Submit the search
        loadPageByFormSubmit(searchForm);
    }

    /**
     * Validation after search has become complete.
     * @throws Exception
     *             if no search result block element could be found
     */
    @Override
    protected void postValidate() throws Exception
    {
        // Get the result of the action
        final HtmlPage page = getHtmlPage();

        // Basic checks - see action 'Homepage' for some more details how and when to use these validators
        HttpResponseCodeValidator.getInstance().validate(page);
        ContentLengthValidator.getInstance().validate(page);
        HtmlEndTagValidator.getInstance().validate(page);

        HeaderValidator.getInstance().validate(page);

        // Check that the desired option result was achieved.
        switch (searchOption)
        {
            case HITS:
                Assert.assertNotNull("Expected at least one hit for '" + phrase + "'.",
                                     HtmlPageUtils.findSingleHtmlElementByID(page, "productOverview"));
                break;

            case NO_HITS:
                Assert.assertFalse("Search phrase '" + phrase + "' should result in no hits.",
                                   HtmlPageUtils.isElementPresent(page, "productOverview"));
                break;

            default:
                Assert.fail("Unknown search option.");
                break;
        }
    }
}


</code></pre><p>Note that the constructor of this class has two parameters. One of them is the search phrase the action has to know about. The other parameter is the previously performed action. To enable a flow, all the actions that will be used in page flows need to provide a constructor with a parameter representing the previous action. Without passing the previous action, each action would be stand-alone and behave as if you had just opened a new web browser. Normally, only the start action does so.</p><p>You&#8217;ll notice that the <code>postValidate()</code> method uses some of the predefined validators. XLT also offers a <code>StandardValidator</code> performing the most common validations in one go. This includes:</p><ul><li>HTTP response code validation,</li><li>HTML end tag validation,</li><li>content length validation, and</li><li>XHTML validation.</li></ul><p>Having the search action at hand, the implementation of a test case using this action is almost done. A very simple test case would be a repeated search for some phrases. These phrases can be stored in a data file and obtained using the XLT data provider mechanism:</p><pre class="java"><code>public class TSearch extends AbstractTestCase
{
    // Container that holds all the search phrases
    private static CustomDataProvider phrases = null;

    @Before
    public void initialize() throws Exception
    {
        // Data container already initialized?
        if(phrases != null) return;
        // No. Go for it.
        phrases = new CustomDataProvider(
                        getProperty("searchphrases.filename", "phrases.txt"),
                        CustomDataProvider.DEFAULT);
    }

    @Test
    public void search() throws Throwable
    {
        // Start on Homepage.
        Startpage start = new Startpage();
        start.run();

        for (int i = 0; i &lt; XltRandom.nextInt(10); i++)
        {
            // Take a random search phrase.
            String searchPhrase = phrases.getRandomRow(false);

            // Search.
            Search search = new Search(start,searchPhrase);
            search.run();
        }
    }
}

</code></pre><p>The example above also demonstrates the use of the <code>XltRandom</code> class offering some convenient randomization features. See the package <code>com.xceptance.xlt.api.util</code> for additional functionality that may help implementing tests.</p><p>Each execution of the search action requires an appropriate search phrase obtained from a <code>CustomDataProvider</code> object. This class offers a generic mechanism to handle and provide test data that is stored in a text file. The name of the text file is, along with a Boolean value telling the appropriate parser whether whitespace should be removed, passed to the constructor as parameter. When the class is instantiated, all data is kept in memory, allowing easy and fast access. XLT is shipped with a predefined set of data files containing email addresses, first and last names, street and city names, and so on. This data can be acquired from the <code>GeneralDataProvider</code> class that uses the appropriate text files located in directory <code>&lt;testsuite&gt;/config/data</code> with <code>&lt;testsuite&gt;</code> referring to your test project directory.</p><p>Last but not least, the present example illustrates how you can use JUnit4 annotations in the standard manner.</p><h2 id="DataDrivenTest">Data-Driven Tests</h2><p>Sometimes a certain test case needs to be executed not just once but multiple times, each time with a different set of test data. For instance, to check not only the &#8220;happy path&#8221; but also some border cases, you might want to specify multiple test data sets automatically recognized by the test framework and used to execute the test case once per specified data set. The test case executions are thus independent from each other and each produces a separate result in the test report. This concept can be referred to as <em>data-driven test</em>.</p><p>XLT will support data-driven tests for any kind of test case (plain Java and scripts) if you run them as (part of) a functional test using a JUnit test runner. Data-driven tests are neither supported when replaying script test cases in Script Developer nor can they be used during load testing. For load tests, other parameters  define how long/often a test will be executed, such as measurement time and arrival rate.</p><p>Typically, test data can be classified as constant or variable test data. <em>Constant test data</em> is fixed for all runs of a test case in a data-driven test. It is either hard-coded into the test case or kept separate from the test code in a data file. <em>Variable test data</em> is different for each run of the test case, which is why it is organized as a list of separate data sets. Each data set contains all variable test data needed for exactly one test run. The number of data sets determines the number of test runs.</p><p>Where do these data sets come from? For a data-driven test, XLT retrieves test data sets from an additional source, which can be, for example, another data file or a database. To be more specific, XLT accesses a data source via <em>data set providers</em> that implement a uniform interface and return a data set as a simple key/value map.</p><p>During test execution, the framework reads the next data set from the configured data set provider, passes it on to the test case instance, and then runs the test. The test instance is responsible for the appropriate application of the test data. This procedure is repeated until all variable data sets have been processed.</p><h3 id="DataSetProviders">Data Set Providers</h3><h4 id="BuiltinProviders">Built-in Providers</h4><p>XLT supports three common sources for test data sets out of the box:</p><h5 id="XMLFiles">XML Files</h5><p>Data sets can be stored as XML data file with a three-level element structure. XML assigns a single top-level element. The elements on the 2nd level define the data sets, the elements on the 3rd level define the values. A data file with two sets of user data might look like this:</p><pre class="xml"><code>&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;data-sets&gt;
    &lt;data-set&gt;
        &lt;userName&gt;fred&lt;/userName&gt;
        &lt;password&gt;topsecret&lt;/password&gt;
    &lt;/data-set&gt;
    &lt;data-set&gt;
        &lt;userName&gt;wilma&lt;/userName&gt;
        &lt;password&gt;cantremember&lt;/password&gt;
    &lt;/data-set&gt;
&lt;/data-sets&gt;
</code></pre><p>Note that you can name the root element (here: <code>data-sets</code>) and the 2nd-level elements (here: <code>data-set</code>) as you like since only the structure matters. However, the tag names on the 3rd level always define the parameter names, so these tag names must be used consistently across all data sets.</p><h5 id="CSVFiles">CSV Files</h5><p>Data sets can also be stored as CSV data files, organized as lines of separated values. The values in the 1st line define the parameter names, the values of all following lines define the values of each data set. The previous example would thus look like this:</p><pre class="plain"><code>userName,password
fred,topsecret
wilma,cantremember
</code></pre><p>You can configure the separator used in the CSV file (typically comma or semicolon) with an XLT property. For example:</p><pre class="plain"><code>## Sets the field separator character for CSV files (defaults to ",").
com.xceptance.xlt.data.dataSetProviders.csv.separator = ; 
</code></pre><p>Note that there&#8217;s no way to specify character encoding information in a CSV file. By default, XLT reads CSV files using UTF-8. To override this default, use the following property:</p><pre class="plain"><code>com.xceptance.xlt.data.dataSetProviders.csv.encoding = ISO-8859-1
</code></pre><h5 id="JDBCDataSources">JDBC Data Sources</h5><p>There is also a &#8220;data file&#8221; for JDBC data sources, yet it doesn&#8217;t directly specify the data sets. Instead, it contains an SQL query that retrieves the data sets when being executed. A query file for user data sets may look like this:</p><pre class="plain"><code>select login as "userName", password as "password" from users;
</code></pre><p>Each row returned from the database is converted to one data set. The alias names in the query define the resulting parameter names. </p><p>Note that the SQL data set provider needs additional setup before it can be used. First, an appropriate JDBC driver has to be present on the class path of your test suite. Copy the respective JAR file to <code>&lt;testsuite&gt;/lib</code> and you are done. Second, configure the URL and credentials of your JDBC database connection that should be used when executing the query:</p><pre class="plain"><code>com.xceptance.xlt.data.dataSetProviders.jdbc.url = jdbc:h2:tcp://localhost/test
com.xceptance.xlt.data.dataSetProviders.jdbc.userName = sa
com.xceptance.xlt.data.dataSetProviders.jdbc.password = YourPassword
</code></pre><h4 id="CustomDataSetProviders">Custom Data Set Providers</h4><p>If the built-in data set providers are not sufficient, you can write your own. Your custom data set provider must implement the general <code>DataSetProvider</code> interface: </p><pre class="java"><code>public interface DataSetProvider
{
    public List&lt;Map&lt;String, String&gt;&gt; getAllDataSets(File dataFile) throws DataSetProviderException;
}
</code></pre><p>To register your own data provider implementation with XLT, add a property to your configuration:</p><pre class="plain"><code>com.xceptance.xlt.data.dataSetProviders.foo = com.yourcompany.FooDataSetProvider
</code></pre><p>This tells XLT to use the class <code>com.yourcompany.FooDataSetProvider</code> for data files with the extension <code>.foo</code>. Note that this way you can also override the built-in providers.</p><h3 id="TestDataSetFileLookup">Test Data Set File Lookup</h3><p>As outlined above, there is always some kind of &#8220;data&#8221; file involved. Typically, these data files are named after the test case (the script name for script-based test cases or the simple class name for Java-based test cases). For instance, if a test script is named <code>TSearch</code>, then the XLT framework will automatically look for files like <code>TSearch_datasets.&lt;ext&gt;</code>, where <code>&lt;ext&gt;</code> is one of the file extensions for which a data set provider has been registered. Accordingly, the resulting list of file name candidates is: <code>TSearch_datasets.csv</code>, <code>TSearch_datasets.xml</code>, <code>TSearch_datasets.sql</code> (and <code>TSearch_datasets.foo</code>).</p><p>The lookup order of data files is defined as:</p><ol><li><code>.csv</code></li><li><code>.xml</code></li><li><code>.sql</code></li><li>(<code>.foo</code>)  </li></ol><p>However, test data files can also have arbitrary names or paths. If so, you have to configure which data file belongs to which test case. Map the data set file name (or path) to the test case&#8217;s Java class using the following notation:</p><pre class="plain"><code>&lt;class_name&gt;.dataSetFile = &lt;data_set_file_path&gt;
</code></pre><p>For example:</p><pre class="plain"><code>com.mycompany.xlt.tests.MyTest1.dataSetsFile = Test1.xml
com.mycompany.xlt.tests.MyTest2.dataSetsFile = ./subdir/Test2.csv
com.mycompany.xlt.tests.MyTest3.dataSetsFile = c:/tmp/Test3.sql
</code></pre><blockquote class="note"><p>Keep in mind that all property files are Java-style property files. Thus, when you use backslashes on Windows, you have to quote it with another backslash, for instance <code>c:\\tmp\\Test3.sql</code>.</p></blockquote><p>Regardless of the test data file being given explicitly or deriving from the test case, the XLT framework looks for such a file in several locations:</p><ol><li>In the current directory (typically <code>&lt;testsuite&gt;</code>),</li><li>in the directory specified by the property <code>com.xceptance.xlt.data.dataSets.dir</code> [this property is set to <code>./config/data)</code> but commented out by default; remove the hash character when you want to use this location],</li><li>in the default script test case directory <code>&lt;testsuite&gt;/scripts</code> (for script-based tests only), and  </li><li>in the class path (for Java-based tests only).  </li></ol><p>As soon as a suitable file is found, the lookup stops.</p><p>Note that if the test case has a qualified name, that is if it has a package part, the data set file also needs to have that package to be found. If there is a test case named <code>your.package.TAuthor</code> (be it a script or a Java test case), the framework will therefore search the aforementioned directories for a corresponding data set file using the file path <code>&lt;dir&gt;/your/package/TAuthor_datasets.&lt;ext&gt;</code> or, alternatively, the file name <code>&lt;dir&gt;/your.package.TAuthor_datasets.&lt;ext&gt;</code>.    </p><h3 id="AccessingDataFromATestDataSetInYourTestCase">Accessing Data From A Test Data Set In Your Test Case</h3><p>The XLT framework makes the current test data set available to your test case. For script test cases, the key/value pairs from the test data set are automatically added to the internal pool of variables and can be accessed in your scripts using the <code>${varName}</code> syntax.</p><p>If you prefer using Java as your scripting language, call the method <code>getTestDataSet()</code> of the super class <code>AbstractTestCase</code> in your test code to access the current test data set as a map of strings representing the key/value pairs. You are responsible to pass the respective values to the pieces of code where they are needed.</p><h3 id="ExecutingDataDrivenTestsviaJUnit">Executing Data-Driven Tests via JUnit  </h3><p>Running data-driven tests via JUnit isn&#8217;t in any different from running normal test cases. You simply add the test cases in question (directly or indirectly) to the list of classes to be run by JUnit. If XLT finds a data set file for a certain test case, it will be passed on to the right data set provider, which returns all data sets. The test case will then be executed once for each data set. This works because all test cases extend the class <code>AbstractTestCase</code>, and this class has all the magic built-in. The same is true for the generic script test case suite class <code>ScriptTestCaseSuite</code>.</p><p>See the demo test suite in directory <code>&lt;xlt&gt;/samples/testsuite-posters</code> for an example of how to configure Ant&#8217;s <code>junit</code> task to run XLT test cases. </p><p>If you want to temporarily disable a data-driven test and let the test cases run only once in spite of data set files, configure the following setting in <code>default.properties</code>:</p><pre class="plain"><code>com.xceptance.xlt.data.dataDrivenTests.enabled = false
</code></pre><p>That being the case, be aware that your test case must provide some default test data.</p><h2 id="PosterDemo">Demo Application and Test Suite</h2><p>XLT ships with a real-world demo web application (<em>Posters</em>) as the system under test and a test suite to test this application. Both can be found in the directory <code>&lt;XLT&gt;/samples</code>.</p><h3 id="RunningPosters">Running <em>Posters</em></h3><p><em>Posters</em> is a shop software written in Java. Being small and easy to deploy, it is well suited to demonstrate testing with XLT. </p><p>To start the demo application, use the following command:</p><pre class="plain"><code>cd &lt;XLT&gt;/samples/app-server/bin
./start.sh

</code></pre><blockquote class="note"><p>Windows users have to use the appropriate <code>.cmd</code> file located in the same directory by entering <code>start.cmd</code> into the command prompt.</p></blockquote><p>This starts an application server containing the <em>Posters</em> application. To access it, open a browser with this URL: <a href="http://localhost:8080/posters/">http://localhost:8080/posters/</a>. Please take some time to become familiar with <em>Posters</em>.</p><h3 id="ThePostersTestSuite">The <em>Posters</em> Test Suite</h3><p>XLT lets you create test cases using different approaches, that is test cases can be written directly in Java with your favourite IDE or they can be recorded as simple scripts with Script Developer. A test suite may contain test cases created with either approach, and so does the <em>Posters</em> test suite. Even though the number of test scenarios is limited here, our sample project perfectly demonstrates the differences between the two approaches, and &#8211; for Java-based test cases &#8211; what APIs are available to implement them.</p><h4 id="TheDirectoryStructureoftheTestSuite">The Directory Structure of the Test Suite</h4><p>An XLT test project has a simple directory structure. The following directories have to exist in order for everything to run smoothly:</p><ul><li><code>&lt;project&gt;/classes</code> contains the compiled code of your project. Normally, your IDE will do the job and place the files there. You can optionally build a JAR and place it in the <code>&lt;project&gt;/lib</code> directory.</li></ul><ul><li><code>&lt;project&gt;/config</code> contains all the properties files used to configure the project.</li></ul><ul><li>There can be an optional <code>&lt;project&gt;/config/data</code> directory where you can place any data file you need for the test, such as address data, logins, and so on. All files are uploaded to the agent before a load test takes place. The programming API provides easy access to this data. </li></ul><ul><li>You can place all your required libraries in <code>&lt;project&gt;/lib</code>. The content is uploaded to the runtime agent and included in the class path. For your local development within an IDE, you have to manually add the libraries to the class path of your project.</li></ul><ul><li>The <code>&lt;project&gt;/src</code> directory holds the Java-based test cases of your project. This code is compiled into classes by your IDE or build environment. It&#8217;s organized in main packages, typically one package for test cases, one for actions, and one for utility classes. Make sure the compiled classes end up in <code>project/classes</code> because this is the directory XLT configures as class path for your test.</li></ul><ul><li>The <code>&lt;project&gt;/scripts</code> directory contains test scenarios as script test cases. Again, the code is organized in main packages, one package for test cases, a.k.a. scenarios, and one for reusable modules.</li></ul><h4 id="UnderstandingtheTestScenarios">Understanding the Test Scenarios</h4><p>Since <em>Posters</em> is a shop software, our test scenarios cover the typical use of an online shop:</p><ul><li><strong>Visitor</strong>: A visitor arrives on the homepage, and that&#8217;s it.</li><li><strong>Browse</strong>: The visitor arrives on the homepage, then starts browsing some main- and sub-categories and views a random product detail page.</li><li><strong>Search</strong>: The visitor arrives on the homepage and enters one or more search phrases, then opens the product detail page for one of the search result items.</li><li><strong>Register</strong>: A visitor creates an account.</li><li><strong>Add To Cart</strong>: <strong>Browse</strong> extended by adding the shown product to the cart and viewing the cart.</li><li><strong>Guest Checkout</strong>: <strong>Add To Cart</strong> with a subsequent checkout process but without submitting the order.</li><li><strong>Checkout</strong>: <strong>Add To Cart</strong> with a subsequent checkout process as registered user and, again, without submitting the order.</li><li><strong>Guest Order</strong>: <strong>Guest Checkout</strong> with a completed checkout including submission of the order.</li><li><strong>Order</strong>: <strong>Checkout</strong> including submission of the order.</li></ul><p>Note that the scenarios share some common steps, thus allowing a clear demonstration of how to reuse code across test cases.</p><h4 id="RunningScriptTestCasesinFirefox">Running Script Test Cases in Firefox</h4><p>To run the script test cases in Firefox, import the test project into Script Developer first.</p><h5 id="ImportingthePostersTestSuiteintoScriptDeveloper">Importing the <em>Posters</em> Test Suite into Script Developer</h5><ol><li>Open Script Developer.</li><li>Click the folder-like toolbar button and choose &#8216;Import...&#8217; to open a file system explorer window.</li><li>Navigate to <code>&lt;XLT&gt;/samples</code>, mark <code>testsuite-posters</code>, and then click <em>Select Folder</em>.</li></ol><p>The tree view on the left-hand side displays all available script test cases and modules.</p><h5 id="ExecutingScriptTestCasesinScriptDeveloper">Executing Script Test Cases in Script Developer</h5><p>Open Script Developer. All script test cases and modules are listed in the script explorer. Double-click the <code>TSearch</code> test case to open it and see the list of its commands and called modules inside the work area. Click the <em>Play</em> icon to start replaying the script in Firefox. Alternatively, right-click on the test case in the tree view and choose <em>Run</em> from the context menu.</p><h4 id="RunningJavaTestCasesinHeadlessMode">Running Java Test Cases in Headless Mode</h4><p>Java-based test cases are executed in headless mode, that is in a simulated browser that doesn&#8217;t perform page rendering. Script test cases may also be run in this mode via Java wrapper classes. Before you can do so, you need to import the sample test suite into your Java IDE.</p><h5 id="ImportPosters">Importing the <em>Posters</em> Test Suite into Eclipse</h5><p>After starting Eclipse and creating a workspace, do the following:  </p><ol><li>Open the import dialog (<em>File</em> &gt; <em>Import</em> &gt; <em>General</em> &gt; <em>Existing Project Into Workspace</em>).</li><li>Select the root directory to search in and point to <code>&lt;XLT&gt;/samples</code>.</li><li>Select the test suite project <code>testsuite-posters</code> from the list.</li><li>Click <em>Finish</em>.</li></ol><p>Since the imported project has dependencies on the libraries of XLT, you have to adjust these dependencies.</p><ol><li>Right-click on your project and select <em>Properties</em>.</li><li>Click <em>Java Build Path</em>.</li><li>Select the <em>Libraries</em> tab, then click <em>Add External JARs</em>.</li><li>Go to <code>&lt;XLT&gt;/lib</code> and select all JARs. Then click <em>Open</em>.</li><li>A list of all these JARs is displayed. Close the dialog with <em>OK</em>.</li></ol><p>Eclipse will rebuild the project and shouldn&#8217;t report any build problems if configured properly.</p><blockquote class="note"><p>Users of other IDEs have to carry out similar steps.</p></blockquote><h5 id="ExecutingJavaTestCasesinEclipse">Executing Java Test Cases in Eclipse</h5><p>Any Java test case can be directly run in Eclipse in headless browser mode. Go to package <code>com.xceptance.xlt.samples.tests</code>, select the test case class (e.g. <code>TSearch</code>), and run it as JUnit test via the Eclipse class file context menu.</p><h2 id="WritingWebTests">Writing Web Tests</h2><p>This chapter will guide you through the essential steps when you want to write web tests using XLT. It provides useful information that help you get started and make the right decisions on your test suite.</p><p>Feel encouraged to carefully study both the approach and the code illustrated in the demo test suite. As all web tests are similar in structure, you will soon get a feeling for how to write your own tests. Thus, the sample test suite may serve as a template for your own test projects.</p><h3 id="CreatingaNewTestSuite">Creating a New Test Suite</h3><p>The required directory structure of XLT test suites is shown below. Note that it can&#8217;t be changed:</p><pre class="plain"><code>--+ &lt;testsuite&gt;
  |--- classes
  |--+ config
  |  `--- data
  |--- lib
  |--- results
  |--- scripts
  `--- src
</code></pre><p>But you don&#8217;t need to create the mandatory directory structure and configuration files from scratch. XLT ships with an empty test suite project, which can be used as a template for your own projects. This template project is located in <code>&lt;xlt&gt;/samples/testsuite-template</code>. Simply copy the complete <code>testsuite-template</code> directory to a location of your choice and rename it, for example <code>c:\test-suites\testsuite-MySite</code>. This directory is referred to as <code>&lt;testsuite&gt;</code> throughout the next sections.</p><blockquote class="note"><p>It is recommended to store your test suite in a directory that is <strong>not</strong> a subdirectory of your XLT installation. This facilitates updating to newer XLT versions because it reduces the risk of overwriting your test cases during an update.</p></blockquote><p>Now import your project into Eclipse and change the displayed project name <em>testsuite_template</em> to <em>testsuite_MySite</em> via Eclipse&#8217;s refactoring functionality. </p><blockquote class="note"><p>If you prefer starting with a test suite that already contains working sample test cases, you may as well use the Posters demo project in <code>&lt;xlt&gt;/samples/testsuite-posters</code> as the template for your project.</p></blockquote><h3 id="ChoosingaSuitableApproach">Choosing a Suitable Approach</h3><p>As described in <a href="user-manual.html#AvailableAPI">XLT Framework</a>, you can make use of different approaches when writing your own test cases. The following questions will help you decide which approach and which API best fits your web test project: </p><ul><li>What is the goal of your test suite?<ul><li>Pure functional test suite</li><li>Functional test suite with some basic load testing</li><li>Load test project</li><li>Time-critical load test project with heavy load</li></ul></li><li>How good are your programming skills?<ul><li>Non-programmer</li><li>Basic programming skills</li><li>Good Java programming skills</li></ul></li><li>Do you have to simulate different browsers in a functional test suite?</li><li>What are the available resources to generate enough load during a load test?</li><li>Do you need random factors in your test cases and do you have to handle conditions? </li></ul><h4 id="StandAloneScriptDeveloperTestSuite">Stand-Alone Script Developer Test Suite</h4><p>Recording your tests with Script Developer and continuing without using any Java code will be a good option if...</p><ul><li>you need a functional test suite you want to run from time to time,</li><li>load testing is an interesting option for the future,</li><li>you have no or just basic Java programming skills,</li><li>it&#8217;s sufficient to use Firefox and if there&#8217;s no need to simulate other browsers,</li><li>you&#8217;re okay with exactly repeating test cases and if you don&#8217;t have to handle conditions,</li><li>you don&#8217;t need any advanced random factors or <a href="user-manual.html#DataDrivenTest">data-driven tests</a>.</li></ul><p>Script Developer lets you record and manually replay the test cases. There&#8217;s no need to write any Java code or to use any tool or IDE other than Script Developer.</p><h4 id="ExecutingPlainScriptTestCasesOutsideScriptDeveloper">Executing Plain Script Test Cases Outside Script Developer</h4><p>When you need to run the tests outside Script Developer, e.g. in Eclipse or during a build process, Script Developer is the best to start with. Simply enable <a href="user-manual.html#JavaCodeSettings">generating Java wrapper classes</a> for the script test cases. With these automatically generated wrapper classes, you can also run a load test &#8211; as long as you&#8217;re fine with the limitations listed above. Again, you don&#8217;t need to write any Java code. Everything is based on Script Developer commands and Firefox context menu validations. </p><p>Note that executing script test cases outside Script Developer also allows you to run <a href="user-manual.html#DataDrivenTest">data-driven tests</a> and to simulate different browsers.</p><h4 id="ScriptingAndWebDriver">Script Developer Export, XLT Scripting API and WebDriver</h4><p>If you need to overcome the limitations of script test cases by switching to Java as the programming language, Script Developer again is the best point to start from because it easily lets you create prototype scripts you can export to Java. This results in a test case class that uses the XLT Scripting API and that can be expanded with the WebDriver API. Another option is an export with the resulting Java code being based on the XLT Action API. The <code>TBlogVisitor</code> <a href="user-manual.html#TBlogVisitorWebDriver">test case</a> is a good example for an export resulting in the XLT Scripting API. This approach will be a good option if...</p><ul><li>you want to benefit from the advantages of Script Developer,</li><li>you want to overcome the limitations imposed by Script Developer,</li><li>you have basic or good Java programming skills,</li><li>you need to simulate different browsers for a functional test,</li><li>data-driven tests are required,</li><li>you want to handle conditions and execution branches in your test cases, </li><li>the planned load testing profile is not causing any resource and timing problems.</li></ul><h4 id="WebDriverfromScratch">WebDriver from Scratch</h4><p>Within the XLT context, you can also directly resort to the WebDriver API without using the XLT Scripting API abstraction level. That&#8217;s a common approach for advanced users and will be a good option if...</p><ul><li>the web test project meets the requirements listed in the <a href="user-manual.html#ScriptingAndWebDriver">section above</a>,</li><li>you&#8217;re happy writing your test cases from scratch without any recording feature,</li><li>you have good Java programming skills.</li></ul><h4 id="XLTActionAPIandHtmlUnit">XLT Action API and HtmlUnit</h4><p>Compared to a low-level API, high-level APIs have a certain overhead that may affect the test performance. In some situations it makes sense to skip the high-level APIs listed above and program the test using HtmlUnit, for instance, when running load tests for which heavy network traffic is required. If so, the XLT Action API serves as a framework that supports the action concept and validations as well as the structuring of your code. You don&#8217;t even need to waive the Script Developer recording feature since exporting the script test cases to Java lets you generate code based on the XLT Action API. The XLT Action API will be a good option if...</p><ul><li>your test suite is meant to work as a high performance load test,</li><li>you have to carefully handle the available resources to generate enough load,</li><li>you need full control over requests and responses on the low-level HtmlUnit API,</li><li>it&#8217;s sufficient to use a headless browser and if there&#8217;s no need to simulate other browsers.</li></ul><p>If you decide to use the XLT Action API as the surrounding framework, you are bound to use the HtmlUnit API to code your tests. Note that this is not true for the other way around; thus, if you use one of the other approaches as the surrounding framework, such as WebDriver, you can still go down to the HtmlUnit level for sections of your test cases.</p><h4 id="XLTLightweightMode">XLT Lightweight Mode</h4><p>You should choose XLT Lightweight Mode to code your test cases if...</p><ul><li>the <em>XLT Action API and HtmlUnit</em> approach is still not sufficient to meet your needs regarding resources consumption and test execution speed.</li></ul><p>XLT provides the Lightweight Mode to code highest performance test cases. This maximum performance can be reached by omitting the creation of a DOM tree and JavaScript. Responses are available as HTML source and test cases have to be coded on this level. Some regular expression knowledge is required to identify elements and perform validations. The test cases also become more complex and you have to expect an increased programming effort.      </p><h3 id="Structuring">Structuring Your Test Suite and Test Cases</h3><h4 id="StructuringinScriptDeveloper">Structuring in Script Developer</h4><h5 id="NamingandTags">Naming and Tags </h5><p>The easiest way for you to structure your test suite is to name the test cases according to a common naming convention. It&#8217;s common to start the name with a capitalized <em>T</em> and  continue with the so-called &#8220;camel-case&#8221; where each element&#8217;s initial letter is capitalized within the compound word. A best-practice example is to specify the test case&#8217;s purpose by compounding single elements, for instance <em>TCartCheckoutCancel</em>, <em>TCartOrder</em>, and so on.</p><p>Script Developer allows you to <a href="user-manual.html#TestCaseDetails">tag test cases and modules</a>. Each test case can have several tags. Tags help to group several test cases and make them easier to find. The list of available test cases can be filtered by tag and name.</p><h5 id="ScriptPackages">Script Packages</h5><p>Script Developer lets you define packages to structure your test cases and modules in the script explorer. You can use this feature to bundle test cases for similar purposes. The packages may be created having a hierarchical structure, but they are always displayed in flat representation (e.g. testcases.cart.order).</p><h5 id="ScriptModulestoStructureTestCases">Script Modules to Structure Test Cases</h5><p>To structure a single script test case, you should use <a href="user-manual.html#Modules">modules</a>. A module is a sequence of script commands. It can be reused in several test cases and call other modules, that is they can be nested. Using modules prevents you from writing or recording the same sequence of commands over and over again. It also helps to keep the test case clear and comprehensive because a module call can be folded to hide the contained commands and display the module&#8217;s name only.</p><h5 id="ActionCommands">Action Commands</h5><p>While recording, Script Developer automatically inserts so-called <em>Actions</em>. You can also insert a new action manually at any position of your test case. Even though actions look similar to comments or other structuring elements, they are in fact <strong>not</strong> meant to visually structure your scripts. See the <a href="user-manual.html#Actions">action-related notes</a> in the Script Developer chapter for details.</p><h4 id="JavaCodeStructuring">Java Code Structuring</h4><p>If you decide to choose an approach that requires to write your test cases in Java, you will have the same options to structure your code as in any other Java program or software development project. Feel free to extract sequences of your test case code to methods, to create Java classes, or to use packages to structure your test suite.</p><h5 id="PackageSuggestions">Package Suggestions</h5><p>The <code>&lt;testsuite&gt;/src</code> directory contains subdirectories with the structure of your Java packages in the standard manner. Your source code should be organized in main packages. Typically, one individual package should be created for test cases, for actions, for flows, for validators, and for utility classes. The resulting directory structure might look like this:</p><ul><li><code>&lt;testsuite&gt;/src/.../actions</code> <em>(only when XLT Action API is used)</em></li><li><code>&lt;testsuite&gt;/src/.../flow</code></li><li><code>&lt;testsuite&gt;/src/.../util</code></li><li><code>&lt;testsuite&gt;/src/.../validators</code></li><li><code>&lt;testsuite&gt;/src/.../tests</code></li></ul><p>If you plan to use more than one of the approaches provided by XLT, it&#8217;s recommended to create packages for each of its test cases, which generates up to three additional subdirectories. Your packages could be named as follows:</p><ul><li><code>&lt;testsuite&gt;/src/.../tests/actionbased</code></li><li><code>&lt;testsuite&gt;/src/.../tests/scripting</code></li><li><code>&lt;testsuite&gt;/src/.../tests/webdriver</code></li></ul><p>Besides these options, each approach also introduces XLT-specific framework conditions structuring your test suite and test cases. In particular, each test case is necessarily implemented as a Java class extending an XLT test case class, which is approach-specific and contains one method annotated with <code>@Test</code>. See the following sections for such specific framework conditions and further ways of structuring.  </p><h5 id="StructuringScriptingAPIandWebDriverTestCases">Structuring Scripting API and WebDriver Test Cases</h5><p>When you export a script test case from Script Developer to Java using the XLT Scripting API, it will be converted into a test case class extending <code>AbstractWebDriverScriptTestCase</code>. Modules are converted to classes extending <code>AbstractWebDriverModule</code>. The test case class and all module classes are created automatically and you generally don&#8217;t have to deal with their creation.</p><p>If you&#8217;ve decided to write the test cases from scratch using the WebDriver API, your test case class should extend <code>AbstractTestCase</code>. Like all test case classes, it can have several methods, but exactly one method has to be annotated with <code>@Test</code>. This class usually contains the statements and lines of code defining the basic structure of the test case, that is the page flow.</p><p>For both approaches, XLT Scripting API and WebDriver API, it&#8217;s recommended to structure the page flow by actions. This is especially important if you plan to run a load test because the load test reports are designed for analysis and evaluation based on XLT actions.</p><p>The XLT Scripting API offers a very simple command to start a new action: <code>startAction("MyNewAction")</code>. When solely using the WebDriver API, the following line of code starts a new action: <code>Session.getCurrent().setWebDriverActionName("MyNewAction")</code>.</p><p>Again, be aware that actions should only be used to represent the page flow. See the <a href="user-manual.html#BasicConcepts">basic concepts of the XLT framework</a> for details. </p><h5 id="StructuringXLTActionBasedTestCases">Structuring XLT Action-Based Test Cases</h5><p>Test cases based on the XLT Action API are closely related to actions. As in all other approaches, an action interacts with the current page and, as a result, loads the next page. The latter is associated with this action and becomes the current page for the next action in the test case. In contrast to previous approaches, however, an action is now implemented as a Java class extending <code>AbstractHtmlPageAction</code>. These XLT action classes can be seen as reusable building blocks to write your test case and define the page flow.</p><p>See <a href="user-manual.html#XLTActionAPI">XLT Action API</a> for more information on how the API forces you to structure code and validations by implementing methods.</p><h5 id="Flow">Creating a Flow</h5><p>When creating XLT test cases, you sometimes might want to reuse blocks of code containing more than one single action. Just like for modules, you can create your own class with one method that combines a sequence of several XLT actions as a <strong>flow</strong>. Different test cases can call this method now to reuse the flow. This is a concept for code structuring you can implement if needed, yet explicit support is neither available in the XLT framework nor necessary when you manually create a flow.</p><p>Flows will be created automatically only if you export script modules containing more than one action to the XLT Action API.</p><h3 id="TestSuiteConfig">Test Suite and Framework Configuration</h3><blockquote class="note"><p>When solely using Script Developer for recording/writing and replaying script test cases, the following section doesn&#8217;t apply. Yet a configuration of the test suite and framework by changing properties files may be necessary if you write or run test cases from inside Eclipse as JUnit tests or if you want to perform load tests.</p></blockquote><p>To configure the test environment and the test suite, XLT uses <a href="http://en.wikipedia.org/wiki/.properties">Java properties files</a>. The basic characteristics and syntax of this format are also valid for the XLT properties files.</p><p>When reading the properties, XLT distinguishes between <em>load test mode</em> and <em>development mode</em>. As its name implies, the <em>load test mode</em> is active when test cases are executed as load tests by the XLT master controller/agent controller. When test cases are executed as JUnit tests in Eclipse or any other JUnit test runner, they run in <em>development mode</em>. Even though the <em>development mode</em> is mainly used for test case development, it will also be active if your test suite is meant to perform an automated functional test manually triggered from time to time or integrated in a build process. </p><p>XLT makes use of a hierarchical file system so that properties can be distributed to several files with different priorities. Properties from different files complement each other. Furthermore, properties from a file with higher priority can overwrite identical properties from a file with lower priority. This mechanism allows general default values to be specialized for different test run scenarios or projects. It&#8217;s also possible to prepare several configurations in different files and activate one of these configurations by switching between the files.</p><p>All properties are read from the <code>&lt;testsuite&gt;/config/</code> directory. The existing properties files are listed below, sorted by priority from lowest to highest. See the following sections for details.</p><ul><li><strong>default.properties</strong>: default configuration of the XLT test framework</li><li><strong>project.properties</strong>: configuration of your test project</li><li><strong>test.properties</strong>: configuration of a specific load test profile</li><li><strong>dev.properties</strong>: properties only read in development mode</li><li><strong>dev-log4j.properties</strong>: log4j logger settings used in development mode  </li><li><strong>log4j.properties</strong>: log4j logger settings used in load test mode</li><li><strong>jvmargs.cfg</strong>: JVM setting for the agents and therefore only used in load test mode</li></ul><h4 id="DefaultFrameworkConfigurationdefault.properties">Default Framework Configuration &#8211; default.properties</h4><p>The properties in <code>default.properties</code> represent the general XLT framework settings that are all set to their respective default value. They are neither specific for a single test project nor for a single test run.</p><p>If you need to change one of these properties, copy it to the <code>project.properties</code> or <code>test.properties</code> and change the value there to overwrite the value in <code>default.properties</code>. Each of the properties in <code>default.properties</code> can be overridden since they have the lowest priority.</p><p>When updating XLT to a newer version, it is recommended you update the <code>default.properties</code> file as well because newly available properties can be found there, along with their default value and description.<br/>Even though this file isn&#8217;t read-only, it should be treated as such. You can use it as documentation of available XLT framework properties that also defines the default values for these properties.</p><p>The properties listed in <code>default.properties</code> are separated into the following groups (look into the file itself for details):</p><ul><li>HTTP/Protocol Settings</li><li>Browser Emulation Settings</li><li>JavaScript Settings</li><li>CSS Settings</li><li>Test Data Management Settings</li><li>Result Settings</li><li>Test Execution Settings</li><li>Script Engine Settings</li><li>Miscellaneous Settings</li></ul><h4 id="TestProjectConfigurationproject.properties">Test Project Configuration &#8211; project.properties</h4><p>The file <code>project.properties</code> contains project-specific settings. The first and most important property is the reference to <code>test.properties</code>  that should be applied (e.g. <code>com.xceptance.xlt.testPropertiesFile = test-1.properties</code>). Changing the value for this property, you can easily switch between different load test profiles configurations.</p><p>By default, the file stores the test case mapping that maps the test case class onto a load test name. The load test name will be referenced later in the load test configuration.</p><p>It&#8217;s also the best place for all your test case-specific custom properties, such as URLs, credentials, search phrases, or any other data you want to extract from your test cases as properties. See <a href="user-manual.html#PostersDemo">Demo test suite posters</a> for examples on how to use properties in test cases.</p><h4 id="LoadTestProfileConfigurationtest.properties">Load Test Profile Configuration &#8211; test.properties</h4><p>The settings required to configure a particular load test profile are collected in a separate file. See <a href="user-manual.html#LoadTest">Load tests</a> or the properties file itself for details of available load test settings. </p><p>The default name of this file is <code>test.properties</code>. However, it&#8217;s variable and several files with different load test profile configurations may exist. The one file applied for a test run is referenced by a property in <code>project.properties</code> mentioned above.</p><h4 id="DevelopmentEnvironmentConfigurationdev.properties">Development Environment Configuration &#8211; dev.properties</h4><p>The file <code>dev.properties</code> contains development mode settings. Use this file to modify the configuration so that it suits your needs during test case development, that is when you create and debug test cases from within your IDE. </p><p>It&#8217;s read in development mode only, but not during load testing. For development mode, the values in this file have highest priority. Any setting defined here will overwrite the corresponding setting from the other properties files: &#8220;default.properties&#8221;, &#8220;project.properties&#8221;, and the test run-specific properties file, e.g. &#8220;test.properties&#8221;.</p><p>A typical example for differing development settings is <code>com.xceptance.xlt.loadStaticContent = true</code> to enable loading images and other static content in development mode for debugging. The default value ( <code>=false</code> ) for this property is switching off loading static content to save resources during load testing.</p><p>If the default values suffice as development settings for your test suite, <code>dev.properties</code> can also be empty.</p><h4 id="IncludingAdditionalPropertyFiles">Including Additional Property Files</h4><p>When dealing with different test environments, different load profiles, and/or different test data at the same time, managing different combinations of configuration settings can be challenging. To make this easier and less error-prone, properties can be included as a set. This allows to:</p><ul><li>predefine the configuration of certain aspects with certain values in separate files, and</li><li>reuse and combine the predefined settings as needed with a single statement.</li></ul><p>To this end, the files <code>default.properties</code>, <code>project.properties</code>, <code>test.properties</code> (no matter if it&#8217;s been renamed using <code>com.xceptance.xlt.testPropertiesFile</code>), and <code>dev.properties</code> can include further property files. Each of these additional property files has to be placed either directly in the <code>config</code> folder or in one of its sub-directories. Furthermore, the name of all these files must end with <code>.properties</code>. Any included file may also define includes itself.</p><h5 id="HowtoIncludeOtherPropertiesFiles">How to Include Other Properties Files</h5><p>You can include another property file by adding the special <em>include</em> property</p><pre class="plain"><code>com.xceptance.xlt.propertiesInclude.&lt;index&gt; = &lt;relativePathToPropFile&gt;
</code></pre><p>where <code>&lt;index&gt;</code> denotes an integer number. The value of the include property is the relative path to the file to include, starting from the directory in which the current file is located. You can also include all properties files in a certain directory at once by specifying the relative path to that directory:</p><pre class="plain"><code>com.xceptance.xlt.propertiesInclude.&lt;index&gt; = &lt;relativePathToDir&gt;
</code></pre><p>In both cases, the relative path may also go upwards using &#8220;..&#8221; as long as you don&#8217;t leave the <code>config</code> directory of the test suite.</p><pre class="info"><code>Include properties are treated like normal properties. Thus, if there are two include properties having an identical index, only one of them will be applied.
</code></pre><h5 id="ProcessingOrderofIncludedPropertyFiles">Processing Order of (Included) Property Files</h5><p><code>default.properties</code>, <code>project.properties</code>, <code>test.properties</code>, and <code>dev.properties</code> form a hierarchy. So the processing order is as follows: </p><ol><li><code>default.properties</code>, followed by its includes</li><li><code>project.properties</code>, followed by its includes</li><li><code>test.properties</code> (or any other test-run-specific properties file), followed by its includes</li><li><code>dev.properties</code>, followed by its includes (in development mode only)</li></ol><p>Includes will be resolved according to these rules:</p><ul><li>Each property include will be processed recursively in depth-first mode.</li><li>If the include target is a directory, the properties files contained in that directory are processed in alphabetical order.</li><li>If there&#8217;s more than one property include in a file, they will be processed in ascending order sorted by their index.</li></ul><p>During that process, properties read in later will overwrite already existing settings.</p><h5 id="Example3">Example </h5><p>Assume the system under test is deployed to different environments, such as <em>development</em>, <em>live</em>, and <em>pre-live</em>. Each environment requires a different host in the start URL and different access credentials. Furthermore, different load profiles are required for certain types of load tests. Now assume the following directory layout in the test suite:</p><pre class="plain"><code>--+ &lt;testsuite&gt;
  |--- classes
  |--+ config
  |  |--- data
  |  |--- effectiveSettings
  |  |--+ environments
  |  |  |--- development.properties
  |  |  |--- live.properties
  |  |  |--- pre-live.properties
  |  |  `--- test.properties
  |  |--+ loadProfiles
  |  |  |--- smallLoad.properties
  |  |  |--- halfLoad.properties
  |  |  `--- fullLoad.properties
  |  |--- default.properties
  |  |--- project.properties
  |  `--- test.properties
  |--- lib
  |--- results
  |--- scripts
  `--- src
</code></pre><p>As you can see, the load tester has prepared a property set for each environment and each load profile under the <code>config</code> directory. With these predefined property files, you can easily mix and match the environments and the load profiles as needed. For example, add/modify the following lines to your <code>test.properties</code> to apply the full target load to the pre-live environment:</p><pre class="plain"><code>com.xceptance.xlt.propertiesInclude.1 = environments/pre-live.properties
com.xceptance.xlt.propertiesInclude.2 = loadProfiles/fullLoad.properties
</code></pre><p>Alternatively, you may also define a certain directory to be always included:</p><pre class="plain"><code>com.xceptance.xlt.propertiesInclude.1 = effectiveSettings
</code></pre><p>To apply a certain combination of settings, simply empty this directory first and copy the respective predefined property files to this directory (<code>pre-live.properties</code> and <code>fullLoad.properties</code>, for example).</p><h4 id="AdditionalConfigurationFiles">Additional Configuration Files</h4><p>In addition to the files described above, you can find three other files in <code>&lt;testsuite&gt;/config/</code>:</p><ul><li><strong>dev-log4j.properties</strong>: log4j logger settings used in development mode  </li><li><strong>log4j.properties</strong>: log4j logger settings used in load test mode</li><li><strong>jvmargs.cfg</strong>: JVM settings for the agents and therefore only used in load test mode, e.g. settings for Java garbage collector tuning</li></ul><p>Also see <a href="http://logging.apache.org/log4j/1.2/apidocs/index.html">Apache Log4j API Docs</a> for more information on log4j settings. </p><h4 id="PropertyReplacements">Property Replacements</h4><p>In all XLT properties files, you can work with property replacements based on a <strong>${}</strong> syntax. You can define a property and then assign a value to another property by referring to the first property.</p><p>This is especially convenient for <code>project.properties</code> where properties are often defined for each test case to gain flexibility, but where, for example, the login data is identical for all test cases by default.</p><pre class="plain"><code>username = MyUsername
password = MySecretPassword
com.xceptance.xlt.samples.tests.TSearch.username = ${username}
com.xceptance.xlt.samples.tests.TSearch.password = ${password}
com.xceptance.xlt.samples.tests.webdriver.TSearch.username = ${username}
com.xceptance.xlt.samples.tests.webdriver.TSearch.password = ${password} 
</code></pre><h3 id="MavenIvy">Integrating XLT</h3><p>To better support test projects using dependency management systems like Maven or Ivy, Xceptance offers a public repository hosting all XLT releases, including their corresponding <em>POM</em> files. This facilitates updating the XLT version used in test projects. See below for examples on how to configure your test project to use the Xceptance repository.</p><blockquote class="note"><p>When configuring your test project to use a newer XLT version, do not forget to update XLT on your load machines as well. The version you&#8217;ve used to develop your test scripts must match the execution version of your load test environment.</p></blockquote><h4 id="IntegrationintoMaven">Integration into Maven</h4><p>To integrate XLT into your Maven project, copy and paste the following into your <code>pom.xml</code> file:</p><pre class="xml"><code>&lt;repositories&gt;
    &lt;!-- Declare Xceptance repository that hosts the XLT artifacts --&gt;
    &lt;repository&gt;
        &lt;id&gt;xceptance-releases&lt;/id&gt;
        &lt;url&gt;https://lab.xceptance.de/nexus/content/repositories/releases/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
...
&lt;dependencies&gt;
    ...
    &lt;!-- Declare XLT dependency --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.xceptance&lt;/groupId&gt;
        &lt;artifactId&gt;xlt&lt;/artifactId&gt;
        &lt;version&gt;4.3.0&lt;/version&gt;
    &lt;/dependency&gt;
    ...
&lt;/dependencies&gt;
</code></pre><h4 id="IntegrationintoIvy">Integration into Ivy</h4><p>To integrate XLT into your Ivy project, follow these steps: </p><ol><li>Declare the XLT repository as Maven-compatible resolver in your <code>ivysettings.xml</code> file.</li><li>Tell Ivy that your project depends on the artifact <code>xlt</code> of group <code>com.xceptance</code>. This is usually done in your <code>ivy.xml</code> file.</li></ol><h5 id="ivysettings.xml">ivysettings.xml</h5><pre class="xml"><code>&lt;ivysettings&gt;
    &lt;settings defaultResolver="resolver-chain" /&gt;
    &lt;resolvers&gt;
        &lt;chain name="resolver-chain"&gt;
            &lt;filesystem name="local"&gt;
                &lt;artifact pattern="${ivy.default.ivy.user.dir}/.ivy/local/[artifact]-[revision](-[classifier]).[ext]" /&gt;
            &lt;/filesystem&gt;
            &lt;!-- Declare Xceptance repository that hosts the XLT artifacts --&gt;
            &lt;ibiblio name="xceptance-releases" root="https://lab.xceptance.de/nexus/content/repositories/releases" m2compatible="true"/&gt;
        &lt;/chain&gt;
    &lt;/resolvers&gt;
    &lt;conflict-managers&gt;
        &lt;compatible-cm/&gt;
    &lt;/conflict-managers&gt;
&lt;/ivysettings&gt;
</code></pre><h5 id="ivy.xml">ivy.xml</h5><pre class="xml"><code>&lt;dependencies&gt;
    ...
    &lt;!-- Declare XLT dependency --&gt;
    &lt;dependency org="com.xceptance" name="xlt" rev="4.3.0" /&gt;
    ...
&lt;/dependencies&gt;
</code></pre><h2 id="LoadTest">Load and Performance Testing</h2><h3 id="Overview2">Overview</h3><h4 id="LoadModels">Load Models</h4><p>A load model defines the attributes you may influence to reach a specific load and performance behavior. XLT supports two load models:</p><ul><li>a user count model and</li><li>an arrival rate model.</li></ul><p>Both have different characteristics and use cases as illustrated below.  </p><h5 id="UserCountModel">User Count Model</h5><p>The <em>user count model</em> is a static and non-feedback-based load model.</p><p>When you use it, the load is determined by the number of concurrent users. When you configure a load of 10 users, for instance, XLT runs 10 threads repeatedly executing this scenario. At any given time during the test, the target system has to handle 10 concurrent users, no more, no less. The number of executions (transactions) that can be achieved during a certain period of time depends on the time the target system needs to respond.</p><p>This model is suited best for:</p><ul><li>a simple base line test (single-user test) to assess the base performance of the system under almost no load,</li><li>a real load or performance test to assess the performance under a high but <em>predictable</em> load,</li><li>a test that should be easily repeatable and whose load factor is not influenced by the system under test. </li></ul><h5 id="ArrivalRateModel">Arrival Rate Model</h5><p>The <em>arrival rate model</em> is a feedback-based load model. </p><p>When you use it, the target number of transactions per hour determines the load generation. For an arrival rate of 1,000 transactions per hour, XLT runs the respective scenario 1,000 times, equally distributed across a period of one hour. XLT uses as many concurrent users as necessary to fulfill the given arrival rate but no more than specified. Thus, if the time needed for a transaction is very short, only one user might suffice; if a transaction takes longer, the number of concurrent users will rise.</p><p>The number of concurrent users is not static and influenced by the response time. If the response time temporarily increases, for example due to a server-side background job, the user count may increase as well. As soon as the response times improve, the number of concurrent users will automatically decrease. This way, the generated load is somewhat unpredictable, at least in terms of concurrency.</p><p>The relationship between response times and concurrent users can lead to situations where more users cause more load and thus longer response times. Even more users are now required to run, which eventually causes the server to be overloaded. Even though this behavior appears pretty aggressive, it&#8217;s more realistic. Compare it to a real-world situation where a lot of people are waiting at the check-out counter at the end of a store, but customers are still coming in because they don&#8217;t know that people are waiting already. Or transferred to the online world: when you visit an online presence, you won&#8217;t know the system behaves poorly until you start acting. </p><blockquote class="note"><p>Even though the number of concurrent users is rather a result than an input value for this load model, XLT requires you to specify a user count. This number is used to impose an upper limit to the number of concurrent users, which may help you to restrict the total load on the system if you want to avoid a total overload resulting from the feedback loop. </p></blockquote><p>The arrival rate load model is suited best if the load test is meant to prove that a system is in fact able to handle a certain number of transactions per hour. Since this is the primary purpose of load and performance testing, this model is the best choice for most of your test tasks.</p><h4 id="LoadProfiles">Load Profiles</h4><p>While the load model defines what you can modify to achieve a certain load factor, the load profiles define how you apply these values over a period of time. XLT supports three different load profiles:</p><ul><li>static,</li><li>ramp-up, and</li><li>variable load.</li></ul><p>See below for their detailed explanation.</p><h5 id="StaticLoadProfile">Static Load Profile</h5><p>The load parameter remains unchanged during the test. This is the simplest profile. Note that the target systems must be able to handle the full load right from the beginning.</p><h5 id="RampupLoadProfile">Ramp-up Load Profile</h5><p>The load parameter is steadily increased. This allows the target system to warm up before the full load hits the system, for example to compile and optimize code or to fill caches. </p><p>The ramp-up behavior of the load parameter can be controlled by the following settings:</p><ul><li>initial value: the load parameter value to start with,</li><li>target value: the final load parameter as soon as the ramp-up period has finished,</li><li>step size: the increment added to the load parameter after each ramp-up step,</li><li>ramp-up period: the length of the ramp-up phase,</li><li>steady period: the period to keep the current parameter value until the next ramp-up step</li></ul><blockquote class="note"><p>The steady period and the ramp-up period settings are mutually exclusive. </p></blockquote><p>Use the steady period if you want to keep the load at a certain level for a defined time, no matter how long the total ramp-up phase will be. Use the ramp-up period if you want to finish the ramp-up process after a certain amount of time, no matter how long the resulting steady phases will be.</p><p>If an arrival rate is defined, the ramp-up parameters will be applied to the arrival rate. In case there&#8217;s no such definition, they will be applied to the user count.</p><h5 id="VariableLoadProfile">Variable Load Profile</h5><p>The variable load profile allows full control and lets you vary the load parameter freely during the test. Thus, the load may not only be constantly increased (compare ramp-up), but it can also be increased and decreased at any time. </p><p>The variable profile comes in handy when you want to combine different load levels within one test run, for example a test where phases with regular load alternate with peaks of much higher load. You can also imagine a test that models the load profile of a typical 24 hour day, maybe squeezed into a shorter period of time for a faster test turn-around.</p><p>To define how the load parameter should vary over time, you need to specify a load function. You do so by defining a sequence of time/value pairs, each denoting a point in time when the slope of the load function changes. When you connect the dots by a straight line, the final shape of the load function evolves.</p><p>Multiple time/value pairs can be specified by separating them using one or more spaces, commas, semi-colons, or tab characters. The time part can be given in all formats supported for <a href="#TimePeriodFormat">time periods</a>.</p><p>Imagine the following load function:</p><pre class="plain"><code>0/10, 60m/10, 60m/20, 70m/5
</code></pre><p>This sequence of time/value pairs defines a function that keeps the load parameter constant at 10 for an hour, doubles it to 20 in an instant, then immediately starts decreasing it to 5 throughout the next ten minutes, and eventually keeps it at 5 for the remaining test.</p><pre class="plain"><code>Load /\
     |
  20 +                         *
     |                         **
     |                         * *
     |                         *  * 
  10 +**************************   *
     |                              *          
   5 +                               **************************
     |
     +-------------------------+-----+---------------------------&gt;
     0                        60    70                       Time
</code></pre><p>Note that the time/value pairs must be sorted by their time in ascending order. You can also specify two pairs for a certain time span, which is useful when you want the load parameter to change immediately. If no pair is given for time 0, a pair &#8220;0/1&#8221; will be inserted automatically (implicitly causing a ramp-up behavior). Finally, if the load test runs longer than the last pair, the last known load parameter value will be kept stable.</p><h4 id="LoadTestPhases">Load Test Phases</h4><p>The execution of a test scenario during a load test can be divided into different phases: the initial delay, the warm-up period, the measurement period, and the shutdown period.</p><p>The <em>initial delay</em> is required only if you don&#8217;t want the test scenario to run right from the beginning of the load test, which is useful if this scenario depends on the results created by another test scenario. The initial delay is optional.</p><p>To minimize discrepancies that could be caused by applications and other systems starting up and not yet operating at an optimal level, you can define a <em>warm-up period</em> as the time given before any measurements are taken. The warm-up period is optional. Keep in mind that it creates a period of time during which you don&#8217;t have any insights into your test. It is recommended to omit a warm-up period and modify the report later by specifying a time filter instead. This ensures that you don&#8217;t miss any important information.</p><p>The test is measured during the <em>measurement period</em>. As suggested by its name, this is the only time period where measurements are taken. The measurement period is a required setting.</p><p>To ensure that a test scenario runs to completion even if the measurement period is over, you can set a <em>shutdown period</em> throughout which the users continue to run (without taking measurements though) and try to orderly finish their current iteration. This comes in handy when things need to be cleaned up at the end of the test scenario. As soon as the last iteration is finished, the users automatically stop. If, however, it&#8217;s still not finished at the end of the shutdown period, the users will be forcibly terminated. The shutdown period is optional, but note that if no shutdown period is defined, the users are terminated right after the measurement period.</p><p>The ramp-up period (as defined above as part of the load profile) is commonly put into the warm-up period to ensure the system under test is working at an optimal level before any measurements are taken. That&#8217;s up to you though &#8211; it might as well be interesting to measure the system performance during the ramp-up phase. In that case, a warm-up period mustn&#8217;t be defined.</p><p>The following figure displays the phases in relation to the total test time:</p><p class="illustration"><a href="images/user-manual/load-testprofile-configuration.png"><img alt="Load Test Profile Configuration" title="Load Test Profile Configuration" border="0" src="images/user-manual/load-testprofile-configuration-small.jpg"/></a> <span class="caption">Load Test Phases</span></p><h4 id="LoadTestEnvironment">Load Test Environment</h4><p>Typically, a distributed load generation environment is needed to generate enough load. This requires a cluster of test machines. XLT has to be installed on each of these machines:</p><p class="illustration"><a href="images/user-manual/load-generation-environment.png"><img alt="Load generation environment" title="Load generation environment" border="0" src="images/user-manual/load-generation-environment-small.jpg"/></a> <span class="caption">Load Generation Environment</span></p><ul><li><strong>Master controller</strong>: The master controller can be seen as the &#8220;brain&#8221; of the load test environment. It deploys the test suite to all load machines, evenly distributes the load, and starts/stops the load test. A test cluster may only have one master controller.</li><li><strong>Agent controller</strong>: Since the master controller doesn&#8217;t have direct access to the remote load machines, it needs the agent controller as counterpart on these machines. It acts on behalf of the master controller.</li><li><strong>Agent</strong>: The agent is the component that actually executes the test suite against the system under test. It is started and stopped by the agent controller.</li></ul><h3 id="LoadTestEnvironmentConfiguration">Load Test Environment Configuration</h3><p>Before you can start the load test, configure both the XLT load generation environment and your test suite as outlined below. </p><p>These property files are used to configure the main components of the XLT load generation runtime:</p><ul><li><code>&lt;XLT&gt;/config/agentcontroller.properties</code> - Agent Controller Configuration</li><li><code>&lt;XLT&gt;/config/mastercontroller.properties</code> - Master Controller Configuration</li></ul><h4 id="AgentControllerConfiguration">Agent Controller Configuration</h4><p>Inside the agent controller configuration file, you can define these properties:</p><h5 id="PortNumber">Port Number</h5><p>Port number the agent controller is listening on. Default is 8500. You can pick any free port number, but make sure that the corresponding master controller entry matches that number. Also ensure that the firewall rules in place allow unrestricted communication. The used protocol is HTTPS. If you want to run more than one agent controller per machine, be aware that all controllers have to use different port numbers.</p><pre class="plain"><code>com.xceptance.xlt.agentcontroller.port = &lt;portnumber&gt;
</code></pre><h5 id="KeyStoreCredentials">Key Store Credentials </h5><p>The credentials your key store is encrypted with. You only need to change this if your Java key store password has been modified from the default.</p><pre class="plain"><code>com.xceptance.xlt.agentcontroller.keystore.password = &lt;password&gt;
com.xceptance.xlt.agentcontroller.keystore.key.password = &lt;password&gt; 
</code></pre><h5 id="AgentControllerLogging">Agent Controller Logging</h5><p>The properties below serve to configure the agent controller logging facility. They only affect the agent controller output and don&#8217;t alter the logging of your test code. Most of the time, a modification is not required here.</p><pre class="plain"><code>log4j.rootLogger = info, console, file
log4j.appender.console = org.apache.log4j.ConsoleAppender
log4j.appender.console.layout = org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern = [%d{HH:mm:ss,SSS}] %-5p [%t] - %m%n
</code></pre><p>Also see <a href="http://logging.apache.org/log4j/1.2/apidocs/index.html">Apache Log4j API Docs</a> for more information on log4j settings.</p><h4 id="MasterControllerConfiguration">Master Controller Configuration</h4><p>Inside the master controller configuration file, you can define these properties:</p><h5 id="TestSuiteLocation">Test Suite Location</h5><p>To determine the test suite you want to use for the load test, you need to specify its location either as absolute path or relative to your XLT installation. It is uploaded to the agent controllers from there.</p><pre class="plain"><code>com.xceptance.xlt.mastercontroller.testSuitePath = &lt;location&gt;
</code></pre><p>For example:</p><pre class="plain"><code>com.xceptance.xlt.mastercontroller.testSuitePath = samples/testsuite-posters
</code></pre><blockquote class="note"><p>When running the load test on and from Windows, make sure to use the correct encoding for backslashes because the property file format uses backslashes to quote other special characters. Therefore, you have to quote the backslash with an additional backslash to ensure its original meaning, e.g. <code>c:\\test\\mysuite</code>.</p></blockquote><h5 id="UpdateInterval">Update Interval</h5><p>Defines how often the master controller prints the status of the currently running load test to the console:</p><pre class="plain"><code>com.xceptance.xlt.mastercontroller.ui.status.updateInterval = &lt;time in seconds&gt;
</code></pre><h5 id="StatusDisplay">Status Display</h5><p>Whether or not to display detailed status information for each simulated test user. If set to <em>false</em>, status information will be aggregated into one line per user type. If you have many test users running, it can be helpful to set it to false because you might get overwhelmed by the amount of information presented otherwise. Being a display property, it doesn&#8217;t change the data collection but the final data presentation. </p><pre class="plain"><code>com.xceptance.xlt.mastercontroller.ui.status.detailedList = &lt;true/false&gt;
</code></pre><h5 id="AgentControllers">Agent Controllers</h5><p>This property lists the locations of the agent controllers you want the master controller to use: </p><pre class="plain"><code>com.xceptance.xlt.mastercontroller.agentcontrollers.&lt;id&gt;.url = &lt;url&gt; 
com.xceptance.xlt.mastercontroller.agentcontrollers.&lt;id&gt;.weight = &lt;weight&gt; 
</code></pre><p>You can use any name for the <code>&lt;id&gt;</code> part of the property. It is recommended to resort to name and number combinations, such as ac1 for the first agent controller or blade01-02 for the second agent controller on the first blade. Make sure the agent controller IDs differ from each other because otherwise a later entry in the file will overwrite the previous one.</p><p>To simultaneously use load machines of different power in a load cluster, you may specify a &#8220;weight&#8221; for each agent controller (defaults to 1 if not set). This value influences the automatic distribution of virtual users across the load machines. A machine with a weight of 3 gets 3 times the load of a machine with a weight of 1.</p><pre class="plain"><code>com.xceptance.xlt.mastercontroller.agentcontrollers.ac1.url = https://localhost:8500
com.xceptance.xlt.mastercontroller.agentcontrollers.ac1.weight = 1
com.xceptance.xlt.mastercontroller.agentcontrollers.ac2.url = https://localhost:8501
com.xceptance.xlt.mastercontroller.agentcontrollers.ac2.weight = 3
</code></pre><h5 id="MasterControllerLogging">Master Controller Logging</h5><p>You can set a different logging behavior for the master controller, which helps to solve problems and provides information in case of support inquiries:</p><pre class="plain"><code>log4j.rootLogger = debug, file
log4j.appender.console = org.apache.log4j.ConsoleAppender
log4j.appender.console.layout = org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern = [%d{HH:mm:ss,SSS}] %-5p [%t] - %m%n
</code></pre><h3 id="TestSuiteConfiguration">Test Suite Configuration</h3><p>The test suite itself is configured independently from the master controller. All properties are read from the <code>&lt;testsuite&gt;/config</code> directory. The sections below outline the settings relevant to load testing. See <a href="user-manual.html#TestSuiteConfig">Test Suite and Framework Configuration</a> for details on all configuration files and properties.</p><h4 id="DefaultConfigurationdefault.properties">Default Configuration &#8211; default.properties  </h4><h5 id="ResultDirectoryLocation">Result Directory Location</h5><p>Specifies the directory location where you want to store load test results. Normally, there&#8217;s no need to change it.  </p><pre class="plain"><code>com.xceptance.xlt.result-dir = &lt;directory path&gt;
</code></pre><h5 id="ErrorBehavior">Error Behavior</h5><p>Specifies the framework behavior in case of an error, that is whether or not the framework should abort a transaction if any of the following occurs:</p><ul><li>While loading a page &#8211; If an HTTP error occurred while loading a page.</li><li>Page resource unavailable &#8211; If an HTTP error occurred while loading a resource embedded in a page.</li><li>Java script error &#8211; If a JavaScript error occurred.</li><li>Agent termination in case of server errors &#8211; Maximum number of errors allowed before an agent terminates, which helps to automatically stop unobserved, long-running test cases in the event of severe error conditions, such as unavailability of the system under test. The number of errors specified here is the error count per running agent controller.</li></ul><pre class="plain"><code>com.xceptance.xlt.stopTestOnHttpErrors.page = &lt;true/false&gt;
com.xceptance.xlt.stopTestOnHttpErrors.embedded = &lt;true/false&gt;
com.xceptance.xlt.stopTestOnJavaScriptErrors = &lt;true/false&gt;
com.xceptance.xlt.maxErrors = &lt;number of errors per agent controller&gt;
</code></pre><h5 id="ThinkTimes">Think Times</h5><p>To specify the think time between two subsequent actions or transactions, use the properties below. If a random think time is needed, set the appropriate deviation to a value greater than 0. It specifies the maximum deviation from think time in milliseconds. The respective value is added to or subtracted from the think time using a pseudo-random, uniform distribution.</p><pre class="plain"><code>com.xceptance.xlt.thinktime.action = &lt;time in [ms]&gt;
com.xceptance.xlt.thinktime.action.deviation = &lt;time in [ms]&gt;
com.xceptance.xlt.thinktime.transaction = &lt;time in [ms]&gt;
com.xceptance.xlt.thinktime.transaction.deviation = &lt;time in [ms]&gt;
</code></pre><p>The think time configuration might look like this, for instance:</p><pre class="plain"><code>com.xceptance.xlt.thinktime.action = 100
com.xceptance.xlt.thinktime.action.deviation = 50
com.xceptance.xlt.thinktime.transaction = 0
com.xceptance.xlt.thinktime.transaction.deviation = 0
</code></pre><p>This sets the action think times between 50 and 150ms and no transaction think time whatsoever.</p><blockquote class="note"><p>Note that the deviation has to be smaller than the specified base think time.</p></blockquote><h4 id="TestProjectConfigurationproject.properties2">Test Project Configuration &#8211; project.properties </h4><p>To configure your test project, edit the file <code>project.properties</code>.</p><h5 id="TestPropertiesFile">Test Properties File </h5><p>XLT permits to prepare and use multiple <code>test.properties</code> files for easy maintenance of test setups. This facilitates switching between test setups and prevents configuration errors. This property doesn&#8217;t allow the use of a path-specific file name. The test definition files reside in the same directory as the <code>project.properties</code> file. </p><pre class="plain"><code>com.xceptance.xlt.testPropertiesFile = &lt;filename&gt;.properties
</code></pre><h5 id="TestClassMapping">Test Class Mapping</h5><p>Specifies which test IDs should be used by XLT and, more specifically, which test ID uses which test case implementation. That&#8217;s why you have to specify the fully qualified class names of your tests here. Note that you can map the same class to multiple load test names if needed. This is extremely useful when you want to run the same test case in different configurations.</p><pre class="plain"><code>com.xceptance.xlt.loadtests.&lt;name&gt;.class = &lt;fully qualified class name&gt;
</code></pre><p>A test class mapping might look like this:</p><pre class="plain"><code>com.xceptance.xlt.loadtests.TVisitor.class = com.xceptance.xlt.samples.tests.TVisitor
com.xceptance.xlt.loadtests.TJSVisitor.class = com.xceptance.xlt.samples.tests.TJSVisitor
</code></pre><h5 id="TestClassSpecificSettings">Test Class-Specific Settings</h5><p>You can define project-wide settings that are test case-specific but not test run-specific by using the following syntax:</p><pre class="plain"><code>&lt;fully-qualified class name&gt;.&lt;property-name&gt; = &lt;value&gt;
</code></pre><p>For example:</p><pre class="plain"><code>com.xceptance.xlt.samples.tests.shop-url = http://localhost:8080/posters/
com.xceptance.xlt.samples.tests.TAuthor.username = username
com.xceptance.xlt.samples.tests.TAuthor.password = password
com.xceptance.xlt.samples.tests.webdriver.TAuthor.write-count = 2
</code></pre><h4 id="LoadTestProfileConfigurationtest.properties2">Load Test Profile Configuration &#8211; test.properties</h4><p>Test run-specific settings &#8211; You can also configure an (optional) property file containing the settings specific to a certain load test run. You may define more than one test property file, such as <code>test-target-load.properties</code> and <code>test-2x-target-load.properties</code>. This way, many configurations can be defined and prepared in advance and used as needed. You switch between these files by changing the property <code>com.xceptance.xlt.testPropertiesFile</code> in the <code>project.properties</code> file. </p><p>Load test profile configurations are done inside your test property file, which is named <code>test.properties</code> by default. Using the syntax below, you can define test ID, the number of virtual users, and all other load test-specific settings of tests meant to run in parallel agents:</p><pre class="plain"><code>com.xceptance.xlt.loadtests.&lt;testID&gt;.&lt;setting&gt; = &lt;value&gt;
</code></pre><p>For <code>&lt;testID&gt;</code>, use any appropriate name. The following table lists all supported values for <code>&lt;setting&gt;</code>; required settings are displayed in bold face:</p><table><tr><th>Setting </th><th>Description </th></tr><tr><td>class</td><td>Fully qualified class name of the test case (REQUIRED if not specified in project.properties)</td></tr><tr><td><strong>users</strong></td><td>Number of threads that run the test in parallel (REQUIRED), may be a load function</td></tr><tr><td>iterations</td><td>Number of iterations per thread</td></tr><tr><td>arrivalRate</td><td>Number of transactions per hour, may be a load function</td></tr><tr><td>initialDelay</td><td>Number of seconds to wait at the beginning</td></tr><tr><td>warmUpPeriod</td><td>Number of seconds to run without performing measurements</td></tr><tr><td><strong>measurementPeriod</strong></td><td>Number of seconds to perform measurements (REQUIRED)</td></tr><tr><td>shutdownPeriod</td><td>Number of seconds to continue without performing measurements</td></tr><tr><td>rampUpPeriod</td><td>Number of seconds to steadily increase the user count</td></tr><tr><td>rampUpStepSize</td><td>Number of users to stepwise increase the load during ramp-up</td></tr><tr><td>rampUpSteadyPeriod</td><td>Number of seconds between ramp-up steps</td></tr><tr><td>rampUpInitialValue</td><td>Number of users when starting ramp-up</td></tr><tr><td>loadFactor</td><td>A factor to be applied to users (and arrivalRate if defined). Use this value to scale the load up/down.</td></tr></table><p>A sample load profile configuration is given below:</p><pre class="plain"><code>com.xceptance.xlt.loadtests = TAuthor
com.xceptance.xlt.loadtests.TAuthor.users = 5
com.xceptance.xlt.loadtests.TAuthor.iterations = 100
com.xceptance.xlt.loadtests.TAuthor.arrivalRate = 3600
com.xceptance.xlt.loadtests.TAuthor.initialDelay = 0
com.xceptance.xlt.loadtests.TAuthor.warmUpPeriod = 30s
com.xceptance.xlt.loadtests.TAuthor.measurementPeriod = 10m 0s
</code></pre><p id="TimePeriodFormat">All time period values can be specified in one of the following formats (without quotes):</p><ul><li>total number of seconds: &#8216;1234s&#8217; or '1234'</li><li>natural style: &#8216;0h 12m 0s&#8217;, &#8216;0h 12m&#8217;, &#8216;12m 0s&#8217;, or '12m'</li><li>digit style: &#8216;1:23&#8217;, &#8216;01:23&#8217;, &#8216;0:1:23&#8217;, or &#8216;0:01:23&#8217; </li></ul><p>If you want to run several test cases simultaneously, specify the test case names as value for the property <code>com.xceptance.xlt.loadtests</code> in form of a space-separated list:</p><pre class="plain"><code>com.xceptance.xlt.loadtests = TAuthor TVisitor TCrawler
com.xceptance.xlt.loadtests.TAuthor.users = 5
com.xceptance.xlt.loadtests.TVisitor.users = 3
com.xceptance.xlt.loadtests.TCrawler.users = 4 
</code></pre><h5 id="SampleConfigurations">Sample Configurations</h5><p><em>User Count Model With Constant Load Profile</em></p><pre class="plain"><code>com.xceptance.xlt.loadtests.TAuthor.users = 5
</code></pre><p>Runs exactly 5 users right from the beginning.</p><p><em>User Count Model With Ramp-Up Load Profile</em></p><pre class="plain"><code>com.xceptance.xlt.loadtests.TAuthor.users = 50
com.xceptance.xlt.loadtests.TAuthor.rampUpInitialValue = 10
com.xceptance.xlt.loadtests.TAuthor.rampUpPeriod = 5m
</code></pre><p>Runs exactly 50 users, but ramps up the user count from 10 to 50 over a period of 5 minutes.</p><p><em>User Count Model With Variable Load Profile</em></p><pre class="plain"><code>com.xceptance.xlt.loadtests.TAuthor.users = 0/5 1h/50 2h/50 2h/100 3h/20
</code></pre><p>Runs the TAuthor scenario with a variable number of concurrent users (5&nbsp;&rarr;&nbsp;50&nbsp;&rarr;&nbsp;100&nbsp;&rarr;&nbsp;20). </p><p><em>Arrival Rate Model With Constant Load Profile</em></p><pre class="plain"><code>com.xceptance.xlt.loadtests.TAuthor.users = 5
com.xceptance.xlt.loadtests.TAuthor.arrivalRate = 100
</code></pre><p>Runs the TAuthor scenario exactly 100 times per hour with at most 5 concurrent users.</p><p><em>Arrival Rate Model With Ramp-Up Load Profile</em></p><pre class="plain"><code>com.xceptance.xlt.loadtests.TAuthor.users = 5
com.xceptance.xlt.loadtests.TAuthor.arrivalRate = 100
com.xceptance.xlt.loadtests.TAuthor.rampUpInitialValue = 50
com.xceptance.xlt.loadtests.TAuthor.rampUpSteadyPeriod = 1m
com.xceptance.xlt.loadtests.TAuthor.rampUpStepSize = 10
</code></pre><p>Runs the TAuthor scenario exactly 100 times per hour with at most 5 concurrent users, but starts with an arrival rate of 50 per hour and increases it by 10 every minute until the target level of 100 is reached.</p><p><em>Arrival Rate Model With Variable Load Profile</em></p><pre class="plain"><code>com.xceptance.xlt.loadtests.TAuthor.users = 5 
com.xceptance.xlt.loadtests.TAuthor.arrivalRate = 0/50  1h/100 2h/200 3h/150 
</code></pre><p>Runs the TAuthor scenario with a variable arrival rate (50&nbsp;&rarr;&nbsp;100&nbsp;&rarr;&nbsp;200&nbsp;&rarr;&nbsp;150) and with at most 5 concurrent users.</p><p><em>Arrival Rate Model With Ramp-Up Load Profile and Load Factor</em></p><pre class="plain"><code>com.xceptance.xlt.loadtests.TAuthor.users = 5
com.xceptance.xlt.loadtests.TAuthor.arrivalRate = 100
com.xceptance.xlt.loadtests.TAuthor.rampUpPeriod = 5m
com.xceptance.xlt.loadtests.TAuthor.loadFactor = 2.4
</code></pre><p>Runs the TAuthor scenario exactly 240 times per hour with at most 12 concurrent users, but ramps up the arrival rate from 1/h to 240/h over a period of 5 minutes.</p><h4 id="OtherSettings">Other Settings</h4><p>In case you want to modify the behavior of the logging facility of the load test agents, the test suite configuration directory contains a file named <code>log4j.properties</code> that can be changed to satisfy your needs.</p><p>To launch the JVM that runs the agent with additional parameters, specify them in the <code>jvmargs.cfg</code> file.</p><h3 id="RunLoadTest">Run the Load Test</h3><p>Running load tests consists of two steps: </p><ol><li>running the agent controllers and</li><li>running the master controller.</li></ol><h4 id="RunningtheAgentControllers">Running the Agent Controllers</h4><p>To start the agent controllers, open a command line window/console and type the following command sequence:</p><pre class="plain"><code>cd &lt;XLT&gt;/bin
./agentcontroller.sh

</code></pre><blockquote class="note"><p>Windows users have to use the appropriate <code>.cmd</code> file located in the same directory.</p></blockquote><p>The agent controller starts up and listens on the specified port. The output looks like this:</p><pre class="plain"><code>- Using "C:\Users\AppData\Local\Temp\vfs_cache" as temporary files store.
- Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
- jetty-6.1.19
- Started SslSocketConnector@0.0.0.0:8500
</code></pre><h4 id="RunningtheMasterController">Running the Master Controller</h4><blockquote class="note"><p>Before starting the master controller, make sure all agent controllers are running on all respective load test machines. The master controller cannot be started if the agent controllers aren&#8217;t running. Also check that the test suite has been compiled successfully to avoid errors when uploading it.</p></blockquote><p id="MasterControllerModes">You can start the master controller in one of the following modes:</p><ul><li><strong>Interactive mode</strong>: typical sequence of steps to be executed to run a load test</li><li><strong>Auto mode</strong>: load test is started automatically, without user interaction</li><li><strong>Embedded mode</strong>: running a load test where master controller and agent controller run inside the same JVM</li></ul><h5 id="InteractiveMode">Interactive Mode</h5><p>To start the master controller in interactive mode, use this command line:</p><pre class="plain"><code>cd &lt;XLT&gt;/bin
./mastercontroller.sh

</code></pre><blockquote class="note"><p>Windows users have to use the appropriate <code>.cmd</code> file located in the same directory.</p></blockquote><p>A screen appears that displays the command line menu as below:</p><pre class="plain"><code>Xceptance LoadTest 4.2.0
Copyright (c) 2005-2012 Xceptance Software Technologies GmbH. All rights reserved.
Basic License (5 virtual users). This license does not expire.


(u) Upload test suite
(s) Start agents
(a) Abort agents
(r) Show agent status
(d) Download test results
(c) Create load test report
(q) Quit
=&gt;

</code></pre><p>The following options are offered:</p><ul><li><strong>Upload agent files (u)</strong>: Choose this option to upload your test suite (code, data, and configuration) to all configured agent controllers. This is required at the very beginning and each time you&#8217;ve modified your test suite. XLT will only upload files that have been changed to speed up testing.</li></ul><ul><li><strong>Start agents (s)</strong>: All agent controllers will receive a start command to spin off an agent executing its configured tests. This effectively starts the load test.</li></ul><ul><li><strong>Abort agents (a)</strong>: Choose this option to immediately terminate any running load agent.</li></ul><ul><li><strong>Show agent report (r)</strong>: The current status of the load agents can be monitored by choosing this option. Depending on the configuration, either a short summary (per test case) or a detailed list (per test user) is shown. In either case you get information about:<ul><li>test case name on which you are running the load test,</li><li>how many users are running,</li><li>how often a test case has been executed so far,</li><li>how long it took on average to execute the test case,</li><li>how many events and errors occurred, and</li><li>the overall progress.</li></ul></li></ul><ul><li><strong>Download test results (d)</strong>: Each load agent writes log files and runtime data files. Choose this option to download this data from all configured agent controllers. After entering the <em>d</em> command, a menu appears where you can choose the amount of data to download. Press 1, 2 or 3 here. The files are saved to a newly created directory at the location specified in <code>default.properties</code>. By default, the result directory is set to <code>&lt;XLT&gt;/results</code>. The name of the new directory is given by the current date and time, for example: <code>20110501-161718</code>.</li></ul><ul><li><strong>Create report (c)</strong>: Generates a load test report of the last downloaded test results.</li></ul><ul><li><strong>Quit (q)</strong>: Shuts down the master controller and closes its connections to the agent controllers. Note that this will <em>not</em> stop any running load test. The load agents continue to execute the load test until they have finished. To regain control, reconnect to the test cluster by restarting the master controller.</li></ul><p>As soon as you&#8217;ve chosen an option (by pressing the associated key followed by <code>ENTER</code>), the appropriate action is executed. Afterwards, you immediately return to the menu (unless you&#8217;ve chosen to quit, of course).</p><p>A typical usage scenario for a load test is reflected by the order of the master controller menu items and might look like this:</p><ol><li>Upload the test suite (using (u) shortcut)</li><li>Start the agents (using (s) shortcut)</li><li>Check the agent status regularly (using (r) shortcut)</li><li>Download the test results as soon as the test has finished (using (d) shortcut)</li><li>Create a report of the downloaded results (using (c) shortcut)</li><li>Quit the master controller (using (q) shortcut)</li></ol><h5 id="AutoMode">Auto Mode</h5><p>As outlined in the previous sections, there is a typical sequence of steps to be executed when running a load test. It may quickly become tedious and error-prone to type the necessary keys over and over again. To avoid this repetition, XLT provides another operating mode: the <em>auto</em> mode. In this mode, all the steps mentioned above are executed automatically, without any user interaction. To start XLT in this operating mode, use the following command line:</p><p>Unix-based systems: </p><pre class="plain"><code>cd &lt;XLT&gt;/bin
./mastercontroller.sh -auto

</code></pre><p>Windows: </p><pre class="plain"><code>cd &lt;XLT&gt;\bin
mastercontroller.cmd -auto

</code></pre><p>If the test suite files were uploaded and the load agents started successfully, XLT automatically refreshes the agent status on a regular basis. As soon as the test has finished, the test results are downloaded and XLT quits. </p><p>If the command is followed by the option <code>-report</code>, a load test and performance report will be automatically generated after the test has finished and the results have been downloaded. </p><pre class="plain"><code>cd &lt;XLT&gt;\bin
mastercontroller.cmd -auto -report

</code></pre><p>See below for what the screen displays in <em>auto</em> mode:</p><pre class="plain"><code>Xceptance LoadTest 4.2.0
Copyright (c) 2005-2012 Xceptance Software Technologies GmbH. All rights reserved.
Basic License (5 virtual users). This license does not expire.

Uploading test suite ...
    0% ... 10% ... 20% ... 30% ... 40% ... 50% ... 60% ... 70% ... 80% ... 100% - OK

Starting agents ...
     0% ... 100% - OK

Test Case       State         Running Users   Iterations   Last Time   Avg. Time   Total Time      Events   Errors   Progress
-------------   --------   ----------------   ----------   ---------   ---------   ----------   ---------   ------   --------
TAddToCart_lw   Running        10 of     10            0      0,00 s      0,00 s      0:00:00           0        0         0%
TAddToCart      Running        10 of     10            0      0,00 s      0,00 s      0:00:00           0        0         0%
TCreateUser     Running        10 of     10            1      0,72 s      0,72 s      0:00:01           0        0         0%

Test Case       State         Running Users   Iterations   Last Time   Avg. Time   Total Time      Events   Errors   Progress
-------------   --------   ----------------   ----------   ---------   ---------   ----------   ---------   ------   --------
TAddToCart_lw   Running        10 of     10           72      0,67 s      0,77 s      0:00:06           0        2         5%
TAddToCart      Running        10 of     10           55      0,70 s      1,03 s      0:00:06           0        0         5%
TCreateUser     Running        10 of     10           83      0,95 s      0,67 s      0:00:06           0       17         5%

.
.
.

Test Case       State         Running Users   Iterations   Last Time   Avg. Time   Total Time      Events   Errors   Progress
-------------   --------   ----------------   ----------   ---------   ---------   ----------   ---------   ------   --------
TAddToCart_lw   Running        10 of     10        1.472      0,69 s      0,66 s      0:01:37          17       65        96%
TAddToCart      Running        10 of     10        1.412      0,66 s      0,68 s      0:01:37           0        0        96%
TCreateUser     Running        10 of     10        1.525      0,91 s      0,63 s      0:01:37           0      316        96%

Test Case       State         Running Users   Iterations   Last Time   Avg. Time   Total Time      Events   Errors   Progress
-------------   --------   ----------------   ----------   ---------   ---------   ----------   ---------   ------   --------
TAddToCart_lw   Finished        0 of     10        1.533      1,16 s      0,65 s      0:01:41          17       65       100%
TAddToCart      Finished        0 of     10        1.476      1,17 s      0,68 s      0:01:40           0        0       100%
TCreateUser     Finished        0 of     10        1.590      0,79 s      0,63 s      0:01:41           0      325       100%

Downloading test results ... (Please be patient, this might take a while)
    0% ... 30% ... 60% ... 100% - OK

</code></pre><p>To abort the test prematurely, press <code>CTRL-C</code> to terminate the master controller. This terminates all running agents as well and triggers the download of all test results generated so far. Note that it&#8217;s therefore impossible to disconnect the master controller from the test cluster while keeping the load test running.</p><blockquote class="note"><p>For long-running load tests, it is recommended to run the test without the <code>-auto</code> option because this allows a disconnect from the test and inhibits accidental test termination. </p></blockquote><h5 id="EmbeddedMode">Embedded Mode</h5><p>Both interactive mode and auto mode can be combined with the command line option <code>-embedded</code>. It starts the master controller together with an internal agent controller. </p><p>This is useful if you want to run load tests without a distributed load test environment, but run just one agent controller together with the master controller on the same machine. There&#8217;s no need to manually start an agent controller before you run the load test, which facilitates the handling of automated load tests started from within a build process. This option is also recommended when playing around with the <a href="user-manual.html#PosterDemo">posters demo</a> for training purposes because it simplifies the process of running a load test. </p><blockquote class="note"><p>When you use the <code>-embedded</code> option, the local agent controller settings will override the set of agent controllers configured in <code>mastercontroller.properies</code>.</p></blockquote><h2 id="TestResultsandReports">Test Results and Reports</h2><h3 id="CollectedValues">Collected Values   </h3><p>When running a load test, the XLT framework automatically collects a lot of information about the transactions, actions, and requests being executed and certain events. Additional custom timers and events can be added programmatically using the XLT API. Last but not least, each agent process monitors its resource usage and logs these values as well. All this data will later be the source for the XLT load test report.</p><p>These values are stored &#8212; separately for each test case and each virtual user &#8212; in a file named <code>results/&lt;TestCaseName&gt;/&lt;UserNo&gt;/timers.csv</code>. Agent resource usage data will be written to <code>results/Agent-JVM-Monitor/0/timers.csv</code>. As the name already suggests, the file format is CSV. See the following snippet for an example:</p><pre class="plain"><code>R,PublishArticle.1,1366360224994,25,false,566,48930,200,http://localhost:8080/pebble/manageBlogEntry.secureaction,text/html,0,0,15,8,15,23
R,PublishArticle.7,1366360225027,19,false,456,653,200,http://localhost:8080/pebble/dwr/interface/Pebble.js,text/plain,0,0,2,0,2,2
R,PublishArticle.3,1366360225027,22,false,446,43876,200,http://localhost:8080/pebble/dwr/engine.js,text/javascript,0,0,5,0,5,5
E,Failed to download resource,1366360225027,TAuthor,==[404]== http://localhost:8080/themes/default/images/favicon.ico
A,PublishArticle,1366360224993,76,false
R,ConfirmPublishing.1,1366360225141,62,false,609,134,302,http://localhost:8080/pebble/publishBlogEntry.secureaction,,0,0,60,0,60,60
R,ConfirmPublishing.2,1366360225189,15,false,462,55555,200,http://localhost:8080/pebble/2013/04/19/1366360200018.html,text/html,0,0,9,5,9,14
R,ConfirmPublishing.8,1366360225221,15,false,457,653,200,http://localhost:8080/pebble/dwr/interface/Pebble.js,text/plain,0,0,2,0,2,2
R,ConfirmPublishing.4,1366360225221,16,false,447,43876,200,http://localhost:8080/pebble/dwr/engine.js,text/javascript,0,0,3,0,3,3
A,ConfirmPublishing,1366360225141,108,false
R,Logout.1,1366360225348,19,false,489,200,302,http://localhost:8080/pebble/logout.action?redirectUrl=http://localhost:8080/pebble/,,0,0,3,0,3,3
R,Logout.2,1366360225364,35,false,477,51601,200,http://localhost:8080/pebble/,text/html,0,0,15,8,15,23
R,Logout.8,1366360225395,5,false,471,653,200,http://localhost:8080/pebble/dwr/interface/Pebble.js,text/plain,0,0,1,0,1,1
R,Logout.4,1366360225395,6,false,461,43876,200,http://localhost:8080/pebble/dwr/engine.js,text/javascript,0,0,2,0,2,2
A,Logout,1366360225347,74,false
T,TAuthor,1366360222893,2599,false,
</code></pre><p>As you can see, the lines can have a different number of columns as they represent different types of information. The following table explains the meaning of each column depending on the data record type:</p><table><tr><th>Column</th><th>Transaction</th><th>Action</th><th>Request</th><th>Custom Timer</th><th>Event</th><th>Agent Resource Usage</th></tr><tr><td style="text-align: center;"><strong>1</strong></td><td>type code (T)</td><td>type code (A)</td><td>type code (R)</td><td>type code (C)</td><td>type code (E)</td><td>type code (J)</td></tr><tr><td style="text-align: center;"><strong>2</strong></td><td>name</td><td>name</td><td>name</td><td>name</td><td>name</td><td>agent name</td></tr><tr><td style="text-align: center;"><strong>3</strong></td><td>start time</td><td>start time</td><td>start time</td><td>start time</td><td>time</td><td>time</td></tr><tr><td style="text-align: center;"><strong>4</strong></td><td>run time [ms]</td><td>run time [ms]</td><td>run time [ms]</td><td>run time [ms]</td><td>transaction name</td><td>current CPU usage (agent only) [%]</td></tr><tr><td style="text-align: center;"><strong>5</strong></td><td>failed flag</td><td>failed flag</td><td>failed flag</td><td>failed flag</td><td>event message</td><td>used main memory (absolute)</td></tr><tr><td style="text-align: center;"><strong>6</strong></td><td>exception stack trace if failed</td><td>- </td><td>bytes sent</td><td>- </td><td>- </td><td>current main memory usage (relative) [%]</td></tr><tr><td style="text-align: center;"><strong>7</strong></td><td>- </td><td>- </td><td>bytes received</td><td>- </td><td>- </td><td>used heap memory (absolute)</td></tr><tr><td style="text-align: center;"><strong>8</strong></td><td>- </td><td>- </td><td>response code</td><td>- </td><td>- </td><td>total heap memory (absolute)</td></tr><tr><td style="text-align: center;"><strong>9</strong></td><td>- </td><td>- </td><td>request URL</td><td>- </td><td>- </td><td>current heap memory usage (relative) [%]</td></tr><tr><td style="text-align: center;"><strong>10</strong></td><td>- </td><td>- </td><td>response content type</td><td>- </td><td>- </td><td>threads in state "runnable"</td></tr><tr><td style="text-align: center;"><strong>11</strong></td><td>- </td><td>- </td><td>connect time [ms]</td><td>- </td><td>- </td><td>threads in state "blocked"</td></tr><tr><td style="text-align: center;"><strong>12</strong></td><td>- </td><td>- </td><td>send time [ms]</td><td>- </td><td>- </td><td>threads in state &#8220;waiting&#8221; or "timed waiting"</td></tr><tr><td style="text-align: center;"><strong>13</strong></td><td>- </td><td>- </td><td>server busy time [ms]</td><td>- </td><td>- </td><td>minor GC cycles since start</td></tr><tr><td style="text-align: center;"><strong>14</strong></td><td>- </td><td>- </td><td>receive time [ms]</td><td>- </td><td>- </td><td>minor GC time since start [ms]</td></tr><tr><td style="text-align: center;"><strong>15</strong></td><td>- </td><td>- </td><td>time to first bytes [ms]</td><td>- </td><td>- </td><td>current minor GC CPU usage [%]</td></tr><tr><td style="text-align: center;"><strong>16</strong></td><td>- </td><td>- </td><td>time to last bytes [ms]</td><td>- </td><td>- </td><td>full GC cycles since start</td></tr><tr><td style="text-align: center;"><strong>17</strong></td><td>- </td><td>- </td><td>request ID</td><td>- </td><td>- </td><td>full GC time since start [ms]</td></tr><tr><td style="text-align: center;"><strong>18</strong></td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>current full GC CPU usage [%]</td></tr><tr><td style="text-align: center;"><strong>19</strong></td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>minor GC time since last update [ms]</td></tr><tr><td style="text-align: center;"><strong>20</strong></td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>full GC time since last update [ms]</td></tr><tr><td style="text-align: center;"><strong>21</strong></td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>minor GC cycles since last update</td></tr><tr><td style="text-align: center;"><strong>22</strong></td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>full GC cycles since last update</td></tr><tr><td style="text-align: center;"><strong>23</strong></td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>current CPU usage (total) [%]</td></tr></table><blockquote class="note"><p>Note that the file format might be changed or extended in future XLT releases.</p></blockquote><h3 id="XLTResultBrowser">XLT Result Browser</h3><p>When running test cases outside Script Developer, that is either in Eclipse as a load test or as an Ant build, you can save the page output to disk. The relevant property is <code>com.xceptance.xlt.output2disk</code>. By default, it is set to <code>never</code>. If you want to enable page output to disk, copy the following lines to <code>dev.properties</code> or <code>test.properties</code>:</p><pre class="plain"><code>## Enables page output to disk. Possible values are:
## - never ..... pages are never logged
## - onError ... pages are logged only if the transaction had errors
## - always .... pages are logged always 
com.xceptance.xlt.output2disk = always   
</code></pre><p>All saved results can be found in the <code>&lt;testsuite&gt;/results</code> directory. See the lines below for details of the results subdirectory structure:</p><pre class="plain"><code>---+ results
   `---+ [testcase]
       `---+ [virtual-user]
           `---+ output
               `---+ [transaction-ID]
                   |---- css
                   |---- images
                   |---+ pages
                   |   `--- cache
                   `---- responses
</code></pre><p>In the folders for each test run (<code>results/[testcase]/[virtual-user]/output/[transaction-ID]</code>), you find an <code>index.html</code> containing the <em>XLT Result Browser</em>. The result browser offers an integrated navigation to browse the complete page output of the transaction and to look at every single request in detail. The file <code>last.html</code> in the output folder <code>results/[testcase]/[virtual-user]/output</code> references the result browser for the last executed transaction of this virtual user.</p><p>The result browser navigation will only permit access to the pages of a transaction if they are directly related to actions. Therefore, defining actions correctly is very important to make the most effective use of the result browser. For details on how to structure test cases and create actions, also see <a href="user-manual.html#BasicConcepts">Basic Concepts</a> and <a href="user-manual.html#Structuring">Code Structuring Recommendations.</a></p><p class="illustration"><a href="images/user-manual/result-browser_1.png"><img alt="XLT Result Browser - Page Output" title="XLT Result Browser - Page Output" border="0" src="images/user-manual/result-browser_1-small.png"/></a> <span class="caption">XLT Result Browser &#8211; Page Output</span> </p><p>If you click on one of the action names in the navigation, the result browser will show the respective page. When you double-click an action name, the navigation will expand to list all related requests. The listed requests are color-coded with black, <span style="color:grey">grey</span>, <span style="color:red">red</span>, <span style="color:blue">blue</span>, <span style="color:#7D28C0">lilac</span> and <span style="color:green">green</span> based on the following algorithm:</p><ol><li>If the request&#8217;s status code is 301 or 302 then set its color to <span style="color:grey">grey</span> since it is a Redirect.</li><li>If the request&#8217;s status code is 0 or greater than equal to 400 then set its color to <span style="color:red">red</span> because it is an Error.</li><li>Set the color initially to black and check the content type of the response if it matches the following criteria:<ol><li>It contains the string <code>javascript</code> or is equal to <code>application/json</code>. If this is the case, change the color to <span style="color:#7D28C0">lilac</span>.</li><li>It starts with the string <code>image</code>. In this case change the color to <span style="color:green">green</span>.</li><li>It is equal to <code>text/css</code>. This content type denotes CSS and thus change the color to <span style="color:blue">blue</span>.</li></ol></li></ol><blockquote class="note"><p>Please note that the content type is determined by the appropriate HTTP response header value. Thus, if an JavaScript file is delivered as content type <code>text/plain</code> then this request will be color-coded with black.</p></blockquote><p>When you select one of the requests from the navigation, the page content will be replaced by detailed information about the request and the related response that you can access via the four tabs on top of the page. The following information is available:</p><ul><li><strong>Request/Response Information</strong><ul><li>General Information</li><li>Request and Response Headers</li><li>URL Query and POST Parameters (if any)</li></ul></li><li><strong>Request Body (Raw)</strong></li><li><strong>Response Content</strong></li></ul><p class="illustration"><a href="images/user-manual/result-browser_2.png"><img alt="XLT Result Browser - Request Details" title="XLT Result Browser - Request Details" border="0" src="images/user-manual/result-browser_2-small.png"/></a> <span class="caption">XLT Result Browser &#8211; Request Details</span> </p><h3 id="CreatingandEvaluatingLoadTestReports">Creating and Evaluating Load Test Reports</h3><p>As the most important tool for analyzing the results of a load test run, XLT offers three types of load test reports, which are thoroughly illustrated in the sections below:</p><ul><li><strong>Load and Performance Test Report</strong></li><li><strong>Performance Comparison Report</strong></li><li><strong>Performance Trend Report</strong></li></ul><p>To create the reports, download all load test results from the agent controllers to the master controller. See <a href="user-manual.html#RunLoadTest">Run The Load Test</a> for details.</p><p>As soon as you&#8217;ve downloaded the load test results to your local disk, you can create the test reports with the XLT report generator. Enter a command in the console following this pattern:</p><pre class="plain"><code>cd &lt;XLT&gt;/bin
./&lt;report-shortname&gt;.(sh/cmd) ../results/&lt;downloaded-results-dir&gt; [options] 

</code></pre><p>The <code>&lt;downloaded-results-dir&gt;</code> and <code>&lt;report-shortname&gt;</code> have to be replaced with the appropriate values. For example:</p><pre class="plain"><code>./create_report.sh ../results/20110503-152920
</code></pre><p>This tells the report generator to take the specified results directory as input for the report. By default, the generated report is saved to <code>&lt;XLT&gt;/reports</code>. The report subdirectory is named after the respective results directory.</p><p>The report generator supports these options:</p><ul><li><code>-o &lt;dir&gt;</code>: an alternative output directory (optional)</li><li><code>-from &lt;time&gt;</code>: ignore results generated before the given time (optional)</li><li><code>-to &lt;time&gt;</code>: ignore results generated after the given time (optional)</li></ul><p>Using the <code>-o</code> option, you can specify an alternative output directory. Keep in mind that you have to specify a target directory name including the final directory for your report. With <code>-o</code>, the directory name is not automatically set but your specified directory will be created instead. For example: </p><pre class="plain"><code>./create_report.sh ../results/20110503-152920 -o D:/Test_Reports/MyLatestReport
</code></pre><p>If you&#8217;re only interested in creating a report for a particular time range, do the following:</p><pre class="plain"><code>./create_report.sh ../results/20110503-152920 -from 20110503-152600 -to 20110503-152800
</code></pre><blockquote class="note"><p>Note that <code>&lt;time&gt;</code> has to be specified in the format <em>yyyyMMdd-HHmmss</em> and that it has to match the time zone of your local machine. By default, the resulting report is rendered using your machine&#8217;s time zone.</p></blockquote><p>All this information is transferred to HTML pages that you can view using a standard web browser. When the report is generated, which may take a while depending on the amount of data gathered during the load test, you will find the file <code>index.html</code> in the root of the appropriate test report directory. Open it in a web browser to view the report.</p><h4 id="LoadandPerformanceTestReport">Load and Performance Test Report</h4><p>The <em>Load and Performance Test Report</em> gives you all the information needed for a detailed analysis of a load test run. It provides several sections, each consisting of at least one table and one or more charts visualizing the graphic development of relevant measurements over time.</p><p class="illustration"><a href="images/user-manual/load_test_report_1.png"><img alt="Load and Performance Test Report" title="Load and Performance Test Report" border="0" src="images/user-manual/load_test_report_1-small.png"/></a> <span class="caption">Load and Performance Test Report</span></p><p class="illustration"><a href="images/user-manual/load_test_report_2.png"><img alt="Load and Performance Test Report - Charts" title="Load and Performance Test Report - Charts" border="0" src="images/user-manual/load_test_report_2-small.png"/></a> <span class="caption">Load and Performance Test Report &#8211; Charts</span>  </p><ul><li><strong>Overview</strong>: This section shows some general information about the load test (e.g. start and end time, duration), the load profile and the test comment (if any was given). It also displays a performance summary and network statistic for HTTP/HTML-based load tests. </li></ul><ul><li><strong>Transactions</strong>: A transaction is a completed test case. The test case consists of one or more actions. The displayed transaction runtime includes the runtime of all actions within the test case, think times, and the processing time of the test code itself. If the test path of the test case is heavily randomized, the runtime of transactions might vary significantly. The average runtime shows the development of tests over time and especially helps to evaluate the outcome of long-running tests. </li></ul><ul><li><strong>Actions</strong>: An action is part of a test case and consists of prevalidation, execution, and postvalidation. The data shown here indicates the time spent in the execution routine of an action. Therefore, its runtime includes the runtime of a request, e.g. an HTTP operation, and the time necessary to prepare, sent, wait, and receive the data. </li></ul><ul><li><strong>Requests</strong>: The request section is the most important statistics section when testing web applications. It directly reflects the loading time of pages or page components. Each row holds the data of one specific request. Its name is defined within the test case as timer name. The Count section of the table shows the total number of executions (Total), the calculated executions per seconds (1/s), minute (1/min), as well as projections or calculations of the executions per hour (1/h) and day (1/d). The Error section displays the total amount (Total) of errors that occurred throughout page or page component loading. The error count doesn&#8217;t include errors detected during the post-validation of the data received. Typical error situations are HTTP response codes such as 404 and 505, timeouts, or connection resets. The runtime section of the table shows the median, the arithmetic mean, the minimum and maximum runtime encountered as well as the standard deviation of all data within that series. The runtime segmentation sections depicts several runtime segments and the number of requests within the segment&#8217;s definition. If the runtime of the test case is shorter than the displayed time period, e.g. test runtime was 30 min and the time period is hour, the numbers will be a linear projection. That means they will show a possible outcome of a longer test run if load and application behavior remained the same. </li></ul><ul><li><strong>Network</strong>: The network section covers the areas of incoming and outgoing traffic during the load test. Sent Bytes is an estimated number based on the data given to the network layer. Cookies, for instance, are not included. Received Bytes is an accurate number because it&#8217;s based on the data received and includes HTTP header information. Depending on the test runtime, the numbers per hour and per day might be estimations based on a linear projection of the available data. If the test run included web activities or other activities returning an HTTP response code, it can be found here as well. Furthermore, all hosts that participated in the test run are listed in a separate table along with the appropriate number of requests that hit this host. Last but not least, this section contains a table that breaks down the received content to their announced type.</li></ul><ul><li><strong>Custom Timers &amp; Values</strong>: The custom timers includes all timers that have been placed individually within the test code. The chart and data description is identical to the request section. In case custom samplers have been run during the test, the collected data is shown in the <em>Custom Values</em> subsection below.</li></ul><ul><li><strong>External Data</strong>: All external data gathered by other tools during the test run is shown here according to the configuration. Please see <a href="user-manual.html#ExternalData">External Data</a> for details on how to include external data in the report.</li></ul><ul><li><strong>Errors &amp; Events</strong>: As its name suggests this section is made up of two parts: Errors and Events (events are used to indicate that the test has encountered a special situation that is not an error but too important to ignore or to write to the log only). The first part &#8211; Errors &#8211; shows a table that contains all errors and their stack traces thrown by the test cases along with an overview of all error types. The second part &#8211; Events &#8211; consists of a single table that lists all events that occurred during the test run including their name, amount, detail message and the name of the test case that produced this event. </li></ul><ul><li><strong>Agents</strong>: This section reports the resource utilization of each user agent in terms of CPU and memory usage. It helps to identify potential resource bottlenecks that might have influenced the load test. Note that all data is local to the JVM of the agent and therefore only covers a process view.</li></ul><ul><li><strong>Configuration</strong>: The configuration section lists the test configuration as well as the load profile used to run the test. It facilitates test reproduction and preserves the test settings for later test evaluation.</li></ul><h5 id="CreateALoadAndPerformanceTestReport">Create A Load And Performance Test Report </h5><p>To generate a load and performance test report, use this command:</p><pre class="plain"><code>create_report.(sh/cmd) ../results/&lt;testDataDir&gt; [options]
</code></pre><p>For example:</p><pre class="plain"><code>./create_report.sh ../results/20110503-160520
</code></pre><p>As an alternative to the command above, you can also create a load and performance test report with the <code>(c)</code> shortcut from the <a href="user-manual.html#InteractiveMode">master controller&#8217;s command line menu</a>. It creates a report of the least recently downloaded results.</p><h5 id="ReportGeneratorConfiguration">Configuring the Report Generator</h5><h6 id="LinkingtoResultBrowserDirectories">Linking to Result Browser Directories</h6><p>If an error occurred during the load test run, the corresponding error message and stack trace will be displayed in the <em>Errors</em> section of the load test report. If you enabled storing the visited pages to disk, you will also find a directory name as part of the error information. To view the visited pages, use this directory name to locate the corresponding result browser in the results directory of the load test.</p><p>You can also access the result browsers directly from the load test report. This greatly speeds up error analysis because you would just have to click the directory name next to an error entry to open the respective result browser. To make this work, you first need to ensure that:</p><ol><li>the results will be provided at the target location, and</li><li>the results directory will never be renamed oder moved.</li></ol><p>Otherwise, viewers of the report experience broken links.</p><p>To let the report generator create links from the load report to the result browsers, set the property <code>com.xceptance.xlt.reportgenerator.linkToResultBrowsers</code> in <code>&lt;XLT&gt;/config/reportgenerator.properties</code> to <code>true</code>.</p><p>By default, the report generator calculates the path from the report to the result browsers based on the results directory (given on the report generator&#8217;s command line) and the reports directory (either being the default directory or the one explicitly given as command line argument). The computed path will be a relative path if possible and an absolute path otherwise (on Windows, if report and results are on different drives).</p><p>Sometimes the relative path approach is not suitable, for example, if you only send the report to your team members, not the results. In that case, the results must be made available somewhere on the net. Furthermore, the report generator needs to know about this location to appropriately generate the links. To this end, you configure a results base URI, for instance <code>http://myhost/results</code>. The URI is a <em>base</em> URI as it&#8217;s common for the results of all your load tests. The report generator automatically appends the name of the results directory (for example <code>20121106-111751</code>) to this URI when generating the links to the result browsers, so the resulting link might look like this: <code>http://myhost/results/20121106-111751/ac01_00/TSearch/126/output/1352194484275/index.html</code> </p><p>Using a base URI, you don&#8217;t need to reconfigure the report generator when generating the report for another load test, unless you choose to publish the results at a totally different location. To configure the base URI, set the property <code>com.xceptance.xlt.reportgenerator.resultsBaseUri</code> in <code>&lt;XLT&gt;/config/reportgenerator.properties</code> to the appropriate value.</p><h6 id="repGenScaleMode">Charts Scaling and Boundaries</h6><p>Sometimes the runtime charts have extremely high peaks. By default, the charts are scaled such that the y-axis goes up to slightly more than the maximum value. However, in case of such peaks, the area of interest the big majority of the values falls into may be extremely compressed into the chart because the scale has a really large maximum value. To get more significant charts, the properties </p><pre class="plain"><code>com.xceptance.xlt.reportgenerator.charts.scaleMode
</code></pre><p>and</p><pre class="plain"><code>com.xceptance.xlt.reportgenerator.charts.capAtMultipleOfAverage
</code></pre><p>have been introduced.</p><p>Use the first property to set the mode for the y-axis scale of the runtime charts. By default, a linear scale is used. If you set this property, a logarithmic mode will be used alternatively. The allowed values for this property are &#8220;linear&#8221; and &#8220;logarithmic&#8221;, any other value results in an exception. When you use this approach, the graphs in the charts are flattened and extreme peaks are compressed in their representation.</p><p><strong>Example</strong>: To have runtime charts with a logarithmic scale on the y-axis, use</p><pre class="plain"><code>com.xceptance.xlt.reportgenerator.charts.scaleMode = logarithmic
</code></pre><p>Use the second property to define the upper bound of the y-axis in the charts. As the values in different charts may extremely differ, the given value is interpreted as multiple of the average. Any double value is allowed. By default, there is no capping at all.</p><p><strong>Example</strong>: The average runtime for a request is 250 milliseconds, but a few requests ran into a timeout with 30 seconds. Then setting a value of 3 for this property results in the y-axis being capped at 750 milliseconds (3 times the average value).</p><pre class="plain"><code>com.xceptance.xlt.reportgenerator.charts.capAtMultipleOfAverage = 3
</code></pre><p><strong>Hint</strong>: To indicate fractional numbers, make sure to not use commas but points, for example 3.5 to denote 3 and a half. </p><p>To avoid useless capping, all current values are checked. There&#8217;s no capping if all values in the measured data are smaller than the average value multiplied with the configured factor. If capping has been applied, a magenta line will be drawn and an entry will be made in the legend of the chart.</p><p><strong>Hint</strong>: This property only takes integers as arguments.</p><h4 id="PerformanceComparisonReport">Performance Comparison Report</h4><p>The <em>Performance Comparison Report</em> gives you a quick overview on performance improvements (green color tones) and performance declines (red color tones) between two test runs. The initial test run is labeled <strong>baseline</strong>. The test run being compared to the baseline is labeled <strong>measurement run</strong>.</p><p>Every section of the comparison report displays a table with performance changes and is divided into three parts:</p><ul><li><strong>Count</strong>: The percentage values show the development of the performance in comparison to the baseline. Positive numbers in the count section indicate an improvement of the throughput over the baseline. Negative values indicate a decrease in throughput.</li></ul><ul><li><strong>Errors</strong>: Positive numbers indicate an increase in the number of errors, negative numbers a decrease. An infinite sign indicates the occurrence of errors in comparison to an error-free baseline.</li></ul><ul><li><strong>Runtime</strong>: Positive values indicate a poorer performance, negative values an improvement (smaller runtime values) over the baseline.</li></ul><p>When you hover the mouse over the columns of the report table, you can see the actual measurement results, which lets you determine whether or not the reported percentage change is significant.</p><p class="illustration"><a href="images/user-manual/comparison_report_1.png"><img alt="Performance Comparison Report - Overview" title="Performance Comparison Report - Overview" border="0" src="images/user-manual/comparison_report_1-small.png"/></a> <span class="caption">Performance Comparison Report &#8211; Overview</span></p><p class="illustration"><a href="images/user-manual/comparison_report_2.png"><img alt="Performance Comparison Report" title="Performance Comparison Report" border="0" src="images/user-manual/comparison_report_2-small.png"/></a> <span class="caption">Performance Comparison Report</span></p><p>The <em>Performance Comparison Report</em> contains the following sections:</p><ul><li><strong>Overview</strong>: The overview section shows general information about both load tests. It lets you compare settings, runtime, and profiles. In the later sections, the percentage values depict the development of the performance in comparison to the baseline. Note that the total columns (total throughput and total errors) might present misleading values if the load tests used different runtime configurations. All other values are normalized with respect to the runtime and therefore easily comparable. Positive numbers in the count section stand for an improvement of the throughput over the baseline, negative values for a decrease in throughput. An increase in the number of errors is indicated with positive numbers, a decrease with negative numbers. An infinite sign indicates the occurrence of errors in comparison to an error-free baseline. For all runtime numbers, positive values signify a poorer performance, negative values an improvement, or smaller runtime values, over the baseline. Added or removed transactions, actions, or requests are displayed, but for them no comparison is provided. </li></ul><ul><li><strong>Transactions</strong><ul><li>Count</li><li>Errors</li><li>Runtime</li></ul></li></ul><ul><li><strong>Actions</strong><ul><li>Count</li><li>Errors</li><li>Runtime </li></ul></li></ul><ul><li><strong>Requests</strong><ul><li>Count</li><li>Errors</li><li>Runtime</li></ul></li></ul><ul><li><strong>Custom Timers</strong><ul><li>Count</li><li>Errors</li><li>Runtime </li></ul></li></ul><h5 id="CreateaPerformanceComparisonReport">Create a Performance Comparison Report</h5><p>A performance comparison report can only be generated between two existing load and performance test reports. That is, you first have to create both of these reports.  </p><p>Then you can generate a performance comparison report using the following command:</p><pre class="plain"><code>create_diff_report.(sh/cmd) &lt;testReportDir_1&gt; &lt;testReportDir_2&gt; [options]
</code></pre><p>For example:</p><pre class="plain"><code>./create_diff_report.sh ../reports/20110503-152920 ../reports/20110503-160520
</code></pre><h4 id="PerformanceTrendReport">Performance Trend Report</h4><p>A trend report depicts the development of the performance over time. Multiple measurements are taken into account and evaluated against each other. It shows how your system performs over time, how your tuning effort pays out, and how your live environment acts under a changing load situation if used as monitoring. </p><p>Two trend report types are available: </p><ul><li>Difference to First Run and </li><li>Difference to Previous Run. </li></ul><p>The Difference to the First Run reports the changes compared to your first test run, mostly referred to as baseline. Each table column displays the difference between your baseline run and the run you&#8217;re interested in. The quality of your baseline run defines how valuable this report may be. You can also look at it as a long-term performance trend report. </p><p>The Difference to Previous Run visualizes the improvements between two adjacent test runs, which lets you recognize how your last change or tuning effort payed out in comparison to the previous run. It helps you to see whether or not you are on the right track regarding the improvement of your application&#8217;s performance. It also emphasizes sudden improvements or set-backs and can be seen as a short-term performance trend report. </p><p>When you hover the mouse over the columns of the trend report table, you can see the actual measurement results. This will will give you a better idea whether or not the reported percentage change is significant. Please keep in mind that changes up to 10% are measurement fluctuation most of the time. </p><p class="illustration"><a href="images/user-manual/performance_trend_1.png"><img alt="Performance Trend Report - Overview" title="Performance Trend Report - Overview" border="0" src="images/user-manual/performance_trend_1-small.png"/></a> <span class="caption">Performance Trend Report &#8211; Overview</span></p><p class="illustration"><a href="images/user-manual/performance_trend_2.png"><img alt="Performance Trend Report" title="Performance Trend Report" border="0" src="images/user-manual/performance_trend_2-small.png"/></a> <span class="caption">Performance Trend Report</span></p><p>Similar to the other reports, the trend report is divided into the following sections, each containing the tables and charts mentioned above:</p><ul><li><strong>Overview</strong> </li><li><strong>Transactions</strong></li><li><strong>Actions</strong></li><li><strong>Requests</strong></li><li><strong>Custom Timers</strong>  </li></ul><h5 id="CreateaPerformanceTrendReport">Create a Performance Trend Report</h5><p>To generate a performance trend report on several test reports, use the command below:</p><pre class="plain"><code>create_trend_report &lt;testReportDir_1&gt; ... &lt;testReportDir_n&gt; [options]
</code></pre><p>For example:</p><pre class="plain"><code>./create_trend_report.sh ../reports/20110503-152920 ../reports/20110503-160520 ../reports/20110503-161030
</code></pre><h3 id="CustomValues">Custom Values</h3><p>During a load test, XLT is logging a large amount of data relevant to the test run. Nevertheless, sometimes it comes in handy to log additional information about the system under test (SUT) directly during the load test run. For this purpose, XLT provides custom values.</p><p><strong>Example</strong>: An eCommerce application is typically connected to several third-party systems to use external services like credit-worthiness check. The response time of these third-party systems can have a major impact on the SUT&#8217;s response to the client request. By default, this application-internal information isn&#8217;t visible to XLT during a load test. A typical example for custom values in this context is logging the response time of requests to third-party systems. To do so, you have to write custom code to access the relevant sources, for instance via remote connection to the application server. The additional data can then be logged by XLT during the load test runtime and is automatically integrated into the load and performance test report.</p><h4 id="Sampler">Sampler</h4><p>Custom Samplers let you query custom sources and log data (<em>samples</em>) during the load test runtime. To this end, provide a custom sampler class extending <code>com.xceptance.xlt.api.engine.AbstractCustomSampler</code>. The sampler gets configured in the test suite configuration files. The recommended location for the relevant configuration is <em>project.properties</em>.</p><p>The provided sampler must override the <code>execute()</code> method that is called after each interval time (see configuration). Furthermore, the sampler might override the methods <code>initialize()</code> or <code>shutdown()</code> getting called just once for the sampler. While <code>initialize()</code> is called before the first call of <code>execute()</code>, <code>shutdown()</code> is called on shutdown.</p><p>The logged custom value is the return value of the <code>execute()</code> method.</p><p>The <strong>AbstractCustomSampler</strong> can store any &#8216;double&#8217; value. The stored value indicates the absolute value at a certain point in time. The corresponding report chart directly shows the logged value.</p><h4 id="Configuration">Configuration</h4><p>To configure samplers, provide these properties:</p><pre class="plain"><code>com.xceptance.xlt.customSamplers.1.class = com.xceptance.xlt.samples.ValueSamplerDemo
com.xceptance.xlt.customSamplers.1.name = ValueSamplerDemo
com.xceptance.xlt.customSamplers.1.description = This sampler logs a custom value which is just a random number
com.xceptance.xlt.customSamplers.1.interval = 1000
com.xceptance.xlt.customSamplers.1.chart.title = ValueSamplerDemo
com.xceptance.xlt.customSamplers.1.chart.yAxisTitle = Value
#com.xceptance.xlt.customSamplers.1.property.foo = 123
#com.xceptance.xlt.customSamplers.1.property.bar = abc
...
com.xceptance.xlt.customSamplers.9.class = ...
com.xceptance.xlt.customSamplers.9.name = ...
...
</code></pre><ul><li><em>com.xceptance.xlt.customSamplers.n.</em> is the saved key for custom sampler properties. Each sampler configuration block must have a unique number (called <em>n</em> in this example). The numbers don&#8217;t need to be in strictly successive order.</li><li><em>class</em> points to the sampler class (including full package path).</li><li><em>name</em> is a customizable name of the sampler. This name <strong>must</strong> be used when instantiating a sample class (it&#8217;s recommended to use the method <code>getSamplerName()</code>). </li><li><em>interval</em> defines the period the sampler is started at (unit of time is milliseconds). The value must be positive (including 0). A new sampler will be started only if it is executed for the first time or if the previous sampler has come to an end.</li><li>Providing a chart title is optional. By default, the sampler name is used. <em>yAxisTitle</em> defines the title of the y-axis for the rendered chart.</li><li>Providing further sampler properties is optional. The properties can be accessed by calling the methods <code>getProperties()</code> or <code>getProperty(key)</code> (where <em>key</em> is the string in the configuration between <em>com.xceptance.xlt.customSamplers.n.property.</em> and the equals sign (<em>=</em>). In the present example, the keys are <em>foo</em> and <em>bar</em> with the value <em>123</em> and <em>abc</em>). Sampler property keys <strong>must not</strong> contain dots or whitespace. Apart from that they are free in name and count.</li></ul><h4 id="Example4">Example</h4><p>The code below shows an example of a very simple custom sampler logging random values:</p><pre class="java"><code>public class ValueSamplerDemo extends AbstractCustomSampler
{
    public ValueSamplerDemo()
    {
        super();
    }

    @Override
    public void initialize()
    {
        // initialize
    }

    @Override
    public double execute()
    {
        // generate random value based on the configured limits

        // get properties
        final String lowerLimitProp = getProperties().getProperty("generatedValueLowerLimit");
        final String upperLimitProp = getProperties().getProperty("generatedValueUpperLimit");

        // convert to integer
        try
        {
            final int lowerLimit = Integer.valueOf(lowerLimitProp);
            final int upperLimit = Integer.valueOf(upperLimitProp);

            // return the value to be logged
            return XltRandom.nextInt(lowerLimit, upperLimit) + XltRandom.nextDouble();
        }
        catch (final NumberFormatException e)
        {
            // log 0 in case of an exception
            return 0;
        }
    }

    @Override
    public void shutdown()
    {
        // clean up
    }
}

</code></pre><p>The resulting chart is automatically integrated into the XLT performance and load test report and can be accessed via the report navigation menu item <em>Custom Values</em>.</p><p class="illustration"><a href="images/user-manual/CustomSamplerReport.png"><img alt="Custom sampler report" title="Custom sampler report" border="0" src="images/user-manual/CustomSamplerReport-small.png"/></a> <span class="caption">Custom Sampler Report</span></p><h3 id="ExternalData">External Data Report</h3><p>As an alternative to <a href="user-manual.html#CustomValues">custom values</a>, the XLT report generator lets you include external data files in the report. This can be used if it&#8217;s impossible to access the external data source directly during the load test runtime. </p><h4 id="Parser">Parser</h4><p>To read the external files and integrate them into the load test report, a <em>Parser</em> class is needed for each type of file or format. XLT provides a set of predefined parsers for CSV files. If you plan to integrate some other file formats or reports, you have to write your own parser class extending <code>AbstractLineParser</code> in <code>com.xceptance.xlt.api.report.external</code>.</p><ul><li><strong>SimpleCsvParser</strong>: Extracts the data by splitting each line into the comma-separated values.</li><li><strong>HeadedCsvParser</strong>: Like <em>SimpleCsvParser</em>, but additionally names the values by the column value in the first line.</li></ul><p>XLT ships with a demo test suite <em>demo-external-data</em> to demonstrate the use of external data in load test reports. As an example of advanced parser classes, two parsers for handling logs of the command line tool <em>iostat</em> can be found in the source directory of the test suite project <code>&lt;XLT&gt;\samples\demo-external-data\src</code> in package <code>com.xceptance.xlt.report.external</code>.  </p><ul><li><strong>IostatCpuParser</strong>: parses the CPU section of a log of command line tool <em>iostat</em> (with parameter <em>-t</em> to log time stamps).</li><li><strong>IostatDeviceParser</strong>: parses the Device section of a log of command line tool <em>iostat</em> (with parameter <em>-t</em> to log time stamps).</li></ul><h4 id="Configuration2">Configuration</h4><h5 id="Location">Location</h5><p>The configuration file <em>externaldataconfig.xml</em> is expected in the result directory of the respective load test run. If you can&#8217;t locate it there, it will get looked up in the master controller&#8217;s <em>config</em> directory. See the provided sample file <code>&lt;XLT\config\externaldataconfig.xml.sample</code> and adapt it to your needs after removing the .sample extension. At least adapt the source file and columns/indexes. The configuration file modified for the <em>demo-external-data</em> project is located at <code>&lt;XLT&gt;\samples\demo-external-data\results\20110621-101041</code>.</p><h5 id="Structure">Structure</h5><p>The configuration defines files to parse by a specified parser and the column indexes to mark relevant data and other settings like headlines, descriptions, or colors to enrich the chart/table information for the report.</p><ul><li><strong><code>file</code></strong>: Define which file gets processed by which parser. If you don&#8217;t set a file encoding, the default encoding <em>UTF-8</em> will be used.</li><li><strong><code>charts</code></strong>: You can configure one or more charts. The minimum chart configuration requires the <em>chart title</em> and the <em>column indexes</em> of the series. See the following list of available attributes and their default values:<ul><li>Chart defaults:<ul><li><em>title</em> [mandatory and unique] (chart title)</li><li><em>yAxisTitle/yAxisTitle = "Values"</em> (title of the 1st/2nd y-axis (value axis))</li><li><em>xAxisTitle = "Time"</em> (title of the x-axis (time axis))</li><li>Series defaults:<ul><li><em>valueName</em> [mandatory] (e.g. column index for <em>SimpleCvsParser</em> or column headline for <em>HeadedCsvParser</em>)</li><li><em>title = [_valueName</em>]_ (series title)</li><li><em>axis = "1"</em> (number of axes (1 or 2))</li><li><em>color/averageColor = [a color of internal default color set]</em> (hex based RGB color e.g. &#8220;#1a2b3f&#8221;)</li><li>average = [no average] (percentage of data used to build an average value)</li></ul></li></ul></li></ul></li><li><strong><code>tables</code></strong>: You can configure one or more tables. The minimum table configuration requires the <em>table title</em> and the <em>column indexes</em> of the rows. See the following list of available attributes and their default values:<ul><li>Table defaults:<ul><li><em>title</em> [mandatory] (table title)</li><li><em>type = "minmaxavg"</em> (currently, there is only this type supported printing the <em>minimum</em>, <em>maximum</em> and <em>average</em> of read column values)</li><li>Row defaults:<ul><li><em>valueName</em> [mandatory] (e.g. column index for <em>SimpleCvsParser</em> or column headline for <em>HeadedCsvParser</em>)</li><li><em>title = [_valueName</em>]_ (table title)</li><li><em>description = [no description]</em></li><li><em>unit = [no unit]</em> (unit of measurement for parsed data)</li></ul></li></ul></li></ul></li><li><strong><code>properties</code></strong>: You may provide properties to the report generation. In case you extend a parser or write your own, this is a good way to provide some free properties.</li></ul><p>Currently, two properties are supported by the framework to convert a human readable date/time to a time stamp (<em>parser.dateFormat.pattern</em> and <em>parser.dateFormat.timeZone</em>). If the time stamp is already a UNIX time stamp, the pattern isn&#8217;t required. It&#8217;s strictly recommended to adapt these properties to your needs. See <em>java.text.SimpleDateFormat</em> for more information on date/time patterns.</p><p>Another supported property is <em>parser.csv.separator</em> that describes the expected field separator character in CSV files. By default, it&#8217;s a comma (,).</p><blockquote class="note"><p><br/>For <em>Tab</em> separated CSV files, use <code>&amp;#x9;</code> as the value of <em>parser.csv.separator</em>.</p></blockquote><h2 id="CommandsLong">Command Reference</h2><p>This chapter thoroughly illustrates the commands available in XLT when you either use the Scripting API or Script Developer. As both these components almost support the exact same commands with the same syntax, they are subsumed under this chapter. It provides useful information on the general structure of commands as well as tables and examples for each command. If you plan on using the XLT Scripting API, see the <a href="user-manual.html#AdditionalHints">additional hints</a> at the end of this chapter. For information on the other XLT components, see the chapters named after them. </p><h3 id="CommandStructure">Command Structure</h3><p>Commands have a name and may have a target parameter and/or a value parameter, depending on the command in question. Most commands somehow deal with parts of an HTML page, so the target of the command is taken to identify the element on the page. A command may contain a delay, for example the number of milliseconds to wait to continue the test after clicking on an element. A command may also deal with some text that should be matched and that is given as argument.</p><p>The exact parameters are given for each command in the command&#8217;s description. In general, a command takes on the following forms:</p><ol><li><em>CommandName()</em>, </li><li><em>CommandName(Target)</em>, </li><li><em>CommandName(Value)</em>, or</li><li><em>CommandName(Target, Value)</em></li></ol><p>The target is used to identify a part of an HTML page and the value is some string or numerical value.</p><p>The parameters define:</p><ol><li>which element of an HTML page should be identified and how it should be identified (see <a href="user-manual.html#Locators">Locators</a> ),</li><li>which text should be matched and how it should be matched (see <a href="user-manual.html#TextMatching">Text Matching</a> ), </li><li>other parameters, such as a delay.</li></ol><p>Note that even with the three parameter types listed above, a command has at most two parameters. </p><p>A command may interpret its parameters differently and deal with them in a very specific way. For example, the command &#8220;storeEval(target, value)&#8221; interprets its target as JavaScript expression.</p><h3 id="Locators">Locators</h3><p>Almost all commands operate on a certain item. Most of the time, this item is an element on a page, but it may also be a window or a frame. To identify, or <em>locate</em>, the item in question, you can specify locator expressions defining which item should be located and by which means.</p><p>The general syntax of a locator expression is: <code>&lt;strategy&gt;=&lt;value&gt;</code></p><p>The strategy defines <em>how</em> to search for an item, the <em>value</em> defines which item is to be addressed. For instance, to locate an element with ID attribute &#8220;foo&#8221;, you would specify this locator: <code>id=foo</code></p><p>Note that the strategy (including the following equal sign) can be omitted. In that case, a built-in default strategy is used. However, it is recommended not to omit the strategy. It may save you some characters to type, but at the same time you would always need to remember the default strategy for a certain locator type.</p><p>If the evaluation of the locator results in no elements being identified, the command will fail. If, on the other hand, the locator identifies more than one element, the first element found will be used.</p><p>Currently, there are four types of locators:</p><ol><li><strong>element locators</strong>: locate elements on an HTML page</li><li><strong>attribute locators</strong>: locate element attributes</li><li><strong>option locators</strong>: locate <code>&lt;option&gt;</code> elements inside a <code>&lt;select&gt;</code> element on an HTML page</li><li><strong>window locators</strong>: locate a browser window </li><li><strong>frame locators</strong>: locate a frame window on an HTML page</li></ol><p>See the sections below for their detailed explanation.</p><h4 id="ElementLocators">Element Locators</h4><p>Elements can be identified by one of the following strategies:</p><table><tr><td style="vertical-align: top;"><strong>css</strong> </td><td>Identifies the first element matching the passed CSS selector.<br/>The syntax is: <code>css=&lt;CSS selector&gt;</code> </td></tr><tr><td style="vertical-align: top;"><strong>dom</strong> </td><td>Identifies the first element matching the passed DOM expression.<br/>The syntax is: <code>dom=&lt;DOM expression&gt;</code> </td></tr><tr><td id="idLocator" style="vertical-align: top;"><strong>id</strong></td><td>Identifies the first element matching the passed ID.<br/>The syntax is: <code>id=&lt;element id&gt;</code> </td></tr><tr><td style="vertical-align: top;"><strong>identifier</strong> </td><td>Identifies an element by its ID, and then by name.<br/>The syntax is: <code>identifier=&lt;element id/name&gt;</code> </td></tr><tr><td style="vertical-align: top;"><strong>link</strong> </td><td>Identifies the first link matching the passed link text.<br/>The syntax is: <code>link=&lt;link text&gt;</code><br/><strong>Note that the text of a hidden link is empty!</strong> </td></tr><tr><td style="vertical-align: top;"><strong>name</strong> </td><td>Identifies the first element matching the passed name.<br/>The syntax is: <code>name=&lt;element name&gt;</code> </td></tr><tr><td style="vertical-align: top;"><strong>xpath</strong> </td><td>Identifies the first element matching the passed XPath expression.<br/>The syntax is: <code>xpath=&lt;XPath expression&gt;</code> </td></tr><tr><td style="vertical-align: top;"><em>(default)</em> </td><td>When no locator strategy is specified, it will behave the same as<ul><li>dom &#8211; if the expression starts with "document."</li><li>xpath &#8211; if the expression starts with "//"</li><li>identifier &#8211; otherwise</li></ul> </td></tr></table><p>Note that not all strategies are applicable to all elements. For instance, the <em>link</em> strategy makes only sense when locating <code>&lt;a&gt;</code> elements.</p><h4 id="AttributeLocators">Attribute Locators</h4><p>When accessing an element&#8217;s attribute, you need to specify two things: where the element itself can be found and the name of the attribute. When you know both of them, you just need to put them together with an @-character.</p><p>Thus, all attribute locators have the form: <em>&lt;Element Locator&gt;@AttributeName</em>, where <em>&lt;Element Locator&gt;</em> is an arbitrary element locator as defined above.</p><h4 id="OptionLocators">Option Locators</h4><p>When dealing with select boxes, you will need two locators: one that identifies the select element, and one that identifies the option element(s) in question. The first locator is a normal element locator. For an option locator, you can use the following strategies:</p><table><tr><td style="vertical-align: top;"><strong>id</strong> </td><td>Same as for the element locator strategy <a href="#idLocator">id</a>. </td></tr><tr><td style="vertical-align: top;"><strong>index</strong> </td><td>Identifies the option with the passed index (counting starts with 0).<br/>The syntax is: <code>index=&lt;element index&gt;</code> </td></tr><tr><td style="vertical-align: top;"><strong>label</strong> </td><td>Identifies the option with the passed text label.<br/>The syntax is: <code>label=&lt;element label&gt;</code><br/><strong>Note that the label of a hidden option is empty!</strong> </td></tr><tr><td style="vertical-align: top;"><strong>value</strong> </td><td>Identifies the option with the passed value.<br/>The syntax is: <code>value=&lt;element value&gt;</code> </td></tr><tr><td style="vertical-align: top;"><em>(default)</em> </td><td>Same as <em>label</em> strategy.<br/>The syntax is: <code>&lt;element label&gt;</code> </td></tr></table><p>When you use any of the commands <em>addSelection</em> or <em>removeSelection</em>, be aware that <em>all</em> options identified by the given option locator will be processed, not just the first one. </p><h4 id="WindowLocators">Window Locators</h4><p>Windows can be identified by one of the following strategies:</p><table><tr><td style="vertical-align: top;"><strong>name</strong> </td><td>Identifies the window with the passed name.<br/>The syntax is: <code>name=&lt;window name&gt;</code> </td></tr><tr><td style="vertical-align: top;"><strong>title</strong> </td><td>Identifies the window with the passed title text.<br/>The syntax is: <code>title=&lt;window title&gt;</code> </td></tr><tr><td style="vertical-align: top;"><em>(default)</em> </td><td>Tries to find a window by name first, then by title.<br/>The syntax is: <code>&lt;window name/title&gt;</code> </td></tr></table><h4 id="FrameLocators">Frame Locators</h4><p>Frames can be identified in two ways:</p><ul><li>locating the frame via special document properties, or  </li><li>locating the corresponding <code>&lt;frame&gt;</code> element on the page.</li></ul><p>The latter can be achieved by a regular element locator; the other strategies work as follows:</p><table><tr><td style="vertical-align: top;"><strong>index</strong> </td><td>Identifies the frame having the passed index (counting starts with 0) relative to the current page.<br/>The syntax is: <code>index=&lt;frame index&gt;</code> </td></tr><tr><td style="vertical-align: top;"><strong>relative=parent</strong> </td><td>Identifies the parent window of the current frame. </td></tr><tr><td style="vertical-align: top;"><strong>relative=top</strong> </td><td>Identifies the top level window of the current frame. </td></tr><tr><td colspan="2"><em>See <a href="user-manual.html#ElementLocators">Element Locators</a> for additional strategies.</em> </td></tr></table><h3 id="TextMatching">Text Matching</h3><p>Many commands check for the presence of some text on the page or wait for some text to appear. Since the text displayed on the page may not always be static but contain dynamic parts (such as the user name in &#8220;Welcome, Mr. John Smith!&#8221;), the provided validation text needs to be somewhat blurry as well (for example, &#8220;Welcome, Mr. *!&#8221;). This is where text-matching expressions come into play.</p><p>The general syntax of a text-matching expression is: <code>&lt;strategy&gt;:&lt;text pattern&gt;</code></p><p>The strategy defines which pattern syntax is used in the text pattern following the colon. For instance, if you want to use the famous &#8220;?&#8221; and &#8220;*&#8221; wildcard characters in the text pattern, you need to specify an expression such as this one: <code>glob:Welcome, Mr. *!</code></p><p>Note that the strategy (including the subsequent colon) can be omitted. In that case, a built-in default strategy is used.</p><p>Currently, there are three text-matching strategies:</p><table><tr><td style="vertical-align: top;"><strong>glob</strong></td><td>The asterisk and question mark are meta-characters, everything else is treated as normal character. A question mark indicates that there has to be exactly one character and that it may be an arbitrary one, an asterisk indicates an arbitrary number (including zero) of arbitrary characters. For example, use &#8220;?all*&#8221; to match &#8220;ball&#8221;, &#8220;wall&#8221;, &#8220;+all&#8221; regardless of what follows, and not to match &#8220;small&#8221; (which required &#8220;??all&#8221; or &#8220;??all*&#8221;). Meta-characters can be used at arbitrary positions inside the text. For instance, use <em>link=result*.css</em> to find the first link starting with &#8220;result&#8221; and ending with &#8220;.css&#8221;, regardless of what there is between &#8220;result&#8221; and &#8220;.css&#8221;. </td></tr><tr><td style="vertical-align: top;"><strong>regexp</strong> </td><td>Indicates that the given text is a regular expression that has to be matched by the inspected text. For example, use <em>link=regexp:\d+</em> to find the first link with the link&#8217;s text consisting of an arbitrary number of digits. </td></tr><tr><td style="vertical-align: top;"><strong>regexpi</strong> </td><td>Indicates that the given text is a regular expression that has to be matched by the inspected text. However, in contrast to regexp, the case of the inspected text is ignored when checking for a match. </td></tr><tr><td style="vertical-align: top;"><strong>exact</strong> </td><td>Indicates that the given text has to exactly match. For instance, use <em>link=exact:?all</em> to find the link with the text &#8220;?all&#8221;. </td></tr><tr><td style="vertical-align: top;"><em>(default)</em> </td><td>Same as <strong>glob</strong>. </td></tr></table><blockquote class="note"><p>If your text pattern text contains a colon, you have to specify an appropriate text-matching strategy since the colon character is used as separator to parse the text-matching strategy and text pattern from the input string.</p></blockquote><p>Be aware that prior to comparing the actual text with the expected text, each sequence of whitespace (spaces, non-breaking spaces, line breaks, or tabs) is replaced by a single space in both the actual text and the expected text. This way, comparisons are independent of the actual amount of whitespace.</p><p>Further note that depending on the command in question, the same expression is treated slightly different. Some commands check whether the text (of the located element / page) matches the expression, while others check that the text <em>contains</em> a subsequence (may be the text itself) matching the expression. For instance, with <em>exact=user</em> the text <em>No users found!</em> may or may not match depending on the command.</p><h3 id="Commands">Commands</h3><p>Commands can be categorized. Some of them interact with the elements of an HTML page (for example, clicking on a button), some verify things (for example, that the page&#8217;s title matches the expected one), some wait for a condition to be fulfilled (for example, waiting for a specific element to become visible), and some store data in variables so that it can be reused later on. Finally, there is a left-over category for miscellaneous commands.</p><p>Commands in different categories may nevertheless be somehow related. Assume, for instance, you want to do something with the text shown on an HTML page. Then you can use <em>assertText</em> to check whether the text occurs on the current page, but you can also use <em>waitForText</em> to wait for the text to show up on the page (it may be inserted by an AJAX call). Furthermore, <em>storeText</em> lets you store the text of the targeted element in a variable for later access. </p><p>The following section explains the available commands and their respective categories. Within each category, they are listed in alphabetical order; examples are provided for better comprehension. </p><blockquote class="note"><p>Note that each of the <em>assertXXX</em> and <em>waitForXXX</em> commands has the negated counterparts <em>assertNotXXX</em>, and <em>waitForNotXXX</em>, respectively. In these cases, references to previously explained commands are used to avoid lengthy repetitions.</p></blockquote><h4 id="InteractionCommands">Commands That Interact With A Page</h4><p>Commands that change the page or trigger events (for example, by clicking on a link). </p><h5 id="addSelection">addSelection</h5><p>Selects options in a multi-selection element. It selects all options that match the command value, so it may select several! Will abort the test with an error if no option matching the command value is found, the element is no multi-selection element, the multi-select element is disabled, or if it doesn&#8217;t support multi-selection. Accepts any element locator except the link locator for the multi-select item and any option locator for the value.</p><p>Example: <em>adddSelection(id=RadioChannel,index=1)</em></p><h5 id="check">check</h5><p>Checks the targeted element. The element must be an input element, of type <em>radio</em> or <em>checkbox</em> , and enabled. Will abort the test with an error otherwise. Accepts any element locator.</p><p>Example: <em>check(id=beautifulWrapping)</em></p><h5 id="checkAndWait">checkAndWait</h5><p>Same as <strong>check</strong> but waits until page load triggered by checking the targeted input element is completed or until the configured timeout is reached. Will abort the test with an error if no page load is triggered and/or the timeout is reached.</p><p>Example: <em>checkAndWait(id=beautifulWrapping)</em></p><h5 id="click">click</h5><p>Clicks on the targeted element. Be aware that the click command doesn&#8217;t wait for a page to be loaded, so don&#8217;t use it to click on a link. In that case, use clickAndWait (otherwise, you&#8217;re likely to end up with only a few parts of the new parts being loaded). Will abort the test with an error if there is no element matching the given locator. Accepts any element locator.</p><p>Example: <em>click(id=productsLink)</em></p><h5 id="clickAndWait">clickAndWait</h5><p>Same as <strong>click</strong> but waits until page load triggered by clicking the targeted element is completed or the configured timeout is reached. Will abort the test with an error if no page load is triggered and/or the timeout is reached.</p><p>Example: <em>clickAndWait(id=productsLink)</em></p><h5 id="doubleClick">doubleClick</h5><p>Double-clicks the targeted element. Will abort the test with an error if there is no element matching the given locator. Accepts any element locator.</p><p>Example: <em>doubleClick(id=confirm)</em></p><h5 id="doubleClickAndWait">doubleClickAndWait</h5><p>Same as <strong>doubleClick</strong> but waits after the double-click for the page load triggered by this action to be completed. Will abort the test with an error if no page load is triggered and/or the configured timeout is reached.</p><p>Example: <em>doubleClickAndWait(id=confirm)</em></p><h5 id="mouseDown">mouseDown</h5><p>Clicks on the targeted element with the left mouse button and doesn&#8217;t release it. Will abort the test with an error if there is no element matching the given locator or the element is not visible. Accepts any element locator.</p><p>Example: <em>mouseDown(id=confirmationButton)</em></p><h5 id="mouseDownAt">mouseDownAt</h5><p>Clicks with the left mouse button on the targeted element at the given offset position and doesn&#8217;t release it. Will abort the test with an error if there is no element matching the given locator, the element is not visible or the given offset position is invalid. <br/>Accepts any element locator. The offset position has to be specified as <em>x,y</em> where <em>x</em> and <em>y</em> denote the offsets (number of pixels) to add to the <em>x</em>- and <em>y</em>-coordinate of the target element position. </p><p>Example: <em>mouseDownAt(id=confirmationButton, 0,0)</em></p><h5 id="mouseMove">mouseMove</h5><p>Moves the mouse to the targeted element. Will abort the test with an error message if there is no element matching the given locator or the element is not visible. Accepts any element locator.</p><p>Example: <em>mouseMove(id=confirmationButton)</em></p><blockquote class="note"><p>Note that the mouse is not actually moved but the proper JavaScript event is triggered.</p></blockquote><h5 id="mouseMoveAt">mouseMoveAt</h5><p>Moves the mouse to the targeted element at the given offset position. Will abort the test with an error message if there is no element matching the given locator, the element is not visible or the given offset position is invalid.<br/>Accepts any element locator. The offset position has to be specified as <em>x,y</em> where <em>x</em> and <em>y</em> denote the offsets (number of pixels) to add to the <em>x</em>- and <em>y</em>-coordinate of the target element position. </p><p>Example: <em>mouseMoveAt(id=confirmationButton, 0,0)</em></p><h5 id="mouseOut">mouseOut</h5><p>Moves the mouse away from the targeted element. The mouse hovers to the current page&#8217;s body and the locator is ignored. However, still trying to identify an element using the locator, it will abort the test with an error if such an element can&#8217;t be found or the element is not visible. Accepts any element locator. Note that you <strong>cannot</strong> move the mouse outside the current page&#8217;s HTML body.</p><p>Example: <em>mouseOut(id=confirmationButton)</em></p><h5 id="mouseOver">mouseOver</h5><p>Hovers the mouse over the targeted element. Will abort the test with an error if there is no element matching the given locator or the element is not visible. Accepts any element locator.</p><p>Example: <em>mouseOver(id=confirmationButton)</em></p><blockquote class="note"><p>Note that CSS hover events are <strong>not</strong> triggered by <em>mouserover</em> since these are implemented using native browser events.</p></blockquote><h5 id="mouseUp">mouseUp</h5><p>Releases the left mouse button on the targeted element. Will abort the test with an error if there is no element matching the given locator or the element is not visible. Accepts any element locator.</p><p>Example: <em>mouseUp(id=confirmationButton)</em></p><h5 id="mouseUpAt">mouseUpAt</h5><p>Releases the left mouse button on the targeted element at the given offset position. Will abort the test with an error if there is no element matching the given locator, the element is not visible or the given offset position is invalid. <br/>Accepts any element locator. The offset position has to be specified as <em>x,y</em> where <em>x</em> and <em>y</em> denote the offsets (number of pixels) to add to the <em>x</em>- and <em>y</em>-coordinate of the target element position. </p><p>Example: <em>mouseUpAt(id=confirmationButton, 0,0)</em></p><h5 id="removeSelection">removeSelection</h5><p>Deselects options in the targeted multi-selection element. Deselects all options matching the command value, so it may deselect several! Will abort the test with an error if no option matching the command value is found, the element is no multi-selection element, the multi-select element is disabled, or if it doesn&#8217;t support multi-selection. Accepts any element locator except the link locator for the multi-select element and any option locator for the value.</p><p>Example: <em>removeSelection(id=RadioChannel,index=1)</em></p><h5 id="select">select</h5><p>Selects options in the targeted element. Will abort the test with an error if no element matches the given locator, the element is not an HTML select element, or if it is disabled. Doesn&#8217;t affect disabled options. If the targeted element is a multi-select element, all other options except the ones identified by the locator will be deselected. Accepts any element locator for the target and any option locator for the command value. </p><p>Example: <em>select(id=RadioChannel,index=1)</em></p><h5 id="selectAndWait">selectAndWait</h5><p>Same as <strong>select</strong> but waits for the page load triggered by the selection to be completed or for the configured timeout to be reached. Will abort the test with an error if no page load is triggered and/or the configured timeout is reached.</p><p>Example: <em>selectAndWait(id=RadioChannel,index=1)</em></p><h5 id="submit">submit</h5><p>Submits the targeted form. Will abort the test with an error if the element is not an HTML form. Accepts any element locator.</p><p>Example: <em>submit()</em></p><h5 id="submitAndWait">submitAndWait</h5><p>Same as <strong>submit</strong> but waits for the page load triggered by the form submit to be completed or for the configured timeout to be reached. Will abort the test with an error if the targeted form can&#8217;t be found, if no page load is triggered, and/or if the configured timeout is reached.</p><p>Example: <em>submitAndWait()</em></p><h5 id="type">type</h5><p>Types the command value into the targeted element. Will abort the test with an error if no element can be identified using the locator. Accepts any element locator.</p><p>Example: <em>type(id=quantity,1)</em></p><h5 id="typeAndWait">typeAndWait</h5><p>Same as <strong>type</strong> but waits for the page load triggered by typing the given string to be completed. Will abort the test with an error if no page load is triggered and/or if the configured timeout is reached.</p><p>Example: <em>typeAndWait(id=quantity,1)</em></p><h5 id="uncheck">uncheck</h5><p>Deselects the targeted checkbox or radio button. Will abort the test with an error if no element matches the given locator, the element is not an input element, not of type <em>checkbox</em> or <em>radio</em>, or if it is disabled. Accepts any element locator.</p><p>Example: <em>uncheck(id=birthdayWrapping)</em></p><h5 id="uncheckAndWait">uncheckAndWait</h5><p>Same as <strong>uncheck</strong> but waits for the page load triggered by <strong>uncheck</strong> to be completed. Will abort the test with an error if no page load is triggered and/or the configured timeout is reached.</p><p>Example: <em>uncheckAndWait(id=birthdayWrapping)</em></p><h4 id="CommandsThatMakeAnAssertion">Commands That Make An Assertion</h4><p>See below for an explanation of all <em>assertXXX</em> and <em>assertNotXXX</em> commands. While <em>assertXXX</em> commands check that the given condition evaluates to <em>true</em>,  <em>assertNotXXX</em> commands check that the given condition evaluates to <em>false</em>.</p><h5 id="assertAttribute">assertAttribute</h5><p>Asserts that an element&#8217;s attribute value matches the given expression. Will abort the test with an error message in case there is no element matching the given locator, the element doesn&#8217;t have the requested attribute set or the value of the attribute doesn&#8217;t match the given text expression.</p><p>Example: <em>assertAttribute(id=logo@title,glob:Link to*)</em></p><h5 id="assertChecked">assertChecked</h5><p>Asserts that an element is checked. Will abort the test with an error if there is no element matching the element locator, the found element is neither a checkbox nor a radio element, or if it is not checked.</p><p>Example: <em>assertChecked(id=termsAndConditions)</em></p><h5 id="assertClass">assertClass</h5><p>Asserts that the <em>class</em> attribute of the element in question contains the given class(es). Multiple classes have to be specified as white-space separated string. Will abort the test with an error if there is no element matching the given element locator, the found element doesn&#8217;t have a <em>class</em> attribute or its <em>class</em> attribute doesn&#8217;t contain (any of) the given class(es).</p><p>Example: <em>assertClass(id=header,emphasized clearfix)</em> </p><h5 id="assertElementCount">assertElementCount</h5><p>Asserts that the number of matching elements is equal to the given value. Will abort the test with an error message if the number of matching element is not equal to the given value. Accepts any element locator.</p><p>Example: <em>assertElementCount(css=.small3.span4, 4)</em></p><h5 id="assertElementPresent">assertElementPresent </h5><p>Asserts that an element is present on the page. Will abort the test with an error if there is no element matching the element locator. Accepts any element locator.</p><p>Example: <em>assertElementPresent(id=ProductDetails)</em></p><h5 id="assertEval">assertEval</h5><p>Asserts that the result of evaluating the command&#8217;s target as javascript expression matches the given text pattern. Will abort the test with an error message if the result of the evaluation doesn&#8217;t match the given text pattern.</p><p>Example: <em>assertEval(window.location,regexp:^https?://)</em></p><h5 id="assertLoadTime">assertLoadTime</h5><p>Asserts that the time needed to load the current page doesn&#8217;t exceed the given value. Will abort the test with an error if page loading lasts too long. Has no target (because it aims at the current page) but only a mandatory value representing the number of milliseconds the page load may last at most.</p><p>Example: <em>assertLoadTime(15000)</em></p><h5 id="assertNotAttribute">assertNotAttribute</h5><p>Asserts that an element&#8217;s attribute value does not match the given expression. Will abort the test with an error message in case there is no element matching the given locator, the element doesn&#8217;t have the requested attribute set or the value of the attribute matches the given expression.</p><p>Example: <em>assertNotAttribute(id=logo@title,glob:Return to*)</em></p><h5 id="assertNotChecked">assertNotChecked</h5><p>Asserts that the given element is not checked. Will abort the test with an error if there is no element matching the given element locator, the found element is neither a checkbox nor a radio element, or if it is checked.</p><p>Example: <em>assertNotChecked(name=sameAsBilling)</em></p><h5 id="assertNotClass">assertNotClass</h5><p>Asserts that the <em>class</em> attribute of the element in question does not contain (any of) the given class(es). Will abort the test with an error if there is no element matching the given element locator, the found element doesn&#8217;t have a <em>class</em> attribute or its <em>class</em> attribute contains (any of) the given class(es).</p><p>Example: <em>assertNotClass(id=header,odd hotfix)</em> </p><h5 id="assertNotElementCount">assertNotElementCount</h5><p>Asserts that the number of elements matching the given element locator is not equal to the given value. Will abort the test with an error message if the number of matching elements is equal to the given value.</p><p>Example: <em>assertNotElementCount(xpath=/html/body, 2)</em></p><h5 id="assertNotElementPresent">assertNotElementPresent</h5><p>The inversion of <em>assertElementPresent</em>, thus the syntax is the same except the command name.</p><p>Example: <em>assertNotElementPresent(id=ProductDetails)</em></p><h5 id="assertNotEval">assertNotEval</h5><p>The inversion of <em>assertEval</em>, thus the syntax is the same except the command name.</p><p>Example: <em>assertNotEval(document.title,Homepage)</em></p><h5 id="assertNotSelectedId">assertNotSelectedId</h5><p>Asserts that the ID of no selected option of the given select element matches the argument pattern. Will abort the test with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target and any text-matching approach for the command value.</p><p>Example: <em>assertNotSelectedId(id=shippingMethod,UPS)</em></p><h5 id="assertNotSelectedIndex">assertNotSelectedIndex</h5><p>Asserts that the index of no selected option of the given select element matches the argument index. Will abort the test with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target. The indices start with 0.</p><p>Example: <em>assertNotSelectedIndex(id=shippingMethod,1)</em></p><h5 id="assertNotSelectedLabel">assertNotSelectedLabel</h5><p>Asserts that the label of no selected option of the given select element matches the argument pattern. Will abort the test with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target and any text-matching approach for the command value.</p><p>Example: <em>assertNotSelectedLabel(id=shippingMethod,UPS)</em></p><h5 id="assertNotSelectedValue">assertNotSelectedValue</h5><p>Asserts that the value of no selected option of the given select element matches the argument pattern. Will abort the test with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target and any text-matching approach for the command value.</p><p>Example: <em>assertNotSelectedValue(id=shippingMethod,UPS)</em></p><h5 id="assertNotStyle">assertNotStyle</h5><p>Asserts that the actual element&#8217;s style doesn&#8217;t match the given CSS style declaration(s). Will abort the test with an error message if there is element matching the given locator or the element&#8217;s actual style matches (any of) the given CSS style declaration(s).</p><blockquote class="note"><p>Please notice that any size must be given in pixels and colors must be specified in <em>rgb</em> notation. Furthermore, browser-specific style rules may apply which might add a browser dependency to your tests. Also keep in mind that an element&#8217;s actual style may be influenced by the browser&#8217;s viewport properties (height, width etc.). </p></blockquote><p>Example: <em>assertNotStyle(id=headline,color:rgb(255, 0, 0))</em> </p><h5 id="assertNotText">assertNotText</h5><p>Asserts that the command value (=text) doesn&#8217;t match the text of the identified element. Will abort the test with an error if the element&#8217;s text matches the value text or if there is no element matching the target expression. Accepts any element locator for the command target and any text-matching approach for the command value. <strong>Note that hidden elements have an empty string as text!</strong></p><p>Example: <em>assertNotText(id=availability,regexp:.<span style="color:black">*</span><b></b>out of stock.*)</em></p><h5 id="assertNotTextPresent">assertNotTextPresent</h5><p>Asserts that there is no text on the page matching the command value (=text). Will abort the test with an error if the page&#8217;s text matches the text. Has no target (because it aims at the current page). Accepts any text-matching approach for the value. Differs to <strong>assertNotText</strong> in so far that the text value is matched if the page <em>does contain</em> a text snippet matching the text value, even if you use the <em>exact</em> text-matching approach.</p><p>Example: <em>assertNotTextPresent(exact:? results returned)</em> (also matches if the text is &#8220;No results returned!&#8221; because &#8220;o results returned&#8221; is exactly matched by the command value)</p><h5 id="assertNotTitle">assertNotTitle</h5><p>Asserts that the page title doesn&#8217;t match the command value. Like for <strong>assertNotTextPresent</strong>, there is no target because it aims at the current page&#8217;s title. Unlike <strong>assertNotTextPresent</strong>, however, the text must strictly match the command value (which nevertheless may contain wildcards and such things). Will abort the test with an error if the current page&#8217;s title matches the command value. Accepts any text-matching approach. </p><p>Example: <em>assertNotTitle(exact:? results returned)</em> (doesn&#8217;t match if the title is &#8220;0 results returned!&#8221; due to the exclamation mark)</p><h5 id="assertNotValue">assertNotValue</h5><p>Asserts that the value of the element identified by the locator doesn&#8217;t match the command value. Accepts any element locator for the target and any text-matching approach for the command value.</p><p>Example: <em>assertNotValue(id=shippingMethods,UPS)</em></p><h5 id="assertNotVisible">assertNotVisible</h5><p>Asserts that the element identified by the locator is currently not displayed. Accepts any element locator.</p><p>Example: <em>assertNotVisible(id=shippingMethods)</em></p><h5 id="assertNotXpathCount">assertNotXpathCount</h5><p>Asserts that the number of elements matching the locator doesn&#8217;t equal the command value. Will abort the test with an error if the number of matching elements equals the command value. Only accepts an xpath expression for the locator. </p><p>Example: <em>assertNotXpathCount(xpath=a[.=&#8216;continue&#8217;],1)</em></p><h5 id="assertPageSize">assertPageSize</h5><p>Asserts that the size of the current page doesn&#8217;t exceed the given number of bytes specified in the command value. Will abort the test with an error if the page size is greater than the given value. Just as for the other page-related commands, there is no locator. Make sure to specify the value using an integer number, floating-point numbers result in an error and test abortion. </p><p>Example: <em>assertPageSize(1024)</em></p><h5 id="assertSelectedId">assertSelectedId</h5><p>The exact inversion of <strong>assertNotSelectedId</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>assertSelectedId(id=shippingMethod,UPS)</em></p><h5 id="assertSelectedIndex">assertSelectedIndex</h5><p>Asserts that the index of at least one selected option of the given select element matches the argument index. Will abort the test with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target; also accepts any text-matching approach for the command value, but it only makes sense to give integers as arguments. The indices start with 0. </p><p>Example: <em>assertSelectedIndex(id=shippingMethod,1)</em></p><h5 id="assertSelectedLabel">assertSelectedLabel</h5><p>The exact inversion of <strong>assertNotSelectedLabel</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>assertSelectedLabel(id=shippingMethod,UPS)</em></p><h5 id="assertSelectedValue">assertSelectedValue</h5><p>The exact inversion of <strong>assertNotSelectedValue</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>assertSelectedValue(id=shippingMethod,UPS)</em></p><h5 id="assertStyle">assertStyle</h5><p>Asserts that the actual element&#8217;s style matches the given CSS style declaration(s). Multiple style declarations have to be specified as semicolon-separated string. Will abort the test with an error message if there is element matching the given locator or the element&#8217;s actual style doesn&#8217;t match (any of) the given CSS style declaration(s).</p><p>Example: <em>assertStyle(xpath=/html/body,font-size:12px)</em> </p><h5 id="assertText">assertText</h5><p>The exact inversion of <strong>assertNotText</strong>. The syntax remains the same except the command name.</p><p>Example: <em>assertText(id=availability,regexp:.* in stock.*)</em></p><h5 id="assertTextPresent">assertTextPresent</h5><p>The exact inversion of <strong>assertNotTextPresent</strong>. The syntax remains the same except for the command name.</p><p>Example: <em>assertTextPresent(exact:? results returned)</em></p><h5 id="assertTitle">assertTitle</h5><p>The exact inversion of <strong>assertNotTitle</strong>. The syntax remains the same except for the command name.</p><p>Example: <em>assertTitle(? results returned)</em></p><h5 id="assertValue">assertValue</h5><p>Asserts that the value of the element identified by the locator matches the command value. Accepts any element locator for the target and any text-matching approach for the command value.</p><p>Example: <em>assertValue(id=shippingMethods,UPS)</em></p><h5 id="assertVisible">assertVisible</h5><p>Asserts that the element identified by the locator is currently displayed. Accepts any element locator.</p><p>Example: <em>assertVisible(id=shippingMethods)</em></p><h5 id="assertXpathCount">assertXpathCount</h5><p>The exact inversion of <strong>assertNotXpathCount</strong>. The syntax remains the same except for the command name.</p><p>Example: <em>assertXpathCount(xpath=a[.=&#8216;continue&#8217;], 1)</em></p><h4 id="CommandsThatWaitForAConditionToBecomeTrueFalse">Commands That Wait For A Condition To Become True/False</h4><p>See below for a description of all <em>waitForXXX</em> and <em>waitForNotXXX</em> commands. While <em>waitForXXX</em> commands wait for the condition to become <em>true</em>, <em>waitForNotXXX</em> commands wait for the condition to become <em>false</em>. The waiting time has a default value that can be overridden using the <em>setTimeout</em> command (see <a href="user-manual.html#MiscellaneousCommands">here</a> ).</p><h5 id="waitForAttribute">waitForAttribute</h5><p>Waits for the element&#8217;s attribute value to match the given text pattern. Will abort the test with an error message if there is no element found, the element doesn&#8217;t have the requested attribute set or the attribute value doesn&#8217;t match the given text pattern after the timeout is reached.</p><p>Example: <em>waitForAttribute(id=logo@title,Homepage)</em></p><h5 id="waitForChecked">waitForChecked</h5><p>Waits for the given element to be checked. Will abort the test with an error if the timeout is reached and there is no such radio or checkbox element that is checked.  </p><p>Example: _waitForChecked(name=shipAsGift)</p><h5 id="waitForClass">waitForClass</h5><p>Waits until the given element&#8217;s <em>class</em> attribute contains (all of) the given class(es). Will abort the test with an error message if there is no element found, the found element doesn&#8217;t have the <em>class</em> attribute set or the value of the <em>class</em> attribute doesn&#8217;t contain (any of) the given class(es) after the timeout is reached. </p><p>Example: <em>waitForClass(id=errormessage,hidden)</em></p><h5 id="waitForElementCount">waitForElementCount</h5><p>Waits until the number of elements matching the given locator is equal to the given value. Will abort the test with an error message if the number of matching elements is not equal to the given value after the timeout is reached.</p><p>Example: <em>waitForElementCount(link=More, 3)</em></p><h5 id="waitForElementPresent">waitForElementPresent</h5><p>Waits for the targeted element to be present on the current page. Will abort the test with an error if the timeout is reached and no such element is present. Accepts any element locator.</p><p>Example: <em>waitForElementPresent(id=shippingPanel)</em></p><h5 id="waitForEval">waitForEval</h5><p>Waits for the result of evaluation the command&#8217;s target as javascript expression to match the given text pattern. Will abort the test with an error message if the evaluation result doesn&#8217;t match the given text pattern after the timeout is reached.</p><p>Example: <em>waitForEval(document.forms.item(0).name,Search)</em></p><h5 id="waitForNotAttribute">waitForNotAttribute</h5><p>The exact inversion of <strong>waitForAttribute</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForNotAttribute(link=Toddler@style, regexp:display\s*:\s*none)</em></p><h5 id="waitForNotChecked">waitForNotChecked</h5><p>Waits for the given element to be unchecked. Will abort the test with an error if the timeout is reached and there is no such radio or checkbox element that is not checked.</p><p>Example: _waitForNotChecked(name=newsletter)</p><h5 id="waitForNotClass">waitForNotClass</h5><p>Waits until the given element&#8217;s <em>class</em> attribute does not contain (any of) the given class(es). Will abort the test with an error message if there is no element found, the found element doesn&#8217;t have the <em>class</em> attribute set or the value of the <em>class</em> attribute contains (any of) the given class(es) after the timeout is reached. </p><p>Example: <em>waitForNotClass(id=errormessage,hidden)</em></p><h5 id="waitForNotElementCount">waitForNotElementCount</h5><p>Waits until the number of elements matching the given locator is not equal to the given value. Will abort the test with an error message if the number of matching elements is equal to the given value after the timeout is reached.</p><p>Example: <em>waitForNotElementCount(link=More, 3)</em></p><h5 id="waitForNotElementPresent">waitForNotElementPresent</h5><p>The exact inversion of <strong>waitForElementPresent</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForNotElementPresent(id=minicart)</em></p><h5 id="waitForNotEval">waitForNotEval</h5><p>The exact inversion of <strong>waitForEval</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForNotEval(window.frames.length,3)</em></p><h5 id="waitForNotSelectedId">waitForNotSelectedId</h5><p>Waits for no selected option of the given select element to match the argument pattern as ID. The test will abort with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target and any text-matching approach for the command value.</p><p>Example: <em>waitForNotSelectedId(id=shippingMethod,UPS)</em></p><h5 id="waitForNotSelectedIndex">waitForNotSelectedIndex</h5><p>Waits for no selected option of the given select element to match the argument index. Will abort the test with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target. The indices start with 0.</p><p>Example: <em>waitForNotSelectedIndex(id=shippingMethod,1)</em></p><h5 id="waitForNotSelectedLabel">waitForNotSelectedLabel</h5><p>Waits for no selected option of the given select element to match the argument pattern as label. Will abort the test with an error if no element is found, the found element is not an HTML select, or if it has no options. Accepts any element locator for the target and any text-matching approach for the command value.</p><p>Example: <em>waitForNotSelectedLabel(id=shippingMethod,UPS)</em></p><h5 id="waitForNotSelectedValue">waitForNotSelectedValue</h5><p>Waits for no selected option of the given select element to match the argument pattern as value. Will abort the test with an error if no element is found, the found element is not an HTML select element, or if it has no options. Accepts any text-matching approach for the command value.</p><p>Example: <em>waitForNotSelectedValue(id=shippingMethod,UPS)</em></p><h5 id="waitForNotStyle">waitForNotStyle</h5><p>Waits for the actual style of the given element to not match (any of) the specified CSS style declaration(s). Will abort the test with an error message if element is found or the actual style of the element matches (any of) the given CSS style declaration(s) after the timeout is reached.</p><p>Example: <em>waitForNotStyle(name=email,color:rgb(255, 0, 0))</em></p><h5 id="waitForNotText">waitForNotText</h5><p>The exact inversion of <strong>waitForText</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForNotText(id=current,regexp::.* rainy.*)</em></p><h5 id="waitForNotTextPresent">waitForNotTextPresent</h5><p>The exact inversion of <strong>waitForTextPresent</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForNotTextPresent(weather forecast)</em></p><h5 id="waitForNotTitle">waitForNotTitle</h5><p>The exact inversion of <strong>waitForTitle</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForNotTitle(exact:Product Details Page)</em></p><h5 id="waitForNotValue">waitForNotValue</h5><p>Waits for the targeted element to have its value not matching the command value. Will abort the test with an error if the timeout is reached and no element is found for the given element locator or the value of the found element does match the command value.</p><p>Example: <em>waitForNotValue(name=surname,Doe)</em></p><h5 id="waitForNotVisible">waitForNotVisible</h5><p>Waits for the targeted element to become invisible. Will abort the test with an error if the timeout is reached and no element is found for the given element locator or the found element is visible.</p><p>Example: <em>waitForNotVisible(id=DialogOverlay)</em></p><h5 id="waitForNotXpathCount">waitForNotXpathCount</h5><p>The exact inversion of <strong>waitForXpathCount</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForNotXpathCount(id(<b></b><span style="color:black">'</span><b></b>lines'),5)</em></p><h5 id="waitForPageToLoad">waitForPageToLoad</h5><p>Waits for the page to load. Aborts the test with an error if the timeout is reached and page load hasn&#8217;t been completed. Has neither a target nor a command value.</p><p>Example: <em>waitForPageToLoad()</em></p><h5 id="waitForPopUp">waitForPopUp</h5><p>Waits for the first pop up to be completely loaded. Will abort the test with an error if the timeout is reached and no pop up has been loaded.</p><p>Example: <em>waitForPopUp()</em></p><h5 id="waitForPopUp2">waitForPopUp</h5><p>Same as <strong>waitForPopUp</strong> but uses the given value as timeout instead.</p><p>Example: <em>waitForPopUp(,5000)</em></p><h5 id="waitForPopUp3">waitForPopUp</h5><p>Same as the no parameter-variant of <strong>waitForPopUp</strong> but waits for the targeted pop up instead. Accepts any window locator.</p><p>Example: <em>waitForPopUp(name=popUpConfirmation)</em></p><h5 id="waitForPopUp4">waitForPopUp</h5><p>Same as the variant of <strong>waitForPopUp</strong> having a target only but also has a value that is interpreted as timeout to use. Accepts any window locator.</p><p>Example: <em>waitForPopUp(name=popUpConfirmation,5000)</em></p><h5 id="waitForSelectedId">waitForSelectedId</h5><p>The exact inversion of <strong>waitForNotSelectedId</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForSelectedId(id=shippingMethod,UPS)</em></p><h5 id="waitForSelectedIndex">waitForSelectedIndex</h5><p>Waits for the index of at least one selected option of the given select element to match the argument index. Will abort the test with an error if no element is found, the found element is not an HTML select element, or if it has no options. Accepts any element locator for the target; also accepts any text-matching approach for the command value but it only makes sense to give integers as arguments. The indices start with 0. </p><p>Example: <em>waitForSelectedIndex(id=shippingMethod,1)</em></p><h5 id="waitForSelectedLabel">waitForSelectedLabel</h5><p>The exact inversion of <strong>waitForNotSelectedLabel</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForSelectedLabel(id=shippingMethod,UPS)</em></p><h5 id="waitForSelectedValue">waitForSelectedValue</h5><p>The exact inversion of <strong>waitForNotSelectedValue</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForSelectedValue(id=shippingMethod,UPS)</em></p><h5 id="waitForStyle">waitForStyle</h5><p>The exact inversion of <strong>waitForStyle</strong> (regarding the behavior, not the syntax).</p><p>Example: <em>waitForStyle(name=phone,font-style:italic)</em></p><h5 id="waitForText">waitForText</h5><p>Waits for the targeted element to have its text matching the command value (text matching is done the same way as in <strong>assertNotText</strong>). Will abort the test with an error if the timeout is reached and the text doesn&#8217;t match the command value. Accepts any element locator and any text-matching approach.</p><p>Example: <em>waitForText(id=registerButton,Register)</em></p><h5 id="waitForTextPresent">waitForTextPresent</h5><p>Waits for the current page&#8217;s text to have its text matching the command value. Will abort the test with an error if the timeout is reached and the text doesn&#8217;t match the command value. Accepts any text-matching approach. As with <strong>assertTextPresent</strong>, it&#8217;s sufficient for the page to contain a text snippet such that the snippet matches the command value.</p><p>Example: <em>waitForTextPresent(exact:Registration completed)</em></p><h5 id="waitForTitle">waitForTitle</h5><p>Waits for the page&#8217;s title to match the command value. Will abort the test with an error if the timeout is reached and the title doesn&#8217;t match the command value. Has no target and accepts any text-matching approach in the command value.</p><p>Example: <em>waitForTitle(exact:Registration completed)</em></p><h5 id="waitForValue">waitForValue</h5><p>Waits for the targeted element to have its value matching the command value. Will abort the test with an error if the timeout is reached and no element is found for the given element locator or the value of the found element doesn&#8217;t match the command value.</p><p>Example: <em>waitForValue(phone,regexp:\d+)</em></p><h5 id="waitForVisible">waitForVisible</h5><p>Waits for the targeted element to become visible. Will abort the test with an error if the timeout is reached and no element is found or the found element is visible.</p><p>Example: <em>waitForVisible(id=minicart_items)</em></p><h5 id="waitForXpathCount">waitForXpathCount</h5><p>Waits for the page to have exactly the number of elements matched by the command value. Will abort the test with an error if the timeout is reached and the number of elements that can be identified using the locator doesn&#8217;t match the command value. Accepts only XPath expressions as locators.</p><p>Example: <em>waitForXPathCount(//*[contains(., &#8216;offer&#8217;)],3)</em></p><h4 id="CommandsThatStoreData">Commands That Store Data</h4><p>Store commands are used to store data in variables for later reuse. For example, you may click on a link pointing to the details of a specific product in an online store. To check that the details page for the correct product is shown, you need the product name from the previous page. If you define a variable <em>productName</em> with a store command, you can use it later on by typing <em>${productName}</em>.</p><blockquote class="note"><p>Note that for all store commands the variable name is given as the command&#8217;s <em>value</em>. Thus, for the simple store command the variable value is given as the command target, for the storeEval command it is the expression to evaluate.  </p></blockquote><h5 id="store">store</h5><p>Stores the text given as command target in the variable named after the command value. Can be used to define a value once and use it in several places. If you use random numbers, you may need an opportunity to access the randomly generated value again for checks (calling for a random again will only give the same value by coincidence).</p><p>Example: <em>store(MICHAEL,foo)</em></p><h5 id="storeAttribute">storeAttribute</h5><p>Stores the value of the given element&#8217;s attribute into the variable named after the command value.</p><p>Example: <em>storeAttribute(name=cal@data-uuid, uuid)</em></p><h5 id="storeElementCount">storeElementCount</h5><p>Stores the number of elements that match the given locator into the variable named after the command value.</p><p>Example: <em>storeElementCount(css=.active, nbActives)</em></p><h5 id="storeEval">storeEval</h5><p>Has no real target, but interprets it as JavaScript that is executed instead. Uses the command value as name of the variable in which to store the result of the JavaScript execution. <strong>Note that this will only have an effect if JavaScript is enabled during the test run!</strong></p><p>Example: <em>storeEval(SOME_JAVASCRIPT, foo)</em></p><h5 id="storeText">storeText</h5><p>Stores the text of the targeted element in the variable named after the command value. Accepts any element locator.</p><p>Example: <em>storeText(id=cartBalance, foo)</em></p><h5 id="storeValue">storeValue</h5><p>Stores the value (in case of a &lt;textarea&gt; the contained text) of the element identified by the given locator in the variable named after the command value.</p><p>Example: <em>storeValue(id=cartBalance, foo)</em></p><h5 id="storeXpathCount">storeXpathCount</h5><p>Stores the number of elements identified by using the locator in the variable named after the command value. Interprets the whole target as XPath that should be used to identify elements.</p><p>Example: <em>storeXpathCount(id(&#8216;emptyDiv&#8217;), foo)</em></p><h4 id="MiscellaneousCommands">Miscellaneous Commands</h4><h5 id="close">close</h5><p>Closes the browser window and thus aborts the test (if the browser has remaining tabs, the test will just pause, but it doesn&#8217;t make any sense to continue it). This is likely to be changed such that the <strong>close</strong> command may only be called as the last statement in a test. Has neither a target nor a command value.</p><h5 id="createCookie">createCookie</h5><p>Creates a cookie. This command is exceptional such that it neither has a &#8220;normal&#8221; target nor a &#8220;normal&#8221; command value, but takes two arguments instead. The first one has to be in the form <em>key</em>=_value_, the second one can be <strong>max_age=</strong><b></b><em>an integer</em> or <strong>path=</strong><b></b><em>a path</em> or another arbitrary expression. If the second argument has the form <strong>max_age=</strong><b></b><em>an integer</em>, the cookie will be stored with an expiration of the given integer value as seconds in the future. If the second argument has the form <strong>path=</strong><b></b><em>a path</em>, the cookie will be visible to <em>a path</em> (default is <em>/</em>) only. If the second argument has another form, it will be ignored.</p><p>Example: <em>createCookie(stokkeMovieShown=true,max_age=123456789000)</em></p><h5 id="deleteAllVisibleCookies">deleteAllVisibleCookies</h5><p>Deletes all cookies for the current domain. Has neither a target nor a value.</p><p>Example: <em>deleteAllVisibleCookies()</em></p><h5 id="deleteCookie">deleteCookie</h5><p>Deletes the cookie named after the first argument. Similar to <strong>createCookie</strong>, this instruction has two arguments. Currently, the second argument is ignored, the first one has to be a valid cookie name.</p><p>Example: <em>deleteCookie(stokkeMovie,isIgnored)</em></p><h5 id="open">open</h5><p>Opens the URL given as command value. Has no target. The URL can be relative or absolute.</p><blockquote class="note"><p>If you specify a relative URL, it will be resolved using the currently active base URL, which is usually the test case&#8217;s base URL.</p></blockquote><p>Example: <em>open(article.php?story=2012020309143182)</em> (where the base URL is <em>http://www.groklaw.net/</em>)</p><h5 id="pause">pause</h5><p>Causes the test to pause the given amount of time in milliseconds. Has no target. The time to pause has to be given as integer number as command value.</p><p>Example: <em>pause(5000)</em></p><h5 id="selectFrame">selectFrame</h5><p>Switches to the targeted frame. The <em>relative</em> approaches will never cause the test to abort with an error. The other approaches will do so if no frame is identified using them. Accepts any <a href="user-manual.html#FrameLocators">frame locator</a>. Furthermore, you can simply use the frame&#8217;s name or ID.</p><p>Example: <em>selectFrame(index=1)</em></p><h5 id="selectWindow">selectWindow</h5><p>Similar to <strong>selectWindow</strong> below but has no parameter. Instead, it searches for the window having no name or no title, which is often the top-level window. Will abort the test with an error if there is no such window.</p><p>Example: <em>selectWindow()</em></p><h5 id="selectWindow2">selectWindow</h5><p>Switches to the targeted window. Accepts any <a href="user-manual.html#WindowLocators">window locator</a>. Will abort the test with an error if no window is found using the given window locator.</p><p>Example: <em>selectWindow(title=customerAccount)</em></p><h5 id="setTimeout">setTimeout</h5><p>Sets the timeout used by <strong>...AndWait</strong> and <strong>waitFor...</strong>. The new timeout (in milliseconds) has to be given as integer number as command value.</p><p>Example: <em>setTimeout(30000)</em></p><h3 id="CommandDetails">Command Short Reference</h3><p>While the previous section is meant to provide a thorough understanding of commands and how they work, the command short reference below aims at a formal and concise presentation. The following abbreviations and terms are used: </p><ul><li>&#8220;EL&#8221; indicates any element locator in accordance with &#8220;OL&#8221; (any option locator), &#8220;AL&#8221; (any attribute locator), &#8220;WL&#8221; (any window locator), and &#8220;FL&#8221; (any frame locator). If there are further restrictions, for example just some of the element locators are allowed, the allowed ones are given by their identifier as &#8220;name&#8221;, &#8220;link&#8221;, and so on.</li><li>&#8220;Text matching&#8221; indicates any of the text-matching approaches that, in accordance with the locators &#8220;glob&#8221;, &#8220;exact&#8221; and &#8220;regexp&#8221;, are used if just a subset of them is allowed.</li><li>&#8220;Integer&#8221; indicates an arbitrary integer within the range of 0 to 2147483647, &#8220;JS expression&#8221; indicates a valid JavaScript expression, &#8220;XPath expression&#8221; a valid XPath expression (be careful not to mix it up with the XPath element locator!).</li><li>&#8220;Variable&#8221; indicates an arbitrary variable name. A valid variable name is a non-empty string matching the regular expression: [A-Za-z]([A-Za-z0-9_])*</li><li>&#8220;Class List&#8221; refers to a non-empty list of CSS class names separated by whitespace (e.g.: foo bar baz).</li><li>&#8220;Style&#8221; refers to a non-empty list of CSS style declarations separated by semicolon (e.g.: display:block; font-weight:bold; cursor:pointer).</li><li>&#8220;Arbitrary&#8221; indicates an arbitrary text (a text that should be typed into a form, for instance). Be aware that &#8220;arbitrary&#8221; doesn&#8217;t mean there are no restrictions. Thus, a web application may require a password to have a minimum length or the text is interpreted as a path on a file system, for example.</li></ul><h4 id="InteractingCommands">Interacting Commands</h4><table><tr><th>Command</th><th>Target</th><th>Value</th></tr><tr><td>addSelection</td><td>EL</td><td>OL</td></tr><tr><td>check</td><td>EL</td><td>-</td></tr><tr><td>checkAndWait</td><td>EL</td><td>Integer</td></tr><tr><td>click</td><td>EL</td><td>-</td></tr><tr><td>clickAndWait</td><td>EL</td><td>Integer</td></tr><tr><td>doubleClick</td><td>EL</td><td>-</td></tr><tr><td>doubleClickAndWait</td><td>EL</td><td>Integer</td></tr><tr><td>mouseDown</td><td>EL</td><td>-</td></tr><tr><td>mouseDownAt</td><td>EL</td><td>Integer,Integer</td></tr><tr><td>mouseMove</td><td>EL</td><td>-</td></tr><tr><td>mouseMoveAt</td><td>EL</td><td>Integer,Integer</td></tr><tr><td>mouseOut</td><td>EL</td><td>-</td></tr><tr><td>mousOver</td><td>EL</td><td>-</td></tr><tr><td>mouseUp</td><td>EL</td><td>-</td></tr><tr><td>mouseUpAt</td><td>EL</td><td>Integer,Integer</td></tr><tr><td>removeSelection</td><td>EL</td><td>OL</td></tr><tr><td>select</td><td>EL</td><td>OL</td></tr><tr><td>selectAndWait</td><td>EL</td><td>OL</td></tr><tr><td>submit</td><td>EL</td><td>-</td></tr><tr><td>submitAndWait</td><td>EL</td><td>-</td></tr><tr><td>type</td><td>EL</td><td>Arbitrary</td></tr><tr><td>typeAndWait</td><td>EL</td><td>Arbitrary</td></tr><tr><td>uncheck</td><td>EL</td><td>-</td></tr><tr><td>uncheckAndWait</td><td>EL</td><td>-</td></tr></table><h4 id="AssertCommands">Assert Commands</h4><table><tr><th>Command</th><th>Target</th><th>Value</th></tr><tr><td>assertAttribute</td><td>AL</td><td>Text Pattern</td></tr><tr><td>assertChecked</td><td>EL</td><td>-</td></tr><tr><td>assertClass</td><td>EL</td><td>Class List</td></tr><tr><td>assertElementCount</td><td>EL</td><td>Integer</td></tr><tr><td>assertElementPresent</td><td>EL</td><td>-</td></tr><tr><td>assertEval</td><td>JS Expression</td><td>Text Pattern</td></tr><tr><td>assertLoadTime</td><td>-</td><td>Integer</td></tr><tr><td>assertNotAttribute</td><td>AL</td><td>Text Pattern</td></tr><tr><td>assertNotChecked</td><td>EL</td><td>-</td></tr><tr><td>assertNotClass</td><td>EL</td><td>Class List</td></tr><tr><td>assertNotElementCount</td><td>EL</td><td>Integer</td></tr><tr><td>assertNotElementPresent</td><td>EL</td><td>-</td></tr><tr><td>assertNotEval</td><td>JS Expression</td><td>Text Pattern</td></tr><tr><td>assertNotSelectedId</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertNotSelectedIndex</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertNotSelectedLabel</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertNotSelectedValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertNotStyle</td><td>EL</td><td>Style</td></tr><tr><td>assertNotText</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertNotTextPresent</td><td>-</td><td>Text Pattern</td></tr><tr><td>assertNotTitle</td><td>-</td><td>Text Pattern</td></tr><tr><td>assertNotValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertNotVisible</td><td>EL</td><td>-</td></tr><tr><td>assertNotXpathCount</td><td><em>xpath</em> </td><td>Integer</td></tr><tr><td>assertPageSize</td><td>-</td><td>Integer</td></tr><tr><td>assertSelectedId</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertSelectedIndex</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertSelectedLabel</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertSelectedValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertStyle</td><td>EL</td><td>Style</td></tr><tr><td>assertText</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertTextPresent</td><td>-</td><td>Text Pattern</td></tr><tr><td>assertTitle</td><td>-</td><td>Text Pattern</td></tr><tr><td>assertValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>assertVisible</td><td>EL</td><td>-</td></tr><tr><td>assertXpathCount</td><td><em>xpath</em> </td><td>Integer</td></tr></table><h4 id="WaitCommands">Wait Commands</h4><table><tr><th>Command</th><th>Target</th><th>Value</th></tr><tr><td>waitForAttribute</td><td>AL</td><td>Text Pattern</td></tr><tr><td>waitForChecked</td><td>EL</td><td>-</td></tr><tr><td>waitForClass</td><td>EL</td><td>Class List</td></tr><tr><td>waitForElementCount</td><td>EL</td><td>Integer</td></tr><tr><td>waitForElementPresent</td><td>EL</td><td>-</td></tr><tr><td>waitForEval</td><td>JS Expression</td><td>Text Pattern</td></tr><tr><td>waitForNotAttribute</td><td>AL</td><td>Text Pattern</td></tr><tr><td>waitForNotChecked</td><td>EL</td><td>-</td></tr><tr><td>waitForNotClass</td><td>EL</td><td>Class List</td></tr><tr><td>waitForNotElementCount</td><td>EL</td><td>Integer</td></tr><tr><td>waitForNotElementPresent</td><td>EL</td><td>-</td></tr><tr><td>waitForNotEval</td><td>JS Expression</td><td>Text Pattern</td></tr><tr><td>waitForNotSelectedId</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForNotSelectedIndex</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForNotSelectedLabel</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForNotSelectedValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForNotStyle</td><td>EL</td><td>Style</td></tr><tr><td>waitForNotText</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForNotTextPresent</td><td>-</td><td>Text Pattern</td></tr><tr><td>waitForNotTitle</td><td>-</td><td>Text Pattern</td></tr><tr><td>waitForNotValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForNotVisible</td><td>EL</td><td></td></tr><tr><td>waitForNotXpathCount</td><td><em>xpath</em> </td><td>Integer</td></tr><tr><td>waitForPageToLoad</td><td>-</td><td>Integer</td></tr><tr><td>waitForPopUp</td><td>WL</td><td>-</td></tr><tr><td>waitForSelectedId</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForSelectedIndex</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForSelectedLabel</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForSelectedValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForStyle</td><td>EL</td><td>Style</td></tr><tr><td>waitForText</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForTextPresent</td><td>-</td><td>Text Pattern</td></tr><tr><td>waitForTitle</td><td>-</td><td>Text Pattern</td></tr><tr><td>waitForXpathCount</td><td><em>xpath</em> </td><td>Integer</td></tr><tr><td>waitForValue</td><td>EL</td><td>Text Pattern</td></tr><tr><td>waitForVisible</td><td>EL</td><td>-</td></tr></table><h4 id="StoreCommands">Store Commands</h4><table><tr><th>Command</th><th>Target</th><th>Value</th></tr><tr><td>store</td><td>Arbitrary</td><td>Variable</td></tr><tr><td>storeAttribute</td><td>AL</td><td>Variable</td></tr><tr><td>storeElementCount</td><td>EL</td><td>Variable</td></tr><tr><td>storeEval</td><td>JS expression</td><td>Variable</td></tr><tr><td>storeText</td><td>EL</td><td>Variable</td></tr><tr><td>storeValue</td><td>EL</td><td>Variable</td></tr><tr><td>storeXpathCount</td><td>xpath</td><td>Variable</td></tr></table><h4 id="MiscellaneousCommands2">Miscellaneous Commands</h4><table><tr><th>Command</th><th>Target</th><th>Value</th></tr><tr><td>close</td><td>-</td><td>-</td></tr><tr><td>createCookie</td><td>Arbitrary=Arbitrary</td><td>max_age=Delay path=Arbitrary</td></tr><tr><td>deleteAllVisibleCookies</td><td>-</td><td>-</td></tr><tr><td>deleteCookie</td><td>Arbitrary</td><td>Arbitrary</td></tr><tr><td>open</td><td>-</td><td>Arbitrary</td></tr><tr><td>pause</td><td>-</td><td>Integer</td></tr><tr><td>selectFrame</td><td>FL</td><td>-</td></tr><tr><td>selectWindow</td><td>WL</td><td>-</td></tr><tr><td>setTimeout</td><td>-</td><td>Integer</td></tr></table><h3 id="AdditionalHints">Using the XLT Scripting API</h3><p>Looking at the command examples provided above, you may have noticed their notation: </p><ul><li>xpath=//a[contains(@href,&#8216;1423033.html&#8217;)]</li><li>assertTextPresent(exact:Any Questions?)</li><li>clickAndWait(link=Samsung)</li><li>pause(5000)</li></ul><p>However, this notation neither matches Script Developer nor the XLT Scripting API. Script Developer provides a GUI with fields into which the parts of the commands can be entered and fields that are disabled since they are invalid for the chosen command. Thus, depending on the command in question, you may not be able to enter a target or a value.</p><p>Using the Scripting API, you write Java code where strings have to start and end with double quotes, meta-characters have to be escaped, and statements have to end with a semicolon. However, XLT ignores all this and uses a notation close to Java instead. We think of this as the best readable notation that is intuitively understandable without giving lengthy formal definitions.</p><p>For instance, consider this command: <em>assertTextPresent(regexp:&#8220;\\home&#8221;)</em></p><p>It checks if the text <em>"\home"</em> is present on the current page. When using it in Java, do three things:</p><ol><li>Escape characters appropriately. In the present case, you have to escape the double quote AND the backslash characters.</li><li>The whole argument string has to be enclosed in double quotes.</li><li>The Java statement has to end with a &#8216;;&#8217;.</li></ol><p>As a result, you end up with (added characters are red): <em>assertTextPresent(<span style="color:red">"</span><b></b>regexp:<span style="color:red">\"\</span>\<b></b><span style="color:red">\</span><b></b>\home<b></b><span style="color:red">\</span><b></b>"<span style="color:red">"</span>)<span style="color:red">;</span></em></p><p>Note that in the Script Developer GUI, you don&#8217;t see any opening or closing parentheses for commands. In the XLT Scripting API, however, you need to use parentheses because commands are actually calls to Java methods.</p><p>As scripts are stored as XML files, each command has an appropriate XML representation where the <code>name</code> attribute specifies the command type, the <code>target</code> attribute specifies the command&#8217;s target, and the <code>value</code> attribute specifies the command&#8217;s value. If the command doesn&#8217;t have a target or value, the appropriate XML attribute will be omitted.<br/>The XML representation of the command above looks like this:</p><pre class="xml"><code>&lt;command name="assertTextPresent" value="regexp:&amp;quot;\\home&amp;quot;"/&gt; 
</code></pre><p>Note that the double quotes have been escaped in XML.</p><h2 id="Appendix">Appendix</h2><h3 id="DeveloperShortcuts">Script Developer Keyboard Shortcuts</h3><table><tr><th style="text-align: left;">Context</th><th style="text-align: left;">Command</th><th style="text-align: left;">Keyboard Shortcut</th></tr><tr><td style="vertical-align: top;" rowspan="13">Global</td><td>Increase Replay Speed</td><td>+</td></tr><tr><td>Decrease Replay Speed</td><td>-</td></tr><tr><td>Replay</td><td>Alt+F5</td></tr><tr><td>Single Step Forward</td><td>Alt+F6</td></tr><tr><td>Pause</td><td>Alt+F7</td></tr><tr><td>Stop</td><td>Alt+F8</td></tr><tr><td>Record</td><td>Alt+F9</td></tr><tr><td>Reload Active Script</td><td>F5</td></tr><tr><td>Reload All Scripts</td><td>Shift+F5</td></tr><tr><td>Save Active Script</td><td>Ctrl+S</td></tr><tr><td>Save All Scripts</td><td>Ctrl+Shift+S</td></tr><tr><td>Close Active Tab</td><td>Ctrl+W</td></tr><tr><td>Close All Tabs</td><td>Ctrl+Shift+W</td></tr><tr><td style="vertical-align: top;" rowspan="4">Script Explorer</td><td>Edit</td><td>Return</td></tr><tr><td>Edit Details</td><td>Alt+Return</td></tr><tr><td>Manage Test Data</td><td>Alt+D</td></tr><tr><td>Delete</td><td>Del</td></tr><tr><td style="vertical-align: top;" rowspan="14">Script Editor</td><td>Insert New Command (before currently selected command)</td><td>Insert</td></tr><tr><td>Insert New Command (after currently selected command)</td><td>Shift+Insert</td></tr><tr><td>En/Disable Module/Command</td><td>Ctrl+Shift+C</td></tr><tr><td>Toggle Start Point</td><td>S</td></tr><tr><td>Toggle Breakpoint</td><td>B</td></tr><tr><td>Cut</td><td>Ctrl+X</td></tr><tr><td>Copy</td><td>Ctrl+C</td></tr><tr><td>Paste</td><td>Ctrl+V</td></tr><tr><td>Delete</td><td>Del</td></tr><tr><td>Select All Commands</td><td>Ctrl+A</td></tr><tr><td>Move Command Up</td><td>Alt+Up</td></tr><tr><td>Move Command Down</td><td>Alt+Down</td></tr><tr><td>Multiple Select</td><td>Shift+Up/Down</td></tr><tr><td>Multiple Select Selective</td><td>Ctrl+Up/Down/Spacebar</td></tr></table><h3 id="Terminology">Terminology </h3><ul><li><strong>Action</strong>: An <em>Action</em> represents a self-contained logical step in a test case. Typically, actions are re-usable building blocks which can be used across multiple test cases. Actions themselves may issue one or more requests.</li><li><strong>Agent</strong>: An <em>Agent</em> simulates a group of virtual users, which repeatedly execute certain test cases against the system under test.</li><li><strong>Agent Controller</strong>: An <em>Agent Controller</em> controls one or more agents.</li><li><strong>Master Controller</strong>: The <em>Master Controller</em> controls all agent controllers and gets the results, the logs and the pages generated by the agents from the storage of the agent controller machines.</li><li><strong>Property File</strong>: <em>Property Files</em> are text files that contain Java properties. A Java property represents a mapping from a symbol (property name) to a value (property value).</li><li><strong>Request</strong>: Typically, a <em>Request</em> represents one call to a (remote) server. It does not matter what protocol is used to contact the server.</li><li><strong>System under Test</strong>: The <em>System Under Test</em> is the application being tested. In the XLT demo, for example, this is the poster shop application.</li><li><strong>Test Case</strong>: A <em>Test Case</em> is the program that models a transaction.</li><li><strong>Transaction</strong>: A <em>Transaction</em> represents one execution of a certain test case. Typically, a transaction uses multiple actions to model the test scenario.</li><li><strong>User</strong>: A <em>User</em> is one incarnation of a client that interacts with a server. During a load test, multiple users will operate in parallel to simulate a multitude of human or technical users interacting with the target system at the same time. Typically, users are configured to execute a certain test case repeatedly.</li><li><strong>Virtual User</strong>: See <em>User</em>.</li></ul><h3 id="Acknowledgments">Acknowledgments</h3><ul><li>This product includes software developed by Andy Clark.</li><li>This product includes software developed by <a href="http://www.apache.org/">The Apache Software Foundation</a>.</li><li>This product includes software developed by <a href="http://www.caucho.com/">Caucho Technology</a>.</li><li>This product includes software developed by <a href="http://www.GargoyleSoftware.com/">Gargoyle Software Inc.</a>.</li><li>This product includes software developed by <a href="mailto:mail@digitarald.de">Harald Kirschner</a>.</li><li>This product includes software developed by <a href="http://jquery.org/license">The jQuery Team</a> under MIT license.</li><li>This product includes software developed by <a href="http://alexgorbatchev.com/wiki/SyntaxHighlighter">Alex Gorbatchev</a>.</li><li>This webpage features <a href="https://github.com/krewenki/jquery-lightbox">jQuery LightBox</a> developed by Warren Krewenki published under BSD license.</li><li>The documentation uses icons from the <a href="http://tango.freedesktop.org/Tango_Desktop_Project">Tango Icon Library</a>.</li></ul><p>Please see the directory <em>doc/3rd-party-licenses/</em> for more third party license information.  </p><p id="copyright">Copyright &#169; 2014 by <a href="http://www.xceptance.com/">Xceptance Software Technologies</a>. All rights reserved.</p></div> 
 </div> <!-- Piwik, call it only when the doc is hosted by Xceptance -->

<!-- disabled for now (#2071)

<script type="text/javascript">
(function(w, $){
    var d = w.document,
        l = d.location,
        host = l.hostname || "localhost",
        hostedByXC = host.match(/xceptance[^.]*\.[a-zA-Z]+$/i);

    if(hostedByXC){
        var pkUrl = (("https:" === l.protocol) ? "https://" : "http://") + "stats.xceptance.com/";
        d.write(w.unescape("%3Cscript src='" + pkUrl + "piwik.js' type='text/javascript'%3E%3C/script%3E"));
        $(w).load(function(){
            try {
                var piwikTracker = Piwik.getTracker(pkUrl + "piwik.php", 4);
                piwikTracker.trackPageView();
                piwikTracker.enableLinkTracking();
            } catch(e) {}
        });
    }
})(window, jQuery);
</script>

-->
</body>
</html>